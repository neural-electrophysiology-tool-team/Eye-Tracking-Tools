{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "from itertools import cycle\n",
    "import pickle\n",
    "import pathlib\n",
    "import math\n",
    "import tqdm\n",
    "import scipy.io\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.io\n",
    "import h5py\n",
    "import re\n",
    "from lxml import etree as ET\n",
    "import scipy.signal as sig\n",
    "import pandas as pd\n",
    "from scipy.stats import kde\n",
    "from BlockSync_current import BlockSync\n",
    "import UtilityFunctions_newOE as uf\n",
    "from scipy import signal\n",
    "import bokeh\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "rcParams['pdf.fonttype'] = 42  # Ensure fonts are embedded and editable\n",
    "rcParams['ps.fonttype'] = 42  # Ensure compatibility with vector outputs\n",
    "\n",
    "\n",
    "def bokeh_plotter(data_list, x_axis_list=None, label_list=None,\n",
    "                  plot_name='default',\n",
    "                  x_axis_label='X', y_axis_label='Y',\n",
    "                  peaks=None, peaks_list=False, export_path=False):\n",
    "    \"\"\"Generates an interactive Bokeh plot for the given data vector.\n",
    "    Args:\n",
    "        data_list (list or array): The data to be plotted.\n",
    "        label_list (list of str): The labels of the data vectors\n",
    "        plot_name (str, optional): The title of the plot. Defaults to 'default'.\n",
    "        x_axis (str, optional): The label for the x-axis. Defaults to 'X'.\n",
    "        y_axis (str, optional): The label for the y-axis. Defaults to 'Y'.\n",
    "        peaks (list or array, optional): Indices of peaks to highlight on the plot. Defaults to None.\n",
    "        export_path (False or str): when set to str, will output the resulting html fig\n",
    "    \"\"\"\n",
    "    color_cycle = cycle(bokeh.palettes.Category10_10)\n",
    "    fig = bokeh.plotting.figure(title=f'bokeh explorer: {plot_name}',\n",
    "                                x_axis_label=x_axis_label,\n",
    "                                y_axis_label=y_axis_label,\n",
    "                                plot_width=1500,\n",
    "                                plot_height=700)\n",
    "\n",
    "    for i, data_vector in enumerate(data_list):\n",
    "\n",
    "        color = next(color_cycle)\n",
    "\n",
    "        if x_axis_list is None:\n",
    "            x_axis = range(len(data_vector))\n",
    "        elif len(x_axis_list) == len(data_list):\n",
    "            print('x_axis manually set')\n",
    "            x_axis = x_axis_list[i]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                'problem with x_axis_list input - should be either None, or a list with the same length as data_list')\n",
    "        if label_list is None:\n",
    "            fig.line(x_axis, data_vector, line_color=color, legend_label=f\"Line {i + 1}\")\n",
    "        elif len(label_list) == len(data_list):\n",
    "            fig.line(range(len(data_vector)), data_vector, line_color=color, legend_label=f\"{label_list[i]}\")\n",
    "        if peaks is not None and peaks_list is True:\n",
    "            fig.circle(peaks[i], data_vector[peaks[i]], size=10, color=color)\n",
    "\n",
    "    if peaks is not None and peaks_list is False:\n",
    "        fig.circle(peaks, data_vector[peaks], size=10, color='red')\n",
    "\n",
    "    if export_path is not False:\n",
    "        print(f'exporting to {export_path}')\n",
    "        bokeh.io.output.output_file(filename=str(export_path / f'{plot_name}.html'), title=f'{plot_name}')\n",
    "    bokeh.plotting.show(fig)\n",
    "\n",
    "\n",
    "def load_eye_data_2d_w_rotation_matrix(block):\n",
    "    \"\"\"\n",
    "    This function checks if the eye dataframes and rotation dict object exist, then imports them\n",
    "    :param block: The current blocksync class with verifiec re/le dfs\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        block.left_eye_data = pd.read_csv(block.analysis_path / 'left_eye_data.csv', index_col=0, engine='python')\n",
    "        block.right_eye_data = pd.read_csv(block.analysis_path / 'right_eye_data.csv', index_col=0, engine='python')\n",
    "    except FileNotFoundError:\n",
    "        print('eye_data files not found, run the pipeline!')\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(block.analysis_path / 'rotate_eye_data_params.pkl', 'rb') as file:\n",
    "            rotation_dict = pickle.load(file)\n",
    "            block.left_rotation_matrix = rotation_dict['left_rotation_matrix']\n",
    "            block.right_rotation_matrix = rotation_dict['right_rotation_matrix']\n",
    "            block.left_rotation_angle = rotation_dict['left_rotation_angle']\n",
    "            block.right_rotation_angle = rotation_dict['right_rotation_angle']\n",
    "    except FileNotFoundError:\n",
    "        print('No rotation matrix file, create it')\n",
    "\n",
    "\n",
    "def create_saccade_events_df(eye_data_df, speed_threshold, bokeh_verify_threshold=False, magnitude_calib=1,\n",
    "                             speed_profile=True):\n",
    "    \"\"\"\n",
    "    Detects saccade events in eye tracking data and computes relevant metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - eye_data_df (pd.DataFrame): Input DataFrame containing eye tracking data.\n",
    "    - speed_threshold (float): Threshold for saccade detection based on speed.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): Modified input DataFrame with added columns for speed and saccade detection.\n",
    "    - saccade_events_df (pd.DataFrame): DataFrame containing information about detected saccade events.\n",
    "\n",
    "    Steps:\n",
    "    1. Calculate speed components ('speed_x', 'speed_y') based on differences in 'center_x' and 'center_y'.\n",
    "    2. Compute the magnitude of the velocity vector ('speed_r').\n",
    "    3. Create a binary column ('is_saccade') indicating saccade events based on the speed threshold.\n",
    "    4. Determine saccade onset and offset indices and timestamps.\n",
    "    5. Create a DataFrame ('saccade_events_df') with columns:\n",
    "        - 'saccade_start_ind': Indices of saccade onset.\n",
    "        - 'saccade_start_timestamp': Timestamps corresponding to saccade onset.\n",
    "        - 'saccade_end_ind': Indices of saccade offset.\n",
    "        - 'saccade_end_timestamp': Timestamps corresponding to saccade offset.\n",
    "        - 'length': Duration of each saccade event.\n",
    "    6. Calculate distance traveled and angles for each saccade event.\n",
    "    7. Append additional columns to 'saccade_events_df':\n",
    "        - 'magnitude': Magnitude of the distance traveled during each saccade.\n",
    "        - 'angle': Angle of the saccade vector in degrees.\n",
    "        - 'initial_x', 'initial_y': Initial coordinates of the saccade.\n",
    "        - 'end_x', 'end_y': End coordinates of the saccade.\n",
    "\n",
    "    Note: The original 'eye_data_df' is not modified; modified data is returned as 'df'.\n",
    "    \"\"\"\n",
    "    df = eye_data_df\n",
    "    df['speed_x'] = df['center_x'].diff()  # Difference between consecutive 'center_x' values\n",
    "    df['speed_y'] = df['center_y'].diff()  # Difference between consecutive 'center_y' values\n",
    "\n",
    "    # Step 2: Calculate magnitude of the velocity vector (R vector speed)\n",
    "    df['speed_r'] = (df['speed_x'] ** 2 + df['speed_y'] ** 2) ** 0.5\n",
    "\n",
    "    # Create a column for saccade detection\n",
    "    df['is_saccade'] = df['speed_r'] > speed_threshold\n",
    "\n",
    "    # create a saccade_on_off indicator where 1 is rising edge and -1 is falling edge by subtracting a shifted binary mask\n",
    "    saccade_on_off = df.is_saccade.astype(int) - df.is_saccade.shift(periods=1, fill_value=False).astype(int)\n",
    "    saccade_on_inds = np.where(saccade_on_off == 1)[\n",
    "                          0] - 1  # notice the manual shift here, chosen to include the first (sometimes slower) eye frame, just before saccade threshold crossing\n",
    "    saccade_on_ms = df['ms_axis'].iloc[saccade_on_inds]\n",
    "    saccade_on_timestamps = df['OE_timestamp'].iloc[saccade_on_inds]\n",
    "    saccade_off_inds = np.where(saccade_on_off == -1)[0]\n",
    "    saccade_off_timestamps = df['OE_timestamp'].iloc[saccade_off_inds]\n",
    "    saccade_off_ms = df['ms_axis'].iloc[saccade_off_inds]\n",
    "\n",
    "    saccade_dict = {'saccade_start_ind': saccade_on_inds,\n",
    "                    'saccade_start_timestamp': saccade_on_timestamps.values,\n",
    "                    'saccade_end_ind': saccade_off_inds,\n",
    "                    'saccade_end_timestamp': saccade_off_timestamps.values,\n",
    "                    'saccade_on_ms': saccade_on_ms.values,\n",
    "                    'saccade_off_ms': saccade_off_ms.values}\n",
    "\n",
    "    saccade_events_df = pd.DataFrame.from_dict(saccade_dict)\n",
    "    saccade_events_df['length'] = saccade_events_df['saccade_end_ind'] - saccade_events_df['saccade_start_ind']\n",
    "    # Drop columns used for intermediate steps\n",
    "    df = df.drop(['is_saccade'], axis=1)\n",
    "\n",
    "    distances = []\n",
    "    angles = []\n",
    "    speed_list = []\n",
    "    diameter_list = []\n",
    "    for index, row in tqdm.tqdm(saccade_events_df.iterrows()):\n",
    "        saccade_samples = df.loc[(df['OE_timestamp'] >= row['saccade_start_timestamp']) &\n",
    "                                 (df['OE_timestamp'] <= row['saccade_end_timestamp'])]\n",
    "        distance_traveled = saccade_samples['speed_r'].sum()\n",
    "        if speed_profile:\n",
    "            saccade_speed_profile = saccade_samples['speed_r'].values\n",
    "            speed_list.append(saccade_speed_profile)\n",
    "        saccade_diameter_profile = saccade_samples['pupil_diameter'].values\n",
    "        diameter_list.append(saccade_diameter_profile)\n",
    "        # Calculate angle from initial position to endpoint\n",
    "        initial_position = saccade_samples.iloc[0][['center_x', 'center_y']]\n",
    "        endpoint = saccade_samples.iloc[-1][['center_x', 'center_y']]\n",
    "        overall_angle = np.arctan2(endpoint['center_y'] - initial_position['center_y'],\n",
    "                                   endpoint['center_x'] - initial_position['center_x'])\n",
    "\n",
    "        angles.append(overall_angle)\n",
    "        distances.append(distance_traveled)\n",
    "\n",
    "    saccade_events_df['magnitude_raw'] = np.array(distances)\n",
    "    saccade_events_df['magnitude'] = np.array(distances) * magnitude_calib\n",
    "    saccade_events_df['angle'] = np.where(np.isnan(angles), angles, np.rad2deg(\n",
    "        angles) % 360)  # Convert radians to degrees and ensure result is in [0, 360)\n",
    "    start_ts = saccade_events_df['saccade_start_timestamp'].values\n",
    "    end_ts = saccade_events_df['saccade_end_timestamp'].values\n",
    "    saccade_start_df = df[df['OE_timestamp'].isin(start_ts)]\n",
    "    saccade_end_df = df[df['OE_timestamp'].isin(end_ts)]\n",
    "    start_x_coord = saccade_start_df['center_x']\n",
    "    start_y_coord = saccade_start_df['center_y']\n",
    "    end_x_coord = saccade_end_df['center_x']\n",
    "    end_y_coord = saccade_end_df['center_y']\n",
    "    saccade_events_df['initial_x'] = start_x_coord.values\n",
    "    saccade_events_df['initial_y'] = start_y_coord.values\n",
    "    saccade_events_df['end_x'] = end_x_coord.values\n",
    "    saccade_events_df['end_y'] = end_y_coord.values\n",
    "    saccade_events_df['calib_dx'] = (saccade_events_df['end_x'].values - saccade_events_df[\n",
    "        'initial_x'].values) * magnitude_calib\n",
    "    saccade_events_df['calib_dy'] = (saccade_events_df['end_y'].values - saccade_events_df[\n",
    "        'initial_y'].values) * magnitude_calib\n",
    "    if speed_profile:\n",
    "        saccade_events_df['speed_profile'] = speed_list\n",
    "    saccade_events_df['diameter_profile'] = diameter_list\n",
    "    if bokeh_verify_threshold:\n",
    "        bokeh_plotter(data_list=[df.speed_r], label_list=['Pupil Velocity'], peaks=saccade_on_inds)\n",
    "\n",
    "    return df, saccade_events_df\n",
    "\n",
    "\n",
    "# create a multi-animal block_collection:\n",
    "\n",
    "def create_block_collections(animals, block_lists, experiment_path, bad_blocks=None):\n",
    "    \"\"\"\n",
    "    Create block collections and a block dictionary from multiple animals and their respective block lists.\n",
    "\n",
    "    Parameters:\n",
    "    - animals: list of str, names of the animals.\n",
    "    - block_lists: list of lists of int, block numbers corresponding to each animal.\n",
    "    - experiment_path: pathlib.Path, path to the experiment directory.\n",
    "    - bad_blocks: list of int, blocks to exclude. Default is an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - block_collection: list of BlockSync objects for all specified blocks.\n",
    "    - block_dict: dictionary where keys are block numbers as strings and values are BlockSync objects.\n",
    "    \"\"\"\n",
    "    import UtilityFunctions_newOE as uf\n",
    "\n",
    "    if bad_blocks is None:\n",
    "        bad_blocks = []\n",
    "\n",
    "    block_collection = []\n",
    "    block_dict = {}\n",
    "\n",
    "    for animal, blocks in zip(animals, block_lists):\n",
    "        # Generate blocks for the current animal\n",
    "        current_blocks = uf.block_generator(\n",
    "            block_numbers=blocks,\n",
    "            experiment_path=experiment_path,\n",
    "            animal=animal,\n",
    "            bad_blocks=bad_blocks\n",
    "        )\n",
    "        # Add to collection and dictionary\n",
    "        block_collection.extend(current_blocks)\n",
    "        for b in current_blocks:\n",
    "            block_dict[f\"{animal}_block_{b.block_num}\"] = b\n",
    "\n",
    "    return block_collection, block_dict\n"
   ],
   "id": "4ed4ac1dba16d73b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# BLOCK DEFINITION #\n",
    "# This was the previous run\n",
    "#animals = ['PV_62', 'PV_126', 'PV_57']\n",
    "#block_lists = [[24, 26, 38], [7, 8, 9, 10, 11, 12], [7, 8, 9, 12, 13]]\n",
    "#This with new animals:\n",
    "animals = ['PV_126']\n",
    "block_lists = [[7]]\n",
    "experiment_path = pathlib.Path(r\"Z:\\Nimrod\\experiments\")\n",
    "bad_blocks = [0]  # Example of bad blocks\n",
    "\n",
    "block_collection, block_dict = create_block_collections(\n",
    "    animals=animals,\n",
    "    block_lists=block_lists,\n",
    "    experiment_path=experiment_path,\n",
    "    bad_blocks=bad_blocks\n",
    ")\n",
    "for block in block_collection:\n",
    "    block.parse_open_ephys_events()\n",
    "    block.get_eye_brightness_vectors()\n",
    "    block.synchronize_block()\n",
    "    block.create_eye_brightness_df(threshold_value=20)\n",
    "\n",
    "    # if the code fails here, go to manual synchronization\n",
    "    block.import_manual_sync_df()\n",
    "    block.read_dlc_data()\n",
    "    block.calibrate_pixel_size(10)\n",
    "    #load_eye_data_2d_w_rotation_matrix(block) #should be integrated again... later\n",
    "\n",
    "    for block in block_collection:\n",
    "        # block.left_eye_data = pd.read_csv(block.analysis_path / f'left_eye_data_corr_angles.csv')\n",
    "        # block.right_eye_data = pd.read_csv(block.analysis_path / 'right_eye_data_corr_angles.csv')\n",
    "        #block.left_eye_data = pd.read_csv(block.analysis_path / f'left_eye_data_degrees_raw_xflipped.csv')\n",
    "        #block.right_eye_data = pd.read_csv(block.analysis_path / 'right_eye_data_degrees_raw_xflipped.csv')\n",
    "        block.left_eye_data = pd.read_csv(block.analysis_path / f'left_eye_data_degrees_raw_verified.csv')\n",
    "        block.right_eye_data = pd.read_csv(block.analysis_path / 'right_eye_data_degrees_raw_verified.csv')\n",
    "        # block.left_eye_data = pd.read_csv(block.analysis_path / f'left_eye_data_3d_corr_verified.csv')\n",
    "        # block.right_eye_data = pd.read_csv(block.analysis_path / 'right_eye_data_3d_corr_verified.csv')\n",
    "        #block.left_eye_data = pd.read_csv(block.analysis_path / f'left_eye_data_degrees_rotated_verified.csv')\n",
    "        #block.right_eye_data = pd.read_csv(block.analysis_path / 'right_eye_data_degrees_rotated_verified.csv')\n",
    "\n",
    "    # calibrate pupil diameter:\n",
    "    # if 'pupil_diameter' not in block.left_eye_data.columns:\n",
    "    #     block.left_eye_data['pupil_diameter_pixels'] = block.left_eye_data.major_ax * 2 * np.pi\n",
    "    #     block.right_eye_data['pupil_diameter_pixels'] = block.right_eye_data.major_ax * 2 * np.pi\n",
    "    #     block.left_eye_data['pupil_diameter'] = block.left_eye_data['pupil_diameter_pixels'] * block.L_pix_size\n",
    "    #     block.right_eye_data['pupil_diameter'] = block.right_eye_data['pupil_diameter_pixels'] * block.R_pix_size"
   ],
   "id": "700605717d436f4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for block in block_collection:\n",
    "    if 'pupil_diameter' not in block.left_eye_data.columns:\n",
    "        print(f'calculating pupil diameter for {block} ')\n",
    "        block.left_eye_data['pupil_diameter_pixels'] = block.left_eye_data.major_ax\n",
    "        block.right_eye_data['pupil_diameter_pixels'] = block.right_eye_data.major_ax\n",
    "        block.left_eye_data['pupil_diameter'] = block.left_eye_data['pupil_diameter_pixels'] * block.L_pix_size\n",
    "        block.right_eye_data['pupil_diameter'] = block.right_eye_data['pupil_diameter_pixels'] * block.R_pix_size"
   ],
   "id": "f963c5f72c1638f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This cell defines functions for manual annotation of extreme datapoints\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Sequence, Union, List, Dict\n",
    "\n",
    "\n",
    "# ---------------------------- helpers: contiguous runs on integer index ----------------------------\n",
    "def _runs_from_index(int_index: np.ndarray, min_run_len: int = 1, max_gap: int = 1) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    From a sorted array of integer indices, return [(start_idx, end_idx)] for contiguous runs.\n",
    "    max_gap=1 means consecutive indices (diff == 1) are one run; larger max_gap stitches small breaks.\n",
    "    \"\"\"\n",
    "    if int_index.size == 0:\n",
    "        return []\n",
    "    diffs = np.diff(int_index)\n",
    "    # A new run begins when the gap exceeds max_gap\n",
    "    boundaries = np.where(diffs > max_gap)[0]\n",
    "    starts = np.r_[0, boundaries + 1]\n",
    "    ends = np.r_[boundaries, len(int_index) - 1]\n",
    "    runs = [(int(int_index[s]), int(int_index[e])) for s, e in zip(starts, ends)]\n",
    "    if min_run_len > 1:\n",
    "        runs = [r for r in runs if (r[1] - r[0] + 1) >= min_run_len]\n",
    "    return runs\n",
    "\n",
    "\n",
    "def _pad_ms_bounds(df: pd.DataFrame, start_ms: float, end_ms: float, pre_pad_ms: float, post_pad_ms: float) -> Tuple[\n",
    "    float, float]:\n",
    "    if \"ms_axis\" not in df.columns:\n",
    "        return start_ms, end_ms\n",
    "    ms = df[\"ms_axis\"].values\n",
    "    s = start_ms - float(pre_pad_ms)\n",
    "    e = end_ms + float(post_pad_ms)\n",
    "    # clip to recording bounds if ms_axis is monotonic (typical)\n",
    "    if ms.size:\n",
    "        s = max(min(ms[0], ms[-1]), min(s, max(ms[0], ms[-1])))\n",
    "        e = max(min(ms[0], ms[-1]), min(e, max(ms[0], ms[-1])))\n",
    "    return float(s), float(e)\n",
    "\n",
    "\n",
    "# ---------------------------- main: query -> events df ----------------------------\n",
    "def query_to_events_df(\n",
    "        eye_df: pd.DataFrame,\n",
    "        query_str: str,\n",
    "        *,\n",
    "        animal: str,\n",
    "        block: Union[int, str],\n",
    "        eye: str,\n",
    "        pre_pad_ms: float = 0.0,\n",
    "        post_pad_ms: float = 0.0,\n",
    "        min_run_len: int = 1,\n",
    "        max_gap: int = 1,  # stitch small holes between hits (in index units)\n",
    "        sort_by_start: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Turn a DataFrame slice via df.query(...) into lumped contiguous events for manual review.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Contiguity is defined on the *DataFrame index* (after filtering).\n",
    "    - start_ms/end_ms come from the first/last row's `ms_axis`. If ms_axis absent, it falls back to row index.\n",
    "    - pre/post padding applied in ms (if ms_axis present; otherwise ignored).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame with: ['animal','block','eye','start_ms','end_ms']\n",
    "    \"\"\"\n",
    "    if eye not in (\"L\", \"R\", \"left\", \"right\"):\n",
    "        raise ValueError(\"eye should be 'L'/'R' (or 'left'/'right').\")\n",
    "\n",
    "    eye_short = \"L\" if eye.lower().startswith(\"l\") else \"R\"\n",
    "\n",
    "    # Filter via query (robust to @-locals by not injecting variables here)\n",
    "    try:\n",
    "        sub = eye_df.query(query_str)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"query failed: {e}\")\n",
    "\n",
    "    if sub.empty:\n",
    "        return pd.DataFrame(columns=[\"animal\", \"block\", \"eye\", \"start_ms\", \"end_ms\"])\n",
    "\n",
    "    # Ensure index is integer-like (use position if not)\n",
    "    if not np.issubdtype(sub.index.dtype, np.integer):\n",
    "        # use original positional indices of eye_df for contiguity\n",
    "        pos_idx = eye_df.index.get_indexer(sub.index)\n",
    "        valid = pos_idx >= 0\n",
    "        int_idx = pos_idx[valid]\n",
    "        # align sub to valid subset\n",
    "        sub = sub.iloc[np.where(valid)[0]]\n",
    "    else:\n",
    "        int_idx = sub.index.values\n",
    "\n",
    "    int_idx = np.asarray(int_idx, dtype=int)\n",
    "    runs = _runs_from_index(np.sort(int_idx), min_run_len=min_run_len, max_gap=max_gap)\n",
    "\n",
    "    rows = []\n",
    "    for i0, i1 in runs:\n",
    "        # fetch start/end rows (by index label)\n",
    "        try:\n",
    "            r0 = eye_df.loc[i0]\n",
    "            r1 = eye_df.loc[i1]\n",
    "        except KeyError:\n",
    "            # if label-based lookup fails (non-unique or non-monotonic), fallback to iloc around positions\n",
    "            # This is rare, but keeps the function robust.\n",
    "            r0 = eye_df.iloc[i0] if (0 <= i0 < len(eye_df)) else sub.iloc[0]\n",
    "            r1 = eye_df.iloc[i1] if (0 <= i1 < len(eye_df)) else sub.iloc[-1]\n",
    "\n",
    "        if \"ms_axis\" in eye_df.columns:\n",
    "            s_ms = float(r0[\"ms_axis\"])\n",
    "            e_ms = float(r1[\"ms_axis\"])\n",
    "        else:\n",
    "            # fall back to treating index as a proxy time\n",
    "            s_ms = float(i0)\n",
    "            e_ms = float(i1)\n",
    "\n",
    "        s_ms, e_ms = _pad_ms_bounds(eye_df, s_ms, e_ms, pre_pad_ms, post_pad_ms)\n",
    "        if e_ms <= s_ms:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"animal\": str(animal),\n",
    "            \"block\": str(block),\n",
    "            \"eye\": eye_short,\n",
    "            \"start_ms\": s_ms,\n",
    "            \"end_ms\": e_ms,\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    if sort_by_start and not out.empty:\n",
    "        out = out.sort_values([\"animal\", \"block\", \"eye\", \"start_ms\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------- convenience: threshold -> events df ----------------------------\n",
    "def threshold_to_events_df(\n",
    "        eye_df: pd.DataFrame,\n",
    "        column: str,\n",
    "        *,\n",
    "        op: str,  # one of: '>', '>=', '<', '<=', '==', '!=', 'abs>'\n",
    "        value: float,\n",
    "        animal: str,\n",
    "        block: Union[int, str],\n",
    "        eye: str,\n",
    "        pre_pad_ms: float = 0.0,\n",
    "        post_pad_ms: float = 0.0,\n",
    "        min_run_len: int = 1,\n",
    "        max_gap: int = 1,\n",
    "        sort_by_start: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build events by thresholding a single column without writing a query string.\n",
    "    Examples:\n",
    "        threshold_to_events_df(df, 'k_theta', op='<', value=-50, animal='PV_126', block=7, eye='L')\n",
    "        threshold_to_events_df(df, 'speed_r', op='abs>', value=2.0, ...)\n",
    "    \"\"\"\n",
    "    if column not in eye_df.columns:\n",
    "        raise ValueError(f\"column '{column}' not found in DataFrame.\")\n",
    "\n",
    "    if op == 'abs>':\n",
    "        q = f\"abs({column}) > @value\"\n",
    "    elif op in ('>', '>=', '<', '<=', '==', '!='):\n",
    "        q = f\"{column} {op} @value\"\n",
    "    else:\n",
    "        raise ValueError(\"op must be one of {'>','>=','<','<=','==','!=','abs>'}\")\n",
    "\n",
    "    return query_to_events_df(\n",
    "        eye_df, q,\n",
    "        animal=animal, block=block, eye=eye,\n",
    "        pre_pad_ms=pre_pad_ms, post_pad_ms=post_pad_ms,\n",
    "        min_run_len=min_run_len, max_gap=max_gap, sort_by_start=sort_by_start\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------- per-block CSV IO ----------------------------\n",
    "CSV_NAME = \"manual_event_annotations.csv\"  # one file per block in its analysis folder\n",
    "TAG_COL = \"manual_outlier_detected\"  # boolean (object-dtype), None if untagged\n",
    "TS_COL = \"annotation_timestamp\"  # 'YYYY_MM_DD_HH_MM'\n",
    "\n",
    "\n",
    "def _now_stamp() -> str:\n",
    "    # YYYY_MM_DD_HH_MM (local system time)\n",
    "    return pd.Timestamp.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "\n",
    "\n",
    "def _block_for_key(block_dict: Dict[str, object], animal: str, block_num: Union[str, int]):\n",
    "    # your block_dict has various keys; be permissive\n",
    "    for obj in block_dict.values():\n",
    "        if getattr(obj, \"animal_call\", None) == animal and str(getattr(obj, \"block_num\", None)) == str(block_num):\n",
    "            return obj\n",
    "    # try direct keys e.g. \"PV_126_block_7\"\n",
    "    key1 = f\"{animal}_block_{int(block_num)}\"\n",
    "    key2 = f\"{animal}_block_{int(block_num):03d}\"\n",
    "    if key1 in block_dict: return block_dict[key1]\n",
    "    if key2 in block_dict: return block_dict[key2]\n",
    "    raise KeyError(f\"BlockSync not found for animal='{animal}', block='{block_num}'\")\n",
    "\n",
    "\n",
    "def _ann_path_for_block(bs) -> Path:\n",
    "    p = Path(getattr(bs, \"analysis_path\"))\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p / CSV_NAME\n",
    "\n",
    "\n",
    "def load_block_annotations(bs: object) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load per-block annotation CSV if exists. Returns standardized df:\n",
    "    ['animal_call','block','eye','start_ms','end_ms','manual_outlier_detected','annotation_timestamp']\n",
    "    \"\"\"\n",
    "    path = _ann_path_for_block(bs)\n",
    "    cols = ['animal_call', 'block', 'eye', 'start_ms', 'end_ms', TAG_COL, TS_COL]\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame(columns=cols)\n",
    "    df = pd.read_csv(path)\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    df = df[cols].copy()\n",
    "    # normalize dtypes\n",
    "    df['animal_call'] = df['animal_call'].astype('string')\n",
    "    df['block'] = df['block'].astype('string')\n",
    "    df['eye'] = df['eye'].astype('string')\n",
    "    df['start_ms'] = pd.to_numeric(df['start_ms'], errors='coerce').astype(float)\n",
    "    df['end_ms'] = pd.to_numeric(df['end_ms'], errors='coerce').astype(float)\n",
    "\n",
    "    # boolean-as-object tri-state\n",
    "    def _coerce_tag(v):\n",
    "        if v is None or (isinstance(v, float) and np.isnan(v)): return None\n",
    "        if isinstance(v, bool): return v\n",
    "        s = str(v).strip().lower()\n",
    "        if s in (\"true\", \"t\", \"1\", \"yes\", \"y\"): return True\n",
    "        if s in (\"false\", \"f\", \"0\", \"no\", \"n\"): return False\n",
    "        return None\n",
    "\n",
    "    df[TAG_COL] = df[TAG_COL].map(_coerce_tag).astype('object')\n",
    "    df[TS_COL] = df[TS_COL].astype('string')\n",
    "    return df\n",
    "\n",
    "\n",
    "def _merge_annotations(old_df: pd.DataFrame, new_df: pd.DataFrame, overwrite: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge on (animal_call, block, eye, rounded start_ms, rounded end_ms).\n",
    "    If overwrite=True, newer rows (from new_df) win.\n",
    "    \"\"\"\n",
    "    if old_df is None or old_df.empty:\n",
    "        return new_df.copy()\n",
    "\n",
    "    out = pd.concat([old_df, new_df], ignore_index=True)\n",
    "    # create integer keys for robust equality\n",
    "    out['_s'] = np.rint(out['start_ms']).astype('Int64')\n",
    "    out['_e'] = np.rint(out['end_ms']).astype('Int64')\n",
    "    out['__ord__'] = out.index if overwrite else -out.index\n",
    "    out = (out.sort_values(['animal_call', 'block', 'eye', '_s', '_e', '__ord__'])\n",
    "           .drop_duplicates(['animal_call', 'block', 'eye', '_s', '_e'], keep='last')\n",
    "           .drop(columns=['_s', '_e', '__ord__']))\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _write_block_annotations(bs: object, df_block: pd.DataFrame, overwrite: bool = True) -> Path:\n",
    "    \"\"\"\n",
    "    Write/merge df_block to that block's CSV.\n",
    "    df_block must already be filtered to one (animal_call, block).\n",
    "    \"\"\"\n",
    "    path = _ann_path_for_block(bs)\n",
    "    existing = load_block_annotations(bs)\n",
    "    final = _merge_annotations(existing, df_block, overwrite=overwrite)\n",
    "    final.to_csv(path, index=False)\n",
    "    return path\n",
    "\n",
    "\n",
    "# ---------------------------- UPDATED reviewer (per-block IO) ----------------------------\n",
    "def review_events_multi_with_arena_v2(\n",
    "        block_dict: Dict[str, object],\n",
    "        events_subset_df: pd.DataFrame,\n",
    "        *,\n",
    "        animal_col: str = \"animal\",\n",
    "        block_col: str = \"block\",\n",
    "        eye_col: str = \"eye\",\n",
    "        start_ms_col: Optional[str] = None,  # auto-detect from {'start_ms','saccade_on_ms'}\n",
    "        end_ms_col: Optional[str] = None,  # auto-detect from {'end_ms','saccade_off_ms'}\n",
    "        window_scale: float = 0.85,\n",
    "        text_cols: Tuple[str, ...] = (\"phi\", \"theta\", \"peak_velocity\", \"magnitude_raw_angular\", \"pupil_diameter\"),\n",
    "        font_scale: float = 0.6,\n",
    "        thickness: int = 2,\n",
    "        wait_ms: int = 15,\n",
    "        flip_mode: str = \"vertical\",\n",
    "        ms_tolerance: float = 0.0,  # for preloading within tolerance\n",
    "        overwrite_existing: bool = True,\n",
    "        show_arena: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Like your current reviewer, but:\n",
    "      • Preloads per-block CSVs (manual_event_annotations.csv) from each block's analysis folder.\n",
    "      • Displays the preloaded GOOD/BAD state right away.\n",
    "      • On export (or on-the-fly tagging), writes one CSV per block, merging safely.\n",
    "      • Adds/updates 'annotation_timestamp' (YYYY_MM_DD_HH_MM) whenever a tag is changed.\n",
    "\n",
    "    Controls / shortcuts remain the same:\n",
    "      mark_bad / mark_good, export_annotated_df, Play/Pause, Prev/Next, step, Arena Switch, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- inner utils largely reused from your original (trimmed to only the deltas we need) ----\n",
    "    def _resolve_time_cols(df: pd.DataFrame, start_c: Optional[str], end_c: Optional[str]) -> Tuple[str, str]:\n",
    "        cand_start = [start_c, \"start_ms\", \"saccade_on_ms\"]\n",
    "        cand_end = [end_c, \"end_ms\", \"saccade_off_ms\"]\n",
    "        s = next((c for c in cand_start if c and c in df.columns), None)\n",
    "        e = next((c for c in cand_end if c and c in df.columns), None)\n",
    "        if s is None or e is None:\n",
    "            raise ValueError(\"Could not resolve start/end ms columns.\")\n",
    "        return s, e\n",
    "\n",
    "    # normalize input df\n",
    "    s_col, e_col = _resolve_time_cols(events_subset_df, start_ms_col, end_ms_col)\n",
    "    events = events_subset_df.copy().reset_index(drop=True)\n",
    "\n",
    "    # required cols?\n",
    "    for c in (animal_col, block_col, eye_col, s_col, e_col):\n",
    "        if c not in events.columns:\n",
    "            raise ValueError(f\"events_subset_df is missing required column '{c}'\")\n",
    "\n",
    "    # Working std columns\n",
    "    events[\"_animal_std\"] = events[animal_col].astype(str)\n",
    "    events[\"_block_std\"] = events[block_col].astype(str)\n",
    "    events[\"_eye_std\"] = events[eye_col].astype(str)\n",
    "    events[\"_start_ms_std\"] = pd.to_numeric(events[s_col], errors=\"coerce\").astype(float)\n",
    "    events[\"_end_ms_std\"] = pd.to_numeric(events[e_col], errors=\"coerce\").astype(float)\n",
    "    events[\"_start_key\"] = pd.Series(np.rint(events[\"_start_ms_std\"]).astype(\"Int64\"))\n",
    "    events[\"_end_key\"] = pd.Series(np.rint(events[\"_end_ms_std\"]).astype(\"Int64\"))\n",
    "\n",
    "    # Tri-state annotation col + timestamp\n",
    "    for c in (TAG_COL, TS_COL):\n",
    "        if c not in events.columns:\n",
    "            events[c] = pd.Series([None] * len(events), dtype='object')\n",
    "\n",
    "    def _coerce_tag(v):\n",
    "        if v is None or (isinstance(v, float) and np.isnan(v)): return None\n",
    "        if isinstance(v, bool): return v\n",
    "        s = str(v).strip().lower()\n",
    "        if s in (\"true\", \"t\", \"1\", \"yes\", \"y\"): return True\n",
    "        if s in (\"false\", \"f\", \"0\", \"no\", \"n\"): return False\n",
    "        return None\n",
    "\n",
    "    # ---------------- preload per-block annotations ----------------\n",
    "    # strategy: for each (animal, block), load that block's CSV then match by (eye, start_key, end_key)\n",
    "    # exact match if ms_tolerance==0, else tolerant match\n",
    "    grouped_keys = events.groupby([\"_animal_std\", \"_block_std\"]).indices\n",
    "    filled = 0\n",
    "    for (animal, block), idxs in grouped_keys.items():\n",
    "        bs = _block_for_key(block_dict, animal, block)\n",
    "        old = load_block_annotations(bs)\n",
    "        if old.empty:\n",
    "            continue\n",
    "        old[\"_start_key\"] = pd.Series(np.rint(old[\"start_ms\"]).astype(\"Int64\"))\n",
    "        old[\"_end_key\"] = pd.Series(np.rint(old[\"end_ms\"]).astype(\"Int64\"))\n",
    "\n",
    "        if ms_tolerance <= 0:\n",
    "            lookup = {(str(r.animal_call), str(r.block), str(r.eye), int(r._start_key), int(r._end_key)): (r[TAG_COL],\n",
    "                                                                                                           r[TS_COL])\n",
    "                      for _, r in old.dropna(subset=[\"_start_key\", \"_end_key\"]).iterrows()}\n",
    "            for i in idxs:\n",
    "                sk, ek = events.at[i, \"_start_key\"], events.at[i, \"_end_key\"]\n",
    "                k = (animal, block, str(events.at[i, \"_eye_std\"]), int(sk), int(ek))\n",
    "                if k in lookup and events.at[i, TAG_COL] is None:\n",
    "                    tag, ts = lookup[k]\n",
    "                    events.at[i, TAG_COL] = _coerce_tag(tag)\n",
    "                    events.at[i, TS_COL] = ts if (ts is None or not (isinstance(ts, float) and np.isnan(ts))) else None\n",
    "                    filled += 1\n",
    "        else:\n",
    "            tol = float(ms_tolerance)\n",
    "            for i in idxs:\n",
    "                e_eye = str(events.at[i, \"_eye_std\"])\n",
    "                s0, e0 = events.at[i, \"_start_ms_std\"], events.at[i, \"_end_ms_std\"]\n",
    "                # filter old by same eye\n",
    "                sub_old = old[old[\"eye\"].astype(str) == e_eye]\n",
    "                if sub_old.empty: continue\n",
    "                # look for first within tolerance\n",
    "                hit = sub_old[(sub_old[\"start_ms\"].sub(s0).abs() <= tol) &\n",
    "                              (sub_old[\"end_ms\"].sub(e0).abs() <= tol)]\n",
    "                if not hit.empty and events.at[i, TAG_COL] is None:\n",
    "                    r = hit.iloc[0]\n",
    "                    events.at[i, TAG_COL] = _coerce_tag(r[TAG_COL])\n",
    "                    events.at[i, TS_COL] = r[TS_COL] if pd.notna(r[TS_COL]) else None\n",
    "                    filled += 1\n",
    "    if filled:\n",
    "        print(f\"[preload] Filled {filled} existing annotations from per-block CSVs.\")\n",
    "\n",
    "    # ---------------- OpenCV/UI state identical to your original (with small diffs) ----------------\n",
    "    def _frame_col(df: pd.DataFrame) -> Optional[str]:\n",
    "        for c in (\"eye_frame\", \"frame\", \"frame_idx\", \"video_frame\"):\n",
    "            if c in df.columns: return c\n",
    "        return None\n",
    "\n",
    "    def _lookup_block(animal: str, block_num: str):\n",
    "        return _block_for_key(block_dict, animal, block_num)\n",
    "\n",
    "    def _nearest_row(df: pd.DataFrame, ms: float) -> Optional[pd.Series]:\n",
    "        arr = df[\"ms_axis\"].values\n",
    "        if arr.size == 0: return None\n",
    "        idx = int(np.argmin(np.abs(arr - ms)))\n",
    "        return df.iloc[idx]\n",
    "\n",
    "    def _apply_flip(img: np.ndarray) -> np.ndarray:\n",
    "        return cv2.flip(img, 0) if flip_mode == \"vertical\" else img\n",
    "\n",
    "    # ---- video handles\n",
    "    capL = capR = capA = None\n",
    "    cur_animal = cur_block = None\n",
    "    left_df = right_df = arena_df = None\n",
    "    left_frame_col = right_frame_col = None\n",
    "    arena_frame_col = None\n",
    "    fpsL = fpsR = fpsA = 60.0\n",
    "    Wl = Hl = Wr = Hr = Wa = Ha = 0\n",
    "    arena_idx = 0\n",
    "\n",
    "    def _release_caps():\n",
    "        nonlocal capL, capR, capA\n",
    "        for c in (capL, capR, capA):\n",
    "            try:\n",
    "                if c is not None: c.release()\n",
    "            except Exception:\n",
    "                pass\n",
    "        capL = capR = capA = None\n",
    "\n",
    "    def _resolve_arena_frame_col(df: pd.DataFrame) -> str:\n",
    "        for c in [\"Arena_frame\", \"arena_frame\", \"arena_frames\", \"arena_frame_idx\", \"frame\", \"frame_idx\", \"video_frame\",\n",
    "                  \"arena_idx\"]:\n",
    "            if c in df.columns: return c\n",
    "        raise RuntimeError(\"final_sync_df has no recognizable arena frame column.\")\n",
    "\n",
    "    def _ensure_arena_ms_axis(bs) -> Tuple[pd.DataFrame, str]:\n",
    "        fs = bs.final_sync_df.copy()\n",
    "        fcol = _resolve_arena_frame_col(fs)\n",
    "        if \"ms_axis\" not in fs.columns:\n",
    "            # try joining from eye ms_axis via OE_timestamp\n",
    "            joined = False\n",
    "            for eye_df in [getattr(bs, \"left_eye_data\", None), getattr(bs, \"right_eye_data\", None)]:\n",
    "                if eye_df is not None and \"OE_timestamp\" in fs.columns and \\\n",
    "                        \"OE_timestamp\" in eye_df.columns and \"ms_axis\" in eye_df.columns:\n",
    "                    tmp = eye_df[[\"OE_timestamp\", \"ms_axis\"]].dropna().drop_duplicates(subset=[\"OE_timestamp\"])\n",
    "                    fs = fs.merge(tmp, on=\"OE_timestamp\", how=\"left\")\n",
    "                    joined = True\n",
    "                    break\n",
    "            if not joined:\n",
    "                fs[\"__need_video_time__\"] = True\n",
    "        keep = [c for c in [\"ms_axis\", fcol, \"OE_timestamp\", \"__need_video_time__\"] if c in fs.columns]\n",
    "        out = fs[keep].dropna(subset=[fcol]).copy()\n",
    "        out[fcol] = pd.to_numeric(out[fcol], errors=\"coerce\").astype(\"Int64\")\n",
    "        out = out.dropna(subset=[fcol])\n",
    "        return out, fcol\n",
    "\n",
    "    def _open_for(animal: str, block_num: str):\n",
    "        nonlocal cur_animal, cur_block, left_df, right_df, arena_df\n",
    "        nonlocal left_frame_col, right_frame_col, arena_frame_col\n",
    "        nonlocal capL, capR, capA, fpsL, fpsR, fpsA, Wl, Hl, Wr, Hr, Wa, Ha, arena_idx\n",
    "\n",
    "        if animal == cur_animal and block_num == cur_block:\n",
    "            return\n",
    "\n",
    "        _release_caps()\n",
    "        bs = _lookup_block(animal, block_num)\n",
    "        cur_animal, cur_block = animal, block_num\n",
    "\n",
    "        left_df = getattr(bs, \"left_eye_data\", None)\n",
    "        right_df = getattr(bs, \"right_eye_data\", None)\n",
    "        if left_df is None or right_df is None:\n",
    "            raise RuntimeError(f\"Missing eye data for {animal} B{block_num}\")\n",
    "        if \"ms_axis\" not in left_df.columns or \"ms_axis\" not in right_df.columns:\n",
    "            raise RuntimeError(f\"'ms_axis' missing for {animal} B{block_num}\")\n",
    "        left_frame_col = _frame_col(left_df)\n",
    "        right_frame_col = _frame_col(right_df)\n",
    "\n",
    "        # open videos\n",
    "        lv = Path(bs.le_videos[0]);\n",
    "        rv = Path(bs.re_videos[0])\n",
    "        capL_local = cv2.VideoCapture(str(lv));\n",
    "        capR_local = cv2.VideoCapture(str(rv))\n",
    "        if not capL_local.isOpened(): raise RuntimeError(f\"Cannot open {lv}\")\n",
    "        if not capR_local.isOpened(): raise RuntimeError(f\"Cannot open {rv}\")\n",
    "        capL, capR = capL_local, capR_local\n",
    "        Wl, Hl = int(capL.get(cv2.CAP_PROP_FRAME_WIDTH)), int(capL.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        Wr, Hr = int(capR.get(cv2.CAP_PROP_FRAME_WIDTH)), int(capR.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fpsL = capL.get(cv2.CAP_PROP_FPS) or 60.0\n",
    "        fpsR = capR.get(cv2.CAP_PROP_FPS) or 60.0\n",
    "\n",
    "        # Arena\n",
    "        if show_arena and getattr(bs, \"arena_videos\", None):\n",
    "            arena_df_local, arena_frame_col_local = _ensure_arena_ms_axis(bs)\n",
    "            arena_df = arena_df_local\n",
    "            arena_frame_col = arena_frame_col_local\n",
    "            arena_idx = max(0, min(arena_idx, len(bs.arena_videos) - 1))\n",
    "            av = Path(bs.arena_videos[arena_idx])\n",
    "            capA_local = cv2.VideoCapture(str(av))\n",
    "            if capA_local.isOpened():\n",
    "                capA = capA_local\n",
    "                Wa, Ha = int(capA.get(cv2.CAP_PROP_FRAME_WIDTH)), int(capA.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                fpsA = capA.get(cv2.CAP_PROP_FPS) or 60.0\n",
    "                if \"__need_video_time__\" in arena_df.columns:\n",
    "                    arena_df[\"ms_axis\"] = (arena_df[arena_frame_col].astype(float) / float(fpsA)) * 1000.0\n",
    "                    arena_df = arena_df.drop(columns=[\"__need_video_time__\"], errors=\"ignore\")\n",
    "            else:\n",
    "                print(f\"[arena] Cannot open: {av}\")\n",
    "        else:\n",
    "            arena_df = None\n",
    "\n",
    "    def _seek(cap, idx: int):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, max(0, int(idx)))\n",
    "\n",
    "    def _read(cap):\n",
    "        ret, f = cap.read()\n",
    "        return f if ret else None\n",
    "\n",
    "    def _overlay_text(img, lines: List[str], origin=(10, 24), vstep=22, color=(255, 255, 255)):\n",
    "        x, y = origin\n",
    "        for ln in lines:\n",
    "            cv2.putText(img, ln, (x, y), cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness, cv2.LINE_AA)\n",
    "            y += vstep\n",
    "\n",
    "    def _overlay_ellipse(img, df_eye: Optional[pd.DataFrame], frame_idx: Optional[int]):\n",
    "        if df_eye is None or frame_idx is None: return\n",
    "        col = _frame_col(df_eye)\n",
    "        if col is None: return\n",
    "        hit = df_eye[df_eye[col] == frame_idx]\n",
    "        if hit.empty: return\n",
    "        row = hit.iloc[0]\n",
    "        cx, cy = row.get(\"center_x\", np.nan), row.get(\"center_y\", np.nan)\n",
    "        w, h = row.get(\"width\", np.nan), row.get(\"height\", np.nan)\n",
    "        phi = row.get(\"phi\", np.nan)\n",
    "        if not (pd.isna(cx) or pd.isna(cy) or pd.isna(w) or pd.isna(h)):\n",
    "            cv2.ellipse(\n",
    "                img,\n",
    "                (int(round(cx)), int(round(cy))),\n",
    "                (max(1, int(round(w))), max(1, int(round(h)))),\n",
    "                float(0 if pd.isna(phi) else phi),\n",
    "                0, 360, (0, 255, 0), thickness\n",
    "            )\n",
    "\n",
    "    # display geometry\n",
    "    disp_Wl = disp_Hl = disp_Wr = disp_Hr = disp_Wa = disp_Ha = 0\n",
    "\n",
    "    # controls\n",
    "    ctrl_w, ctrl_h = 540, 420\n",
    "    buttons = {\n",
    "        \"Play\": ((10, 10), (260, 60)),\n",
    "        \"Pause\": ((280, 10), (530, 60)),\n",
    "        \"Prev\": ((10, 80), (260, 130)),\n",
    "        \"Next\": ((280, 80), (530, 130)),\n",
    "        \"Step -1\": ((10, 150), (260, 200)),\n",
    "        \"Step +1\": ((280, 150), (530, 200)),\n",
    "        \"mark_bad\": ((10, 220), (260, 270)),\n",
    "        \"mark_good\": ((280, 220), (530, 270)),\n",
    "        \"Arena Switch\": ((10, 290), (260, 340)),\n",
    "        \"export_annotated_df\": ((280, 290), (530, 340)),\n",
    "    }\n",
    "\n",
    "    COLOR_BG = (60, 60, 60)\n",
    "    COLOR_BORDER = (180, 180, 180)\n",
    "    COLOR_TEXT = (220, 220, 220)\n",
    "    COLOR_BAD = (0, 0, 255)\n",
    "    COLOR_GOOD = (0, 255, 0)\n",
    "    COLOR_EXPORT = (0, 165, 255)\n",
    "    COLOR_INFO = (180, 255, 180)\n",
    "\n",
    "    last_status = \"\"\n",
    "\n",
    "    def _draw_controls(idx: int, current_arena_name: Optional[str]) -> np.ndarray:\n",
    "        img = np.zeros((ctrl_h, ctrl_w, 3), dtype=np.uint8)\n",
    "        state = events[TAG_COL].iloc[idx]\n",
    "        state_str = \"UNSET\" if (state is None or (isinstance(state, float) and np.isnan(state))) else (\n",
    "            \"BAD\" if bool(state) else \"GOOD\")\n",
    "        header = f\"Event {idx + 1}/{len(events)} | state={state_str}\"\n",
    "        cv2.putText(img, header, (10, ctrl_h - 12), cv2.FONT_HERSHEY_SIMPLEX, 0.6, COLOR_TEXT, 1, cv2.LINE_AA)\n",
    "\n",
    "        for name, ((x1, y1), (x2, y2)) in buttons.items():\n",
    "            fill = COLOR_BG\n",
    "            if name == \"mark_bad\" and (state is True):  fill = COLOR_BAD\n",
    "            if name == \"mark_good\" and (state is False): fill = COLOR_GOOD\n",
    "            if name == \"export_annotated_df\":            fill = COLOR_EXPORT\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), fill, -1)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), COLOR_BORDER, 2)\n",
    "            text_size, _ = cv2.getTextSize(name, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "            tx = x1 + (x2 - x1 - text_size[0]) // 2;\n",
    "            ty = y1 + (y2 - y1 + text_size[1]) // 2\n",
    "            cv2.putText(img, name, (tx, ty), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        if last_status:\n",
    "            cv2.putText(img, last_status, (10, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.55, COLOR_INFO, 1, cv2.LINE_AA)\n",
    "\n",
    "        if current_arena_name:\n",
    "            cv2.putText(img, f\"Arena: {current_arena_name}\", (10, 44), cv2.FONT_HERSHEY_SIMPLEX, 0.55, COLOR_INFO, 1,\n",
    "                        cv2.LINE_AA)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _hit_button(x, y):\n",
    "        for name, ((x1, y1), (x2, y2)) in buttons.items():\n",
    "            if x1 <= x <= x2 and y1 <= y <= y2: return name\n",
    "        return None\n",
    "\n",
    "    def _export_per_block():\n",
    "        \"\"\"Split current in-memory annotations by (animal, block) and write per-block CSVs with merge.\"\"\"\n",
    "        nonlocal last_status\n",
    "        # Build a lean df with final schema for writing:\n",
    "        out = pd.DataFrame({\n",
    "            \"animal_call\": events[\"_animal_std\"].astype(str),\n",
    "            \"block\": events[\"_block_std\"].astype(str),\n",
    "            \"eye\": events[\"_eye_std\"].astype(str),\n",
    "            \"start_ms\": events[\"_start_ms_std\"].astype(float),\n",
    "            \"end_ms\": events[\"_end_ms_std\"].astype(float),\n",
    "            TAG_COL: events[TAG_COL].map(_coerce_tag).astype(\"object\"),\n",
    "            TS_COL: events[TS_COL].astype('string')\n",
    "        })\n",
    "        # Partition and write\n",
    "        written = []\n",
    "        for (animal, block), sub in out.groupby([\"animal_call\", \"block\"], dropna=False):\n",
    "            bs = _block_for_key(block_dict, animal, block)\n",
    "            path = _write_block_annotations(bs, sub.copy(), overwrite=overwrite_existing)\n",
    "            written.append(str(path))\n",
    "        last_status = f\"Exported to {len(written)} block file(s).\"\n",
    "\n",
    "    # window setup\n",
    "    cv2.namedWindow(\"Controls\", cv2.WINDOW_NORMAL)\n",
    "    cv2.namedWindow(\"Left Eye\", cv2.WINDOW_NORMAL)\n",
    "    cv2.namedWindow(\"Right Eye\", cv2.WINDOW_NORMAL)\n",
    "    if show_arena:\n",
    "        cv2.namedWindow(\"Arena\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "    playing = False\n",
    "    quit_flag = False\n",
    "    cur_idx = 0\n",
    "    cur_ms = None\n",
    "    step_ms = 1000.0 / 60.0\n",
    "\n",
    "    def _median_step_ms(df: pd.DataFrame) -> float:\n",
    "        if df is None or df.empty: return 1000.0 / 60.0\n",
    "        ms = df[\"ms_axis\"].values\n",
    "        if ms.size < 2: return 1000.0 / 60.0\n",
    "        d = np.diff(ms);\n",
    "        d = d[np.isfinite(d) & (d > 0)]\n",
    "        return float(np.median(d)) if d.size else 1000.0 / 60.0\n",
    "\n",
    "    def _current_arena_name(bs) -> Optional[str]:\n",
    "        try:\n",
    "            if show_arena and getattr(bs, \"arena_videos\", None):\n",
    "                return Path(bs.arena_videos[arena_idx]).name\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "    controls_img = _draw_controls(cur_idx, None)\n",
    "    cv2.imshow(\"Controls\", controls_img)\n",
    "\n",
    "    def on_mouse_controls(event, x, y, flags, param):\n",
    "        nonlocal playing, cur_idx, controls_img, cur_ms, last_status, arena_idx\n",
    "        if event != cv2.EVENT_LBUTTONDOWN:\n",
    "            return\n",
    "        name = _hit_button(x, y)\n",
    "        last_status = \"\"\n",
    "        if name == \"Play\":\n",
    "            playing = True\n",
    "        elif name == \"Pause\":\n",
    "            playing = False\n",
    "        elif name == \"Prev\":\n",
    "            playing = False;\n",
    "            cur_idx = (cur_idx - 1) % len(events);\n",
    "            cur_ms = None\n",
    "        elif name == \"Next\":\n",
    "            playing = False;\n",
    "            cur_idx = (cur_idx + 1) % len(events);\n",
    "            cur_ms = None\n",
    "        elif name == \"Step -1\":\n",
    "            playing = False;\n",
    "            cur_ms = None if cur_ms is None else cur_ms - step_ms\n",
    "        elif name == \"Step +1\":\n",
    "            playing = False;\n",
    "            cur_ms = None if cur_ms is None else cur_ms + step_ms\n",
    "        elif name == \"mark_bad\":\n",
    "            events.at[events.index[cur_idx], TAG_COL] = True\n",
    "            events.at[events.index[cur_idx], TS_COL] = _now_stamp()\n",
    "        elif name == \"mark_good\":\n",
    "            events.at[events.index[cur_idx], TAG_COL] = False\n",
    "            events.at[events.index[cur_idx], TS_COL] = _now_stamp()\n",
    "        elif name == \"Arena Switch\":\n",
    "            bs = _lookup_block(str(events.loc[cur_idx, \"_animal_std\"]), str(events.loc[cur_idx, \"_block_std\"]))\n",
    "            if show_arena and getattr(bs, \"arena_videos\", None):\n",
    "                arena_idx = (arena_idx + 1) % len(bs.arena_videos)\n",
    "                if capA is not None: capA.release()\n",
    "                av = Path(bs.arena_videos[arena_idx])\n",
    "                capA_local = cv2.VideoCapture(str(av))\n",
    "                if capA_local.isOpened():\n",
    "                    globals()['capA'] = capA_local\n",
    "                    last_status = f\"[arena] Showing: {av.name}\"\n",
    "                else:\n",
    "                    last_status = f\"[arena] Cannot open: {av.name}\"\n",
    "        elif name == \"export_annotated_df\":\n",
    "            _export_per_block()\n",
    "\n",
    "        bs = _lookup_block(str(events.loc[cur_idx, \"_animal_std\"]), str(events.loc[cur_idx, \"_block_std\"]))\n",
    "        controls_img = _draw_controls(cur_idx, _current_arena_name(bs))\n",
    "        cv2.imshow(\"Controls\", controls_img)\n",
    "\n",
    "    cv2.setMouseCallback(\"Controls\", on_mouse_controls)\n",
    "\n",
    "    # keyboard shortcuts mirror your originals\n",
    "    while True:\n",
    "        k = cv2.waitKey(wait_ms) & 0xFF\n",
    "        if k in (27, ord('q'), ord('Q')):\n",
    "            quit_flag = True\n",
    "        elif k == 32:\n",
    "            playing = not playing\n",
    "        elif k == ord('['):\n",
    "            playing = False; cur_idx = (cur_idx - 1) % len(events); cur_ms = None\n",
    "        elif k == ord(']'):\n",
    "            playing = False; cur_idx = (cur_idx + 1) % len(events); cur_ms = None\n",
    "        elif k == ord(','):\n",
    "            playing = False; cur_ms = None if cur_ms is None else cur_ms - step_ms\n",
    "        elif k == ord('.'):\n",
    "            playing = False; cur_ms = None if cur_ms is None else cur_ms + step_ms\n",
    "        elif k in (ord('b'), ord('B')):\n",
    "            events.at[events.index[cur_idx], TAG_COL] = True;  events.at[events.index[cur_idx], TS_COL] = _now_stamp()\n",
    "        elif k in (ord('g'), ord('G')):\n",
    "            events.at[events.index[cur_idx], TAG_COL] = False; events.at[events.index[cur_idx], TS_COL] = _now_stamp()\n",
    "        elif k in (ord('e'), ord('E')):\n",
    "            _export_per_block()\n",
    "        elif k in (ord('a'), ord('A')):\n",
    "            on_mouse_controls(cv2.EVENT_LBUTTONDOWN, *buttons[\"Arena Switch\"][0], None, None)\n",
    "\n",
    "        if quit_flag:\n",
    "            break\n",
    "\n",
    "        row = events.iloc[cur_idx]\n",
    "        animal = str(row[\"_animal_std\"]);\n",
    "        block_num = str(row[\"_block_std\"])\n",
    "        start_ms = float(row[\"_start_ms_std\"]);\n",
    "        end_ms = float(row[\"_end_ms_std\"])\n",
    "\n",
    "        _open_for(animal, block_num)\n",
    "\n",
    "        # first-time sizing\n",
    "        if disp_Wl == 0:\n",
    "            disp_Wl, disp_Hl = int(Wl * window_scale), int(Hl * window_scale)\n",
    "            disp_Wr, disp_Hr = int(Wr * window_scale), int(Hr * window_scale)\n",
    "            cv2.resizeWindow(\"Left Eye\", disp_Wl, disp_Hl)\n",
    "            cv2.resizeWindow(\"Right Eye\", disp_Wr, disp_Hr)\n",
    "            if show_arena and capA is not None:\n",
    "                disp_Wa, disp_Ha = int(Wa * window_scale), int(Ha * window_scale)\n",
    "                cv2.resizeWindow(\"Arena\", disp_Wa, disp_Ha)\n",
    "            cv2.resizeWindow(\"Controls\", ctrl_w, ctrl_h)\n",
    "\n",
    "        # step from eye data\n",
    "        step_ms = np.mean([_median_step_ms(left_df), _median_step_ms(right_df)])\n",
    "\n",
    "        if cur_ms is None: cur_ms = start_ms\n",
    "        cur_ms = min(max(cur_ms, start_ms), end_ms)\n",
    "\n",
    "        rowL = _nearest_row(left_df, cur_ms)\n",
    "        rowR = _nearest_row(right_df, cur_ms)\n",
    "\n",
    "        # Left frame\n",
    "        L_img = np.zeros((Hl, Wl, 3), dtype=np.uint8)\n",
    "        if rowL is not None and left_frame_col is not None and pd.notna(rowL[left_frame_col]):\n",
    "            L_idx = int(rowL[left_frame_col]);\n",
    "            capL.set(cv2.CAP_PROP_POS_FRAMES, L_idx);\n",
    "            fL = _read(capL)\n",
    "            if fL is not None:\n",
    "                _overlay_ellipse(fL,left_df,L_idx)\n",
    "                img = _apply_flip(fL.copy());\n",
    "                lines = [f\"Left | {animal} B{block_num} | t={cur_ms:.1f}ms | frame={L_idx}\"]\n",
    "                for c in text_cols:\n",
    "                    if c in left_df.columns:\n",
    "                        v = rowL.get(c, np.nan)\n",
    "                        if pd.notna(v):\n",
    "                            try:\n",
    "                                lines.append(f\"{c}={float(v):.3f}\")\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                _overlay_text(img, lines, origin=(10, 24))\n",
    "                L_img = img\n",
    "        else:\n",
    "            _overlay_text(L_img, [f\"Left | {animal} B{block_num}\", f\"t={cur_ms:.1f}ms\", \"no synchronized frame\"],\n",
    "                          origin=(10, 24))\n",
    "            L_img = _apply_flip(L_img)\n",
    "\n",
    "        # Right frame\n",
    "        R_img = np.zeros((Hr, Wr, 3), dtype=np.uint8)\n",
    "        if rowR is not None and right_frame_col is not None and pd.notna(rowR[right_frame_col]):\n",
    "            R_idx = int(rowR[right_frame_col]);\n",
    "            capR.set(cv2.CAP_PROP_POS_FRAMES, R_idx);\n",
    "            fR = _read(capR)\n",
    "            if fR is not None:\n",
    "                _overlay_ellipse(fR, right_df, R_idx)\n",
    "                img = _apply_flip(fR.copy());\n",
    "                lines = [f\"Right | {animal} B{block_num} | t={cur_ms:.1f}ms | frame={R_idx}\"]\n",
    "                for c in text_cols:\n",
    "                    if c in right_df.columns:\n",
    "                        v = rowR.get(c, np.nan)\n",
    "                        if pd.notna(v):\n",
    "                            try:\n",
    "                                lines.append(f\"{c}={float(v):.3f}\")\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                _overlay_text(img, lines, origin=(10, 24))\n",
    "                R_img = img\n",
    "        else:\n",
    "            _overlay_text(R_img, [f\"Right | {animal} B{block_num}\", f\"t={cur_ms:.1f}ms\", \"no synchronized frame\"],\n",
    "                          origin=(10, 24))\n",
    "            R_img = _apply_flip(R_img)\n",
    "\n",
    "        # Arena frame\n",
    "        if show_arena and capA is not None and arena_df is not None:\n",
    "            arow = _nearest_row(arena_df, cur_ms)\n",
    "            if arow is not None and pd.notna(arow[arena_frame_col]):\n",
    "                A_idx = int(arow[arena_frame_col]);\n",
    "                capA.set(cv2.CAP_PROP_POS_FRAMES, A_idx);\n",
    "                fA = _read(capA)\n",
    "                if fA is not None:\n",
    "                    A_img = _apply_flip(fA.copy())\n",
    "                    _overlay_text(A_img, [f\"Arena | {animal} B{block_num} | t={cur_ms:.1f}ms | frame={A_idx}\"],\n",
    "                                  origin=(10, 24))\n",
    "                    cv2.imshow(\"Arena\", cv2.resize(A_img, (disp_Wa, disp_Ha)))\n",
    "            else:\n",
    "                blank = np.zeros((max(1, Ha), max(1, Wa), 3), dtype=np.uint8)\n",
    "                _overlay_text(blank, [f\"Arena | no synchronized frame\"], origin=(10, 24))\n",
    "                cv2.imshow(\"Arena\", cv2.resize(blank, (disp_Wa, disp_Ha)))\n",
    "\n",
    "        cv2.imshow(\"Left Eye\", cv2.resize(L_img, (disp_Wl, disp_Hl)))\n",
    "        cv2.imshow(\"Right Eye\", cv2.resize(R_img, (disp_Wr, disp_Hr)))\n",
    "        bs = _lookup_block(animal, block_num)\n",
    "        controls_img = _draw_controls(cur_idx, Path(bs.arena_videos[arena_idx]).name if (\n",
    "                    show_arena and getattr(bs, \"arena_videos\", None)) else None)\n",
    "        cv2.imshow(\"Controls\", controls_img)\n",
    "\n",
    "        # playback\n",
    "        if playing:\n",
    "            cur_ms += step_ms\n",
    "            if cur_ms > end_ms:\n",
    "                playing = False\n",
    "                cur_idx = (cur_idx + 1) % len(events)\n",
    "                cur_ms = None\n",
    "\n",
    "    # cleanup & return\n",
    "    capL = None;\n",
    "    capR = None;\n",
    "    capA = None\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return events\n",
    "\n",
    "\n",
    "# ======================= tiny helper to build events_df from a query quickly =======================\n",
    "def make_events_df(\n",
    "        starts: Sequence[Union[int, float]],\n",
    "        ends: Sequence[Union[int, float]],\n",
    "        *,\n",
    "        animal: Union[str, Sequence[str]],\n",
    "        block: Union[Union[int, str], Sequence[Union[int, str]]],\n",
    "        eye: Union[str, Sequence[str]],\n",
    "        time_unit: str = \"ms\",\n",
    "        sort_by_start: bool = True,\n",
    "        drop_invalid: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    n = len(starts)\n",
    "    if n != len(ends): raise ValueError(\"starts and ends must be the same length\")\n",
    "\n",
    "    def _broadcast(x, name):\n",
    "        if isinstance(x, (list, tuple, np.ndarray, pd.Series)):\n",
    "            if len(x) != n: raise ValueError(f\"{name} length must match {n}\")\n",
    "            return list(x)\n",
    "        return [x] * n\n",
    "\n",
    "    animals = _broadcast(animal, \"animal\")\n",
    "    blocks = _broadcast(block, \"block\")\n",
    "    eyes = _broadcast(eye, \"eye\")\n",
    "\n",
    "    starts = pd.to_numeric(pd.Series(starts), errors=\"coerce\").astype(float)\n",
    "    ends = pd.to_numeric(pd.Series(ends), errors=\"coerce\").astype(float)\n",
    "    if time_unit.lower().startswith(\"s\"):\n",
    "        starts *= 1000.0;\n",
    "        ends *= 1000.0\n",
    "    elif time_unit.lower() not in {\"ms\", \"millisecond\", \"milliseconds\"}:\n",
    "        raise ValueError(\"time_unit must be 'ms' or 's'\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"animal\": pd.Series(animals, dtype=\"string\"),\n",
    "        \"block\": pd.Series(blocks, dtype=\"string\"),\n",
    "        \"eye\": pd.Series(eyes, dtype=\"string\"),\n",
    "        \"start_ms\": starts,\n",
    "        \"end_ms\": ends,\n",
    "    }).dropna(subset=[\"start_ms\", \"end_ms\"])\n",
    "    if drop_invalid:\n",
    "        df = df[df[\"start_ms\"] < df[\"end_ms\"]].copy()\n",
    "    if sort_by_start and not df.empty:\n",
    "        df = df.sort_values([\"animal\", \"block\", \"eye\", \"start_ms\", \"end_ms\"]).reset_index(drop=True)\n",
    "    return df\n",
    "def query_to_events_df_merge(\n",
    "    eye_df: pd.DataFrame,\n",
    "    query_str: str,\n",
    "    *,\n",
    "    animal: str,\n",
    "    block: Union[int, str],\n",
    "    eye: str,                      # 'L' or 'R' (case-insensitive)\n",
    "    pre_pad_ms: float = 0.0,\n",
    "    post_pad_ms: float = 0.0,\n",
    "    min_run_len: int = 1,\n",
    "    max_gap: int = 1,\n",
    "    sort_by_start: bool = True,\n",
    "    # --- merging controls ---\n",
    "    existing_qdf: Optional[pd.DataFrame] = None,\n",
    "    merge_tol_ms: float = 1.0,     # rows with |Δstart|<=tol & |Δend|<=tol are considered the same event\n",
    "    binocular_label: str = \"LR\",   # label to use when an event exists for both eyes\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build events from a query and merge into an existing qdf, collapsing binocular duplicates.\n",
    "\n",
    "    Output columns match your workflow: ['animal','block','eye','start_ms','end_ms'].\n",
    "    If a Left and Right event share timestamps (within merge_tol_ms), they are merged into one row\n",
    "    with eye=binocular_label (default 'LR').\n",
    "    \"\"\"\n",
    "    # --- helpers (reuse your previous logic) ---\n",
    "    def _runs_from_index(int_index: np.ndarray, min_run_len: int = 1, max_gap: int = 1) -> List[Tuple[int,int]]:\n",
    "        if int_index.size == 0: return []\n",
    "        diffs = np.diff(int_index)\n",
    "        boundaries = np.where(diffs > max_gap)[0]\n",
    "        starts = np.r_[0, boundaries + 1]\n",
    "        ends   = np.r_[boundaries, len(int_index) - 1]\n",
    "        runs = [(int(int_index[s]), int(int_index[e])) for s, e in zip(starts, ends)]\n",
    "        if min_run_len > 1:\n",
    "            runs = [r for r in runs if (r[1] - r[0] + 1) >= min_run_len]\n",
    "        return runs\n",
    "\n",
    "    def _pad_ms_bounds(df: pd.DataFrame, start_ms: float, end_ms: float, pre_pad_ms: float, post_pad_ms: float) -> Tuple[float,float]:\n",
    "        if \"ms_axis\" not in df.columns:\n",
    "            return start_ms, end_ms\n",
    "        ms = df[\"ms_axis\"].values\n",
    "        s = start_ms - float(pre_pad_ms)\n",
    "        e = end_ms + float(post_pad_ms)\n",
    "        if ms.size:\n",
    "            lo, hi = (ms[0], ms[-1]) if ms[-1] >= ms[0] else (ms[-1], ms[0])\n",
    "            s = min(max(s, lo), hi)\n",
    "            e = min(max(e, lo), hi)\n",
    "        return float(s), float(e)\n",
    "\n",
    "    def _build_from_query() -> pd.DataFrame:\n",
    "        # filter\n",
    "        try:\n",
    "            sub = eye_df.query(query_str)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"query failed: {e}\")\n",
    "        if sub.empty:\n",
    "            return pd.DataFrame(columns=[\"animal\",\"block\",\"eye\",\"start_ms\",\"end_ms\"])\n",
    "\n",
    "        # contiguity on integer-like index (fallback to positional)\n",
    "        if not np.issubdtype(sub.index.dtype, np.integer):\n",
    "            pos_idx = eye_df.index.get_indexer(sub.index)\n",
    "            valid = pos_idx >= 0\n",
    "            int_idx = pos_idx[valid]\n",
    "            sub = sub.iloc[np.where(valid)[0]]\n",
    "        else:\n",
    "            int_idx = sub.index.values\n",
    "        int_idx = np.asarray(np.sort(int_idx), dtype=int)\n",
    "        runs = _runs_from_index(int_idx, min_run_len=min_run_len, max_gap=max_gap)\n",
    "\n",
    "        eye_short = \"L\" if str(eye).lower().startswith(\"l\") else \"R\"\n",
    "        rows = []\n",
    "        for i0, i1 in runs:\n",
    "            # robust loc/iloc fallback\n",
    "            try:\n",
    "                r0 = eye_df.loc[i0]; r1 = eye_df.loc[i1]\n",
    "            except KeyError:\n",
    "                r0 = eye_df.iloc[i0] if (0 <= i0 < len(eye_df)) else sub.iloc[0]\n",
    "                r1 = eye_df.iloc[i1] if (0 <= i1 < len(eye_df)) else sub.iloc[-1]\n",
    "\n",
    "            if \"ms_axis\" in eye_df.columns:\n",
    "                s_ms = float(r0[\"ms_axis\"]); e_ms = float(r1[\"ms_axis\"])\n",
    "            else:\n",
    "                s_ms = float(i0); e_ms = float(i1)\n",
    "\n",
    "            s_ms, e_ms = _pad_ms_bounds(eye_df, s_ms, e_ms, pre_pad_ms, post_pad_ms)\n",
    "            if e_ms <= s_ms:\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"animal\": str(animal),\n",
    "                \"block\":  str(block),\n",
    "                \"eye\":    eye_short,\n",
    "                \"start_ms\": s_ms,\n",
    "                \"end_ms\":   e_ms,\n",
    "            })\n",
    "\n",
    "        out = pd.DataFrame(rows)\n",
    "        if sort_by_start and not out.empty:\n",
    "            out = out.sort_values([\"animal\",\"block\",\"eye\",\"start_ms\"]).reset_index(drop=True)\n",
    "        return out\n",
    "\n",
    "    def _merge_eye_labels(labels: Sequence[str]) -> str:\n",
    "        \"\"\"Combine eyes across duplicates; {L,R} -> binocular_label; pass through singletons.\"\"\"\n",
    "        uniq = set(s.upper() for s in labels if isinstance(s, str) and len(s) > 0)\n",
    "        if binocular_label.upper() in uniq:\n",
    "            return binocular_label  # already merged\n",
    "        if \"L\" in uniq and \"R\" in uniq:\n",
    "            return binocular_label\n",
    "        # single eye remains\n",
    "        if \"L\" in uniq: return \"L\"\n",
    "        if \"R\" in uniq: return \"R\"\n",
    "        # fallback: join unique tokens\n",
    "        return binocular_label if len(uniq) > 1 else (next(iter(uniq)) if uniq else binocular_label)\n",
    "\n",
    "    def _merge_into_existing(new_df: pd.DataFrame, existing: pd.DataFrame) -> pd.DataFrame:\n",
    "        if existing is None or existing.empty:\n",
    "            return new_df.copy()\n",
    "\n",
    "        # standardize columns\n",
    "        need_cols = [\"animal\",\"block\",\"eye\",\"start_ms\",\"end_ms\"]\n",
    "        for df in (existing, new_df):\n",
    "            for c in need_cols:\n",
    "                if c not in df.columns:\n",
    "                    df[c] = np.nan\n",
    "        ex = existing[need_cols].copy()\n",
    "        nw = new_df[need_cols].copy()\n",
    "\n",
    "        # numeric\n",
    "        for c in (\"start_ms\",\"end_ms\"):\n",
    "            ex[c] = pd.to_numeric(ex[c], errors=\"coerce\").astype(float)\n",
    "            nw[c] = pd.to_numeric(nw[c], errors=\"coerce\").astype(float)\n",
    "        for c in (\"animal\",\"block\",\"eye\"):\n",
    "            ex[c] = ex[c].astype(str)\n",
    "            nw[c] = nw[c].astype(str)\n",
    "\n",
    "        # build tolerance keys\n",
    "        tol = max(1e-9, float(merge_tol_ms))\n",
    "        ex[\"_k_start\"] = np.rint(ex[\"start_ms\"] / tol).astype(\"Int64\")\n",
    "        ex[\"_k_end\"]   = np.rint(ex[\"end_ms\"]   / tol).astype(\"Int64\")\n",
    "        nw[\"_k_start\"] = np.rint(nw[\"start_ms\"] / tol).astype(\"Int64\")\n",
    "        nw[\"_k_end\"]   = np.rint(nw[\"end_ms\"]   / tol).astype(\"Int64\")\n",
    "\n",
    "        combined = pd.concat([ex, nw], ignore_index=True)\n",
    "\n",
    "        # group by (animal, block, approx start, approx end) and reduce\n",
    "        def _reduce(group: pd.DataFrame) -> pd.Series:\n",
    "            eyes = group[\"eye\"].tolist()\n",
    "            eye_merged = _merge_eye_labels(eyes)\n",
    "            # choose representative start/end: median is robust if tiny jitter exists\n",
    "            s = float(np.median(group[\"start_ms\"].values))\n",
    "            e = float(np.median(group[\"end_ms\"].values))\n",
    "            # keep canonical animal/block (string)\n",
    "            a = str(group[\"animal\"].iloc[0])\n",
    "            b = str(group[\"block\"].iloc[0])\n",
    "            return pd.Series({\"animal\": a, \"block\": b, \"eye\": eye_merged, \"start_ms\": s, \"end_ms\": e})\n",
    "\n",
    "        merged = (combined\n",
    "                  .groupby([\"animal\",\"block\",\"_k_start\",\"_k_end\"], dropna=False, sort=True)\n",
    "                  .apply(_reduce)\n",
    "                  .reset_index(drop=True))\n",
    "\n",
    "        if sort_by_start and not merged.empty:\n",
    "            merged = merged.sort_values([\"animal\",\"block\",\"start_ms\",\"end_ms\",\"eye\"]).reset_index(drop=True)\n",
    "\n",
    "        return merged\n",
    "\n",
    "    # --- build from query and merge ---\n",
    "    built = _build_from_query()\n",
    "    return _merge_into_existing(built, existing_qdf)\n",
    "\n",
    "# NEW FROM HERE:\n",
    "\n",
    "# === Outlier reporting, tagging, and Bokeh verification ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional, Sequence, Union\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.layouts import gridplot, column\n",
    "from bokeh.models import ColumnDataSource, BoxAnnotation, HoverTool\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()  # comment out if you prefer standalone HTML files only\n",
    "\n",
    "\n",
    "# ------------------------------ configuration dataclass ------------------------------\n",
    "@dataclass\n",
    "class EyeColumns:\n",
    "    ms: str = \"ms_axis\"\n",
    "    phi: str = \"k_phi\"             # angular elevation (deg)\n",
    "    theta: str = \"k_theta\"         # angular azimuth (deg)\n",
    "    pupil: str = \"pupil_diameter\"  # units: your pipeline's pupil metric\n",
    "    frame: Optional[str] = None    # optional, not required here\n",
    "\n",
    "\n",
    "# ------------------------------ robust zscore ------------------------------\n",
    "def _robust_zscore(x: pd.Series) -> pd.Series:\n",
    "    \"\"\"Median/MAD z-score (less sensitive to tails); NaN-safe.\"\"\"\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    med = np.nanmedian(x.values)\n",
    "    mad = np.nanmedian(np.abs(x.values - med))\n",
    "    scale = 1.4826 * (mad if mad > 0 else (np.nanstd(x.values) if np.nanstd(x.values) > 0 else 1.0))\n",
    "    return (x - med) / scale\n",
    "\n",
    "\n",
    "# ------------------------------ helpers: boolean runs -> (start_ms, end_ms) ------------------------------\n",
    "def _boolean_runs_to_events(ms: np.ndarray, mask: np.ndarray, bridge_ms: float) -> List[Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Convert a boolean mask over ms-axis into merged [start_ms, end_ms] events, stitching gaps <= bridge_ms.\n",
    "    \"\"\"\n",
    "    ms = ms.astype(float)\n",
    "    mask = mask.astype(bool)\n",
    "    if ms.size == 0 or not mask.any():\n",
    "        return []\n",
    "\n",
    "    # find initial contiguous runs where mask is True\n",
    "    idx = np.flatnonzero(mask)\n",
    "    # split on gaps > 1 in index (contiguity by sampling steps)\n",
    "    split_points = np.where(np.diff(idx) > 1)[0]\n",
    "    starts = np.r_[0, split_points + 1]\n",
    "    ends = np.r_[split_points, len(idx) - 1]\n",
    "\n",
    "    raw_events = [(ms[idx[s]], ms[idx[e]]) for s, e in zip(starts, ends)]\n",
    "    if not raw_events:\n",
    "        return []\n",
    "\n",
    "    # stitch neighboring events if the gap between them ≤ bridge_ms\n",
    "    merged = [raw_events[0]]\n",
    "    for s, e in raw_events[1:]:\n",
    "        prev_s, prev_e = merged[-1]\n",
    "        gap = max(0.0, s - prev_e)\n",
    "        if gap <= float(bridge_ms):\n",
    "            merged[-1] = (prev_s, e)\n",
    "        else:\n",
    "            merged.append((s, e))\n",
    "    return merged\n",
    "\n",
    "\n",
    "def _overlap(a: Tuple[float, float], b: Tuple[float, float], tol_ms: float = 0.0) -> bool:\n",
    "    return not (a[1] < b[0] - tol_ms or b[1] < a[0] - tol_ms)\n",
    "\n",
    "\n",
    "def _merge_binocular(\n",
    "    events_L: List[Tuple[float, float, str]],\n",
    "    events_R: List[Tuple[float, float, str]],\n",
    "    tol_ms: float = 1.0\n",
    ") -> List[Tuple[float, float, str, str]]:\n",
    "    \"\"\"\n",
    "    Merge L/R event lists (start,end,source) into possibly binocular LR events.\n",
    "    Returns list of (start_ms, end_ms, eye_label, source_merged).\n",
    "    Merging rule: if any L and R event overlap within tol_ms -> one LR event with unioned sources.\n",
    "    Otherwise keep single-eye events.\n",
    "    \"\"\"\n",
    "    out: List[Tuple[float, float, str, str]] = []\n",
    "    used_R = np.zeros(len(events_R), dtype=bool)\n",
    "\n",
    "    for sL, eL, srcL in events_L:\n",
    "        merged_flag = False\n",
    "        for j, (sR, eR, srcR) in enumerate(events_R):\n",
    "            if used_R[j]:\n",
    "                continue\n",
    "            if _overlap((sL, eL), (sR, eR), tol_ms=tol_ms):\n",
    "                s = float(min(sL, sR))\n",
    "                e = float(max(eL, eR))\n",
    "                src = f\"L[{srcL}] + R[{srcR}]\"\n",
    "                out.append((s, e, \"LR\", src))\n",
    "                used_R[j] = True\n",
    "                merged_flag = True\n",
    "                break\n",
    "        if not merged_flag:\n",
    "            out.append((float(sL), float(eL), \"L\", f\"L[{srcL}]\"))\n",
    "\n",
    "    # append remaining R-only\n",
    "    for (sR, eR, srcR), used in zip(events_R, used_R):\n",
    "        if not used:\n",
    "            out.append((float(sR), float(eR), \"R\", f\"R[{srcR}]\"))\n",
    "\n",
    "    # stable sort by start\n",
    "    out.sort(key=lambda r: (r[0], r[1]))\n",
    "    return out\n",
    "\n",
    "\n",
    "# ------------------------------ (1) threshold sweep report ------------------------------\n",
    "def outlier_threshold_report(\n",
    "    block,\n",
    "    z_abs_list: Sequence[float],\n",
    "    cols: EyeColumns = EyeColumns(),\n",
    "    physiol_limits: Dict[str, Tuple[Optional[float], Optional[float]]] = None,\n",
    "    eyes: Sequence[str] = (\"L\", \"R\")\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each |z| in z_abs_list, report how many samples would be flagged per eye & signal.\n",
    "    physiol_limits: dict with keys in {'phi','theta','pupil'} -> (min,max) hard bounds (None to disable a side).\n",
    "    Returns tidy DataFrame for inspection.\n",
    "    \"\"\"\n",
    "    physiol_limits = physiol_limits or {}\n",
    "    rows = []\n",
    "\n",
    "    eye_map = {\n",
    "        \"L\": getattr(block, \"left_eye_data\"),\n",
    "        \"R\": getattr(block, \"right_eye_data\"),\n",
    "    }\n",
    "    for eye in eyes:\n",
    "        df = eye_map[eye]\n",
    "        # ensure columns exist\n",
    "        for c in (cols.ms, cols.phi, cols.theta, cols.pupil):\n",
    "            if c not in df.columns:\n",
    "                raise ValueError(f\"Column '{c}' missing for eye {eye}\")\n",
    "\n",
    "        z_phi = _robust_zscore(df[cols.phi])\n",
    "        z_theta = _robust_zscore(df[cols.theta])\n",
    "        z_pupil = _robust_zscore(df[cols.pupil])\n",
    "\n",
    "        # hard limits\n",
    "        def lim_mask(signal: str) -> np.ndarray:\n",
    "            v = df[getattr(cols, signal)]\n",
    "            lo, hi = physiol_limits.get(signal, (None, None))\n",
    "            lo_ok = np.full(len(v), True) if lo is None else (v >= float(lo))\n",
    "            hi_ok = np.full(len(v), True) if hi is None else (v <= float(hi))\n",
    "            return ~(lo_ok & hi_ok)  # True where violates limits\n",
    "\n",
    "        hard_phi = lim_mask(\"phi\")\n",
    "        hard_theta = lim_mask(\"theta\")\n",
    "        hard_pupil = lim_mask(\"pupil\")\n",
    "\n",
    "        for zthr in z_abs_list:\n",
    "            rel_phi = np.abs(z_phi.values) > float(zthr)\n",
    "            rel_theta = np.abs(z_theta.values) > float(zthr)\n",
    "            rel_pupil = np.abs(z_pupil.values) > float(zthr)\n",
    "            rows.extend([\n",
    "                {\"animal\": block.animal_call, \"block\": str(block.block_num), \"eye\": eye, \"signal\": \"phi\",\n",
    "                 \"criterion\": f\"|z|>{zthr}\", \"count\": int(rel_phi.sum()), \"percent\": 100*rel_phi.mean()},\n",
    "                {\"animal\": block.animal_call, \"block\": str(block.block_num), \"eye\": eye, \"signal\": \"theta\",\n",
    "                 \"criterion\": f\"|z|>{zthr}\", \"count\": int(rel_theta.sum()), \"percent\": 100*rel_theta.mean()},\n",
    "                {\"animal\": block.animal_call, \"block\": str(block.block_num), \"eye\": eye, \"signal\": \"pupil\",\n",
    "                 \"criterion\": f\"|z|>{zthr}\", \"count\": int(rel_pupil.sum()), \"percent\": 100*rel_pupil.mean()},\n",
    "            ])\n",
    "\n",
    "        # add hard-limit rows once (criterion label 'limits')\n",
    "        rows.extend([\n",
    "            {\"animal\": block.animal_call, \"block\": str(block.block_num), \"eye\": eye, \"signal\": \"phi\",\n",
    "             \"criterion\": \"limits\", \"count\": int(hard_phi.sum()), \"percent\": 100*hard_phi.mean()},\n",
    "            {\"animal\": block.animal_call, \"block\": str(block.block_num), \"eye\": eye, \"signal\": \"theta\",\n",
    "             \"criterion\": \"limits\", \"count\": int(hard_theta.sum()), \"percent\": 100*hard_theta.mean()},\n",
    "            {\"animal\": block.animal_call, \"block\": str(block.block_num), \"eye\": eye, \"signal\": \"pupil\",\n",
    "             \"criterion\": \"limits\", \"count\": int(hard_pupil.sum()), \"percent\": 100*hard_pupil.mean()},\n",
    "        ])\n",
    "\n",
    "    rep = pd.DataFrame(rows)\n",
    "    return rep.sort_values([\"eye\", \"signal\", \"criterion\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ------------------------------ (2) tagging with bridging + sources ------------------------------\n",
    "def tag_outliers_to_events(\n",
    "    block,\n",
    "    z_abs_threshold: float,\n",
    "    cols: EyeColumns = EyeColumns(),\n",
    "    physiol_limits: Dict[str, Tuple[Optional[float], Optional[float]]] = None,\n",
    "    bridge_ms: float = 50.0,\n",
    "    binocular_merge: bool = True,\n",
    "    binocular_tol_ms: float = 1.0,\n",
    "    min_duration_ms: float = 0.0,\n",
    "    source_mode: str = \"union\",   # 'union' or 'max'\n",
    "    *,\n",
    "    phi_limits: Optional[Tuple[Optional[float], Optional[float]]] = None,\n",
    "    theta_limits: Optional[Tuple[Optional[float], Optional[float]]] = None,\n",
    "    pupil_limits: Optional[Tuple[Optional[float], Optional[float]]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates an events dataframe:\n",
    "      ['animal','block','eye','start_ms','end_ms','source']\n",
    "\n",
    "    Flags samples by OR of:\n",
    "      • (|z| > z_abs_threshold) for {phi, theta, pupil}\n",
    "      • OR outside hard limits for any provided limits\n",
    "          - precedence: explicit *limits arguments override entries in physiol_limits\n",
    "\n",
    "    Then:\n",
    "      • Builds events per eye from the boolean mask, stitching gaps <= bridge_ms.\n",
    "      • Optionally merges L/R overlaps into a single 'LR' event (within binocular_tol_ms).\n",
    "      • 'source' records which signals triggered within each event.\n",
    "\n",
    "    Parameters added:\n",
    "      phi_limits, theta_limits, pupil_limits:\n",
    "         Tuples of (min, max). Use None to disable a side, e.g. (-40, None).\n",
    "         If provided, they override the corresponding key in physiol_limits.\n",
    "    \"\"\"\n",
    "    # ---- resolve effective hard limits (explicit args override dict) ----\n",
    "    physiol_limits = dict(physiol_limits or {})\n",
    "    if phi_limits is not None:\n",
    "        physiol_limits[\"phi\"] = phi_limits\n",
    "    if theta_limits is not None:\n",
    "        physiol_limits[\"theta\"] = theta_limits\n",
    "    if pupil_limits is not None:\n",
    "        physiol_limits[\"pupil\"] = pupil_limits\n",
    "\n",
    "    def build_eye_events(df: pd.DataFrame, eye_label: str) -> List[Tuple[float, float, str]]:\n",
    "        # robust z\n",
    "        z_phi = _robust_zscore(df[cols.phi])\n",
    "        z_theta = _robust_zscore(df[cols.theta])\n",
    "        z_pupil = _robust_zscore(df[cols.pupil])\n",
    "\n",
    "        rel_phi = np.abs(z_phi.values) > float(z_abs_threshold)\n",
    "        rel_theta = np.abs(z_theta.values) > float(z_abs_threshold)\n",
    "        rel_pupil = np.abs(z_pupil.values) > float(z_abs_threshold)\n",
    "\n",
    "        ms = pd.to_numeric(df[cols.ms], errors=\"coerce\").values.astype(float)\n",
    "\n",
    "        # hard-limit masks (True where value violates limits)\n",
    "        def hard(signal: str) -> np.ndarray:\n",
    "            v = pd.to_numeric(df[getattr(cols, signal)], errors=\"coerce\").values\n",
    "            lo, hi = physiol_limits.get(signal, (None, None))\n",
    "            lo_bad = np.zeros_like(v, dtype=bool) if lo is None else (v < float(lo))\n",
    "            hi_bad = np.zeros_like(v, dtype=bool) if hi is None else (v > float(hi))\n",
    "            return lo_bad | hi_bad\n",
    "\n",
    "        hard_phi = hard(\"phi\")\n",
    "        hard_theta = hard(\"theta\")\n",
    "        hard_pupil = hard(\"pupil\")\n",
    "\n",
    "        # union mask across all signals/criteria\n",
    "        mask = rel_phi | rel_theta | rel_pupil | hard_phi | hard_theta | hard_pupil\n",
    "\n",
    "        # derive events with bridging\n",
    "        intervals = _boolean_runs_to_events(ms, mask, bridge_ms=bridge_ms)\n",
    "\n",
    "        # attribute sources inside each interval\n",
    "        out: List[Tuple[float, float, str]] = []\n",
    "        for s, e in intervals:\n",
    "            in_evt = (ms >= s) & (ms <= e)\n",
    "\n",
    "            causes = []\n",
    "            if rel_phi[in_evt].any():    causes.append(\"phi_z\")\n",
    "            if rel_theta[in_evt].any():  causes.append(\"theta_z\")\n",
    "            if rel_pupil[in_evt].any():  causes.append(\"pupil_z\")\n",
    "            if hard_phi[in_evt].any():   causes.append(\"phi_limit\")\n",
    "            if hard_theta[in_evt].any(): causes.append(\"theta_limit\")\n",
    "            if hard_pupil[in_evt].any(): causes.append(\"pupil_limit\")\n",
    "            if not causes:\n",
    "                causes = [\"unknown\"]\n",
    "\n",
    "            if float(e - s) >= float(min_duration_ms):\n",
    "                if source_mode == \"max\":\n",
    "                    # pick dominant trigger by sample count\n",
    "                    counts = {\n",
    "                        \"phi_z\": int(rel_phi[in_evt].sum()),\n",
    "                        \"theta_z\": int(rel_theta[in_evt].sum()),\n",
    "                        \"pupil_z\": int(rel_pupil[in_evt].sum()),\n",
    "                        \"phi_limit\": int(hard_phi[in_evt].sum()),\n",
    "                        \"theta_limit\": int(hard_theta[in_evt].sum()),\n",
    "                        \"pupil_limit\": int(hard_pupil[in_evt].sum()),\n",
    "                    }\n",
    "                    # keep only keys present in causes to avoid zero-only winners\n",
    "                    counts = {k: v for k, v in counts.items() if k in causes}\n",
    "                    top = max(counts, key=counts.get) if counts else \"unknown\"\n",
    "                    src = top\n",
    "                else:\n",
    "                    src = \"+\".join(sorted(set(causes)))\n",
    "                out.append((float(s), float(e), src))\n",
    "        return out\n",
    "\n",
    "    # sanity checks for required columns\n",
    "    le_df = getattr(block, \"left_eye_data\")\n",
    "    re_df = getattr(block, \"right_eye_data\")\n",
    "    for eye_df in (le_df, re_df):\n",
    "        for c in (cols.ms, cols.phi, cols.theta, cols.pupil):\n",
    "            if c not in eye_df.columns:\n",
    "                raise ValueError(f\"Required column '{c}' missing in eye dataframe.\")\n",
    "\n",
    "    events_L = build_eye_events(le_df, \"L\")\n",
    "    events_R = build_eye_events(re_df, \"R\")\n",
    "\n",
    "    # binocular merge (overlap -> LR)\n",
    "    if binocular_merge:\n",
    "        merged = _merge_binocular(events_L, events_R, tol_ms=binocular_tol_ms)\n",
    "    else:\n",
    "        merged = ([(s, e, \"L\", f\"L[{src}]\") for (s, e, src) in events_L] +\n",
    "                  [(s, e, \"R\", f\"R[{src}]\") for (s, e, src) in events_R])\n",
    "        merged.sort(key=lambda r: (r[0], r[1]))\n",
    "\n",
    "    out_df = pd.DataFrame({\n",
    "        \"animal\": str(block.animal_call),\n",
    "        \"block\": str(block.block_num),\n",
    "        \"eye\": [lab for (_, _, lab, _) in merged],\n",
    "        \"start_ms\": [s for (s, _, _, _) in merged],\n",
    "        \"end_ms\": [e for (_, e, _, _) in merged],\n",
    "        \"source\": [src for (_, _, _, src) in merged],\n",
    "    })\n",
    "    return out_df[[\"animal\", \"block\", \"eye\", \"start_ms\", \"end_ms\", \"source\"]]\n",
    "\n",
    "\n",
    "# ------------------------------ (3) Bokeh verification plotter ------------------------------\n",
    "def bokeh_verify_outliers(\n",
    "    block,\n",
    "    events_df: pd.DataFrame,\n",
    "    cols: EyeColumns = EyeColumns(),\n",
    "    title: str = \"Eye signals with tagged intervals\",\n",
    "    export_html: Optional[str] = None  # path to write a standalone HTML (optional)\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates an interactive Bokeh view:\n",
    "      - Left/Right eye: phi(t), theta(t), pupil(t) on shared x-range (ms).\n",
    "      - Tagged intervals shown as shaded spans; L, R, and LR use different colors.\n",
    "      - Pan/zoom linked across all plots.\n",
    "    \"\"\"\n",
    "    # pull data\n",
    "    L = getattr(block, \"left_eye_data\")\n",
    "    R = getattr(block, \"right_eye_data\")\n",
    "    for df in (L, R):\n",
    "        for c in (cols.ms, cols.phi, cols.theta, cols.pupil):\n",
    "            if c not in df.columns:\n",
    "                raise ValueError(f\"Column '{c}' missing for verification: {c}\")\n",
    "\n",
    "    srcL = ColumnDataSource(dict(\n",
    "        ms=L[cols.ms].astype(float),\n",
    "        phi=pd.to_numeric(L[cols.phi], errors=\"coerce\"),\n",
    "        theta=pd.to_numeric(L[cols.theta], errors=\"coerce\"),\n",
    "        pupil=pd.to_numeric(L[cols.pupil], errors=\"coerce\"),\n",
    "    ))\n",
    "    srcR = ColumnDataSource(dict(\n",
    "        ms=R[cols.ms].astype(float),\n",
    "        phi=pd.to_numeric(R[cols.phi], errors=\"coerce\"),\n",
    "        theta=pd.to_numeric(R[cols.theta], errors=\"coerce\"),\n",
    "        pupil=pd.to_numeric(R[cols.pupil], errors=\"coerce\"),\n",
    "    ))\n",
    "\n",
    "    # figures (shared x_range)\n",
    "    pL_phi = figure(title=f\"{title} – Left φ\", width=1200, height=180, tools=\"pan,wheel_zoom,box_zoom,reset,save\",\n",
    "                    x_axis_label=\"time (ms)\")\n",
    "    pL_theta = figure(title=\"Left θ\", width=1200, height=180, x_range=pL_phi.x_range,\n",
    "                      tools=\"pan,wheel_zoom,box_zoom,reset,save\")\n",
    "    pL_pupil = figure(title=\"Left pupil\", width=1200, height=180, x_range=pL_phi.x_range,\n",
    "                      tools=\"pan,wheel_zoom,box_zoom,reset,save\", x_axis_label=\"time (ms)\")\n",
    "\n",
    "    pR_phi = figure(title=\"Right φ\", width=1200, height=180, x_range=pL_phi.x_range,\n",
    "                    tools=\"pan,wheel_zoom,box_zoom,reset,save\")\n",
    "    pR_theta = figure(title=\"Right θ\", width=1200, height=180, x_range=pL_phi.x_range,\n",
    "                      tools=\"pan,wheel_zoom,box_zoom,reset,save\")\n",
    "    pR_pupil = figure(title=\"Right pupil\", width=1200, height=180, x_range=pL_phi.x_range,\n",
    "                      tools=\"pan,wheel_zoom,box_zoom,reset,save\", x_axis_label=\"time (ms)\")\n",
    "\n",
    "    for p in (pL_phi, pL_theta, pL_pupil, pR_phi, pR_theta, pR_pupil):\n",
    "        p.add_tools(HoverTool(tooltips=[(\"t (ms)\", \"@ms\")], mode=\"vline\"))\n",
    "\n",
    "    # draw lines\n",
    "    pL_phi.line(\"ms\", \"phi\", source=srcL)\n",
    "    pL_theta.line(\"ms\", \"theta\", source=srcL)\n",
    "    pL_pupil.line(\"ms\", \"pupil\", source=srcL)\n",
    "\n",
    "    pR_phi.line(\"ms\", \"phi\", source=srcR)\n",
    "    pR_theta.line(\"ms\", \"theta\", source=srcR)\n",
    "    pR_pupil.line(\"ms\", \"pupil\", source=srcR)\n",
    "\n",
    "    # add shaded tags\n",
    "    def add_spans(figs, start_ms, end_ms, eye):\n",
    "        if eye == \"L\":\n",
    "            ba = BoxAnnotation(left=start_ms, right=end_ms, fill_alpha=0.18, fill_color=\"red\")\n",
    "            for p in figs[:3]:  # left panels\n",
    "                p.add_layout(ba)\n",
    "        elif eye == \"R\":\n",
    "            ba = BoxAnnotation(left=start_ms, right=end_ms, fill_alpha=0.18, fill_color=\"blue\")\n",
    "            for p in figs[3:]:  # right panels\n",
    "                p.add_layout(ba)\n",
    "        else:  # LR\n",
    "            ba = BoxAnnotation(left=start_ms, right=end_ms, fill_alpha=0.12, fill_color=\"purple\")\n",
    "            for p in figs:\n",
    "                p.add_layout(ba)\n",
    "\n",
    "    figs = [pL_phi, pL_theta, pL_pupil, pR_phi, pR_theta, pR_pupil]\n",
    "    for _, row in events_df.iterrows():\n",
    "        add_spans(figs, float(row[\"start_ms\"]), float(row[\"end_ms\"]), str(row[\"eye\"]).upper())\n",
    "\n",
    "    layout = gridplot([[pL_phi], [pL_theta], [pL_pupil], [pR_phi], [pR_theta], [pR_pupil]], toolbar_location=\"above\")\n",
    "    if export_html:\n",
    "        from bokeh.io import output_file\n",
    "        output_file(export_html, title=title)\n",
    "    show(layout)\n"
   ],
   "id": "6910117d5966361",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Example usage with a single BlockSync object ===\n",
    "\n",
    "# 0) choose your block and column mapping if different names are used\n",
    "cols = EyeColumns(\n",
    "    ms=\"ms_axis\",\n",
    "    phi=\"k_phi\",\n",
    "    theta=\"k_theta\",\n",
    "    pupil=\"pupil_diameter\",\n",
    ")\n",
    "\n",
    "# 1) sweep report to help pick a threshold\n",
    "z_list = [2.0, 2.5, 3.0, 3.5, 4.0,4.5]  # absolute robust z-scores to preview\n",
    "phys_limits = {\n",
    "    # Use None to disable a side; set your physiology-based bounds here (deg / diameter unit)\n",
    "    \"phi\":   (-30.0, 30.0),\n",
    "    \"theta\": (-50.0, 25.0),\n",
    "    \"pupil\": (1.5, 2.5),\n",
    "}\n",
    "report_df = outlier_threshold_report(block, z_abs_list=z_list, cols=cols, physiol_limits=phys_limits)\n",
    "display(report_df.head(20))"
   ],
   "id": "416554054c7a4f0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2) tag events with your chosen parameters\n",
    "events_df = tag_outliers_to_events(\n",
    "    block,\n",
    "    z_abs_threshold=4.5,     # pick based on the report\n",
    "    cols=cols,\n",
    "    physiol_limits=phys_limits,\n",
    "    bridge_ms=50.0,          # contiguous if gaps <= 50 ms\n",
    "    binocular_merge=True,    # collapse overlapping L/R into LR\n",
    "    binocular_tol_ms=1.0,    # overlap tolerance when merging\n",
    "    min_duration_ms=0.0,     # drop micro events if you want\n",
    "    source_mode=\"union\",phi_limits=(-40,40),theta_limits=(-45,45)     # 'union' (collect all causes) or 'max' (dominant cause)\n",
    ")"
   ],
   "id": "afd7c881e9a1838c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This prints in your requested format (+ a 'source' column):\n",
    "print(events_df.head())"
   ],
   "id": "32f88cfbdd56fbcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(len(events_df))",
   "id": "5b522e7a198869ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3) visual verification (opens interactive Bokeh; shaded spans show L/R/LR tags)\n",
    "\n",
    "bokeh_verify_outliers(block, events_df, cols=cols, title=f\"{block.animal_call} B{block.block_num} verification\",export_html=block.analysis_path / 'outlier_tags_verifier.html')"
   ],
   "id": "cc226e8ff7c2e297",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4) (optional) launch your existing manual reviewer for final GOOD/BAD triage\n",
    "# from your provided functions (must be in the same kernel/session):\n",
    "reviewed_df = review_events_multi_with_arena_v2(block_dict, events_df)\n",
    "# reviewed_df will include tri-state 'manual_outlier_detected' + timestamps and write per-block CSV on export.\n"
   ],
   "id": "d45d0da7305c494e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "reviewed_df",
   "id": "a458741ac46b7d58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# THIS IS WHERE WE MAKE CLEAN EYE_DFs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "# assumes EyeColumns dataclass, TAG_COL, load_block_annotations() exist in scope\n",
    "\n",
    "def apply_manual_outlier_cleanup(\n",
    "    block,\n",
    "    annotations_df: Optional[pd.DataFrame] = None,\n",
    "    *,\n",
    "    cols: EyeColumns = EyeColumns(),\n",
    "    bad_col: str = TAG_COL,            # \"manual_outlier_detected\"\n",
    "    value_cols: Tuple[str, str, str] = (\"k_theta\", \"k_phi\", \"pupil_diameter\"),\n",
    "    add_mask_columns: bool = True,     # add boolean mask columns to the *clean* dfs for audit\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create cleaned eye-data copies on the block by setting 'bad' intervals to NaN in value_cols.\n",
    "    'Bad' is defined by the GUI review's per-interval tag bad_col == True.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    block : BlockSync-like object with:\n",
    "        - animal_call, block_num\n",
    "        - left_eye_data, right_eye_data (DataFrames with cols.ms present)\n",
    "    annotations_df : optional DataFrame of events with columns:\n",
    "        ['animal_call','block','eye','start_ms','end_ms', bad_col]\n",
    "        If None, loads from this block's per-block CSV via load_block_annotations(block).\n",
    "    cols : EyeColumns\n",
    "        Names for ms, phi, theta, pupil in the eye DataFrames (defaults match your pipeline).\n",
    "    bad_col : str\n",
    "        Column name in annotations_df whose True values denote BAD intervals to wipe.\n",
    "    value_cols : tuple\n",
    "        The columns that will be set to NaN inside BAD intervals.\n",
    "    add_mask_columns : bool\n",
    "        If True, adds a boolean 'clean_badmask' column to each clean df for traceability.\n",
    "\n",
    "    Side effects\n",
    "    -----------\n",
    "    Sets on `block`:\n",
    "        - block.left_eye_data_clean  (pd.DataFrame)\n",
    "        - block.right_eye_data_clean (pd.DataFrame)\n",
    "      If block has `left_eye_df`/`right_eye_df`, also sets:\n",
    "        - block.left_eye_df_clean / block.right_eye_df_clean\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys: {'left_eye_data_clean','right_eye_data_clean'}\n",
    "    \"\"\"\n",
    "    # --- resolve annotations ---\n",
    "    if annotations_df is None:\n",
    "        annotations_df = load_block_annotations(block)\n",
    "\n",
    "    if annotations_df is None or annotations_df.empty:\n",
    "        # nothing to wipe; just copy originals\n",
    "        L_clean = getattr(block, \"left_eye_data\").copy()\n",
    "        R_clean = getattr(block, \"right_eye_data\").copy()\n",
    "        if add_mask_columns:\n",
    "            L_clean[\"clean_badmask\"] = False\n",
    "            R_clean[\"clean_badmask\"] = False\n",
    "        setattr(block, \"left_eye_data_clean\", L_clean)\n",
    "        setattr(block, \"right_eye_data_clean\", R_clean)\n",
    "        # optional compatibility mirror\n",
    "        if hasattr(block, \"left_eye_df\"):\n",
    "            setattr(block, \"left_eye_df_clean\", L_clean.copy())\n",
    "        if hasattr(block, \"right_eye_df\"):\n",
    "            setattr(block, \"right_eye_df_clean\", R_clean.copy())\n",
    "        return {\"left_eye_data_clean\": L_clean, \"right_eye_data_clean\": R_clean}\n",
    "\n",
    "    # --- filter to this block only ---\n",
    "    a_str = str(block.animal_call)\n",
    "    b_str = str(block.block_num)\n",
    "    ann = annotations_df.copy()\n",
    "\n",
    "    # normalize expected columns\n",
    "    needed = {\"animal\",\"block\",\"eye\",\"start_ms\",\"end_ms\", bad_col}\n",
    "    missing = [c for c in needed if c not in ann.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"annotations_df missing required columns: {missing}\")\n",
    "\n",
    "    ann[\"animal\"] = ann[\"animal\"].astype(str)\n",
    "    ann[\"block\"] = ann[\"block\"].astype(str)\n",
    "    ann = ann[(ann[\"animal\"] == a_str) & (ann[\"block\"] == b_str)]\n",
    "\n",
    "    if ann.empty:\n",
    "        # no intervals for this block\n",
    "        L_clean = getattr(block, \"left_eye_data\").copy()\n",
    "        R_clean = getattr(block, \"right_eye_data\").copy()\n",
    "        if add_mask_columns:\n",
    "            L_clean[\"clean_badmask\"] = False\n",
    "            R_clean[\"clean_badmask\"] = False\n",
    "        setattr(block, \"left_eye_data_clean\", L_clean)\n",
    "        setattr(block, \"right_eye_data_clean\", R_clean)\n",
    "        if hasattr(block, \"left_eye_df\"):\n",
    "            setattr(block, \"left_eye_df_clean\", L_clean.copy())\n",
    "        if hasattr(block, \"right_eye_df\"):\n",
    "            setattr(block, \"right_eye_df_clean\", R_clean.copy())\n",
    "        return {\"left_eye_data_clean\": L_clean, \"right_eye_data_clean\": R_clean}\n",
    "\n",
    "    # Keep only rows explicitly tagged BAD == True\n",
    "    def _coerce_bool(v):\n",
    "        if isinstance(v, bool): return v\n",
    "        s = str(v).strip().lower()\n",
    "        if s in {\"true\",\"t\",\"1\",\"yes\",\"y\"}: return True\n",
    "        if s in {\"false\",\"f\",\"0\",\"no\",\"n\",\"nan\",\"none\",\"\"}: return False\n",
    "        return False\n",
    "\n",
    "    ann[\"_is_bad\"] = ann[bad_col].map(_coerce_bool)\n",
    "    ann_bad = ann[ann[\"_is_bad\"]].copy()\n",
    "    if ann_bad.empty:\n",
    "        # nothing to mask\n",
    "        L_clean = getattr(block, \"left_eye_data\").copy()\n",
    "        R_clean = getattr(block, \"right_eye_data\").copy()\n",
    "        if add_mask_columns:\n",
    "            L_clean[\"clean_badmask\"] = False\n",
    "            R_clean[\"clean_badmask\"] = False\n",
    "        setattr(block, \"left_eye_data_clean\", L_clean)\n",
    "        setattr(block, \"right_eye_data_clean\", R_clean)\n",
    "        if hasattr(block, \"left_eye_df\"):\n",
    "            setattr(block, \"left_eye_df_clean\", L_clean.copy())\n",
    "        if hasattr(block, \"right_eye_df\"):\n",
    "            setattr(block, \"right_eye_df_clean\", R_clean.copy())\n",
    "        return {\"left_eye_data_clean\": L_clean, \"right_eye_data_clean\": R_clean}\n",
    "\n",
    "    # --- pull eye data, sanity checks ---\n",
    "    L = getattr(block, \"left_eye_data\")\n",
    "    R = getattr(block, \"right_eye_data\")\n",
    "    for df_name, df in ((\"left_eye_data\", L), (\"right_eye_data\", R)):\n",
    "        if cols.ms not in df.columns:\n",
    "            raise ValueError(f\"{df_name} missing time column '{cols.ms}'\")\n",
    "        for vc in value_cols:\n",
    "            if vc not in df.columns:\n",
    "                raise ValueError(f\"{df_name} missing value column '{vc}'\")\n",
    "\n",
    "    # --- build masks from intervals ---\n",
    "    msL = pd.to_numeric(L[cols.ms], errors=\"coerce\").values.astype(float)\n",
    "    msR = pd.to_numeric(R[cols.ms], errors=\"coerce\").values.astype(float)\n",
    "    maskL = np.zeros(msL.shape, dtype=bool)\n",
    "    maskR = np.zeros(msR.shape, dtype=bool)\n",
    "\n",
    "    # Accept eye in {'L','R','LR'} (case-insensitive); treat 'LR' as both eyes\n",
    "    for _, row in ann_bad.iterrows():\n",
    "        s = float(row[\"start_ms\"])\n",
    "        e = float(row[\"end_ms\"])\n",
    "        eye = str(row[\"eye\"]).upper().strip() if pd.notna(row[\"eye\"]) else \"LR\"\n",
    "        if e < s:\n",
    "            s, e = e, s  # swap if misordered\n",
    "\n",
    "        if eye in (\"L\", \"LR\"):\n",
    "            maskL |= (msL >= s) & (msL <= e)\n",
    "        if eye in (\"R\", \"LR\"):\n",
    "            maskR |= (msR >= s) & (msR <= e)\n",
    "\n",
    "    # --- create cleaned copies and set NaNs in requested columns ---\n",
    "    L_clean = L.copy()\n",
    "    R_clean = R.copy()\n",
    "    for vc in value_cols:\n",
    "        L_clean.loc[maskL, vc] = np.nan\n",
    "        R_clean.loc[maskR, vc] = np.nan\n",
    "\n",
    "    if add_mask_columns:\n",
    "        L_clean[\"clean_badmask\"] = maskL\n",
    "        R_clean[\"clean_badmask\"] = maskR\n",
    "\n",
    "    # --- attach back to block (primary names) ---\n",
    "    setattr(block, \"left_eye_data_clean\", L_clean)\n",
    "    setattr(block, \"right_eye_data_clean\", R_clean)\n",
    "\n",
    "    # --- optional compatibility: if the project also uses *_eye_df names, mirror the cleans ---\n",
    "    if hasattr(block, \"left_eye_df\"):\n",
    "        setattr(block, \"left_eye_df_clean\", L_clean.copy())\n",
    "    if hasattr(block, \"right_eye_df\"):\n",
    "        setattr(block, \"right_eye_df_clean\", R_clean.copy())\n",
    "\n",
    "    return {\"left_eye_data_clean\": L_clean, \"right_eye_data_clean\": R_clean}\n"
   ],
   "id": "77d94eb28dc19c5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "apply_manual_outlier_cleanup(block,reviewed_df)",
   "id": "a726d978596bfa61",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
