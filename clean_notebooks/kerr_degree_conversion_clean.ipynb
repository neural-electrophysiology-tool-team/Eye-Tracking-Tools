{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-04T15:43:06.699100Z",
     "start_time": "2025-11-04T15:43:06.604092Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from itertools import cycle\n",
    "import pickle\n",
    "import pathlib\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import bokeh\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['pdf.fonttype'] = 42  # Ensure fonts are embedded and editable\n",
    "rcParams['ps.fonttype'] = 42  # Ensure compatibility with vector outputs\n",
    "%matplotlib inline\n",
    "\n",
    "def horizontal_flip_eye_data(df: pd.DataFrame, frame_width: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Horizontally flip eye-tracking data across the vertical (y) axis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain columns 'center_x', 'center_y', and 'phi' (in degrees).\n",
    "    frame_width : int\n",
    "        Width of the video/frame in pixels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A copy of df where:\n",
    "          - center_x → frame_width − center_x\n",
    "          - center_y unchanged\n",
    "          - phi      → (phi + 90) % 360\n",
    "    \"\"\"\n",
    "    df_flipped = df.copy()\n",
    "    # mirror x\n",
    "    df_flipped['center_x'] = frame_width - df_flipped['center_x']\n",
    "    # phi shift by +90°\n",
    "    df_flipped['phi'] = (180 - df_flipped['phi']) % 360\n",
    "    return df_flipped\n",
    "\n",
    "\n",
    "def bokeh_plotter(data_list, x_axis_list=None, label_list=None,\n",
    "                  plot_name='default',\n",
    "                  x_axis_label='X', y_axis_label='Y',\n",
    "                  peaks=None, peaks_list=False, export_path=False):\n",
    "    \"\"\"Generates an interactive Bokeh plot for the given data vector.\n",
    "    Args:\n",
    "        data_list (list or array): The data to be plotted.\n",
    "        label_list (list of str): The labels of the data vectors\n",
    "        plot_name (str, optional): The title of the plot. Defaults to 'default'.\n",
    "        x_axis (str, optional): The label for the x-axis. Defaults to 'X'.\n",
    "        y_axis (str, optional): The label for the y-axis. Defaults to 'Y'.\n",
    "        peaks (list or array, optional): Indices of peaks to highlight on the plot. Defaults to None.\n",
    "        export_path (False or str): when set to str, will output the resulting html fig\n",
    "    \"\"\"\n",
    "    color_cycle = cycle(bokeh.palettes.Category10_10)\n",
    "    fig = bokeh.plotting.figure(title=f'bokeh explorer: {plot_name}',\n",
    "                                x_axis_label=x_axis_label,\n",
    "                                y_axis_label=y_axis_label,\n",
    "                                plot_width=1500,\n",
    "                                plot_height=700)\n",
    "\n",
    "    for i, data_vector in enumerate(data_list):\n",
    "\n",
    "        color = next(color_cycle)\n",
    "\n",
    "        if x_axis_list is None:\n",
    "            x_axis = range(len(data_vector))\n",
    "        elif len(x_axis_list) == len(data_list):\n",
    "            print('x_axis manually set')\n",
    "            x_axis = x_axis_list[i]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                'problem with x_axis_list input - should be either None, or a list with the same length as data_list')\n",
    "        if label_list is None:\n",
    "            fig.line(x_axis, data_vector, line_color=color, legend_label=f\"Line {i + 1}\")\n",
    "        elif len(label_list) == len(data_list):\n",
    "            fig.line(range(len(data_vector)), data_vector, line_color=color, legend_label=f\"{label_list[i]}\")\n",
    "        if peaks is not None and peaks_list is True:\n",
    "            fig.circle(peaks[i], data_vector[peaks[i]], size=10, color=color)\n",
    "\n",
    "    if peaks is not None and peaks_list is False:\n",
    "        fig.circle(peaks, data_vector[peaks], size=10, color='red')\n",
    "\n",
    "    if export_path is not False:\n",
    "        print(f'exporting to {export_path}')\n",
    "        bokeh.io.output.output_file(filename=str(export_path / f'{plot_name}.html'), title=f'{plot_name}')\n",
    "    bokeh.plotting.show(fig)\n",
    "\n",
    "\n",
    "def load_eye_data_2d_w_rotation_matrix(block):\n",
    "    \"\"\"\n",
    "    This function checks if the eye dataframes and rotation dict object exist, then imports them\n",
    "    :param block: The current blocksync class with verifiec re/le dfs\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        block.left_eye_data = pd.read_csv(block.analysis_path / 'left_eye_data.csv', index_col=0, engine='python')\n",
    "        block.right_eye_data = pd.read_csv(block.analysis_path / 'right_eye_data.csv', index_col=0, engine='python')\n",
    "    except FileNotFoundError:\n",
    "        print('eye_data files not found, run the pipeline!')\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(block.analysis_path / 'rotate_eye_data_params.pkl', 'rb') as file:\n",
    "            rotation_dict = pickle.load(file)\n",
    "            block.left_rotation_matrix = rotation_dict['left_rotation_matrix']\n",
    "            block.right_rotation_matrix = rotation_dict['right_rotation_matrix']\n",
    "            block.left_rotation_angle = rotation_dict['left_rotation_angle']\n",
    "            block.right_rotation_angle = rotation_dict['right_rotation_angle']\n",
    "    except FileNotFoundError:\n",
    "        print('No rotation matrix file, create it')\n",
    "\n",
    "\n",
    "def create_saccade_events_df(eye_data_df, speed_threshold, bokeh_verify_threshold=False, magnitude_calib=1,\n",
    "                             speed_profile=True):\n",
    "    \"\"\"\n",
    "    Detects saccade events in eye tracking data and computes relevant metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - eye_data_df (pd.DataFrame): Input DataFrame containing eye tracking data.\n",
    "    - speed_threshold (float): Threshold for saccade detection based on speed.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): Modified input DataFrame with added columns for speed and saccade detection.\n",
    "    - saccade_events_df (pd.DataFrame): DataFrame containing information about detected saccade events.\n",
    "\n",
    "    Steps:\n",
    "    1. Calculate speed components ('speed_x', 'speed_y') based on differences in 'center_x' and 'center_y'.\n",
    "    2. Compute the magnitude of the velocity vector ('speed_r').\n",
    "    3. Create a binary column ('is_saccade') indicating saccade events based on the speed threshold.\n",
    "    4. Determine saccade onset and offset indices and timestamps.\n",
    "    5. Create a DataFrame ('saccade_events_df') with columns:\n",
    "        - 'saccade_start_ind': Indices of saccade onset.\n",
    "        - 'saccade_start_timestamp': Timestamps corresponding to saccade onset.\n",
    "        - 'saccade_end_ind': Indices of saccade offset.\n",
    "        - 'saccade_end_timestamp': Timestamps corresponding to saccade offset.\n",
    "        - 'length': Duration of each saccade event.\n",
    "    6. Calculate distance traveled and angles for each saccade event.\n",
    "    7. Append additional columns to 'saccade_events_df':\n",
    "        - 'magnitude': Magnitude of the distance traveled during each saccade.\n",
    "        - 'angle': Angle of the saccade vector in degrees.\n",
    "        - 'initial_x', 'initial_y': Initial coordinates of the saccade.\n",
    "        - 'end_x', 'end_y': End coordinates of the saccade.\n",
    "\n",
    "    Note: The original 'eye_data_df' is not modified; modified data is returned as 'df'.\n",
    "    \"\"\"\n",
    "    df = eye_data_df\n",
    "    df['speed_x'] = df['center_x'].diff()  # Difference between consecutive 'center_x' values\n",
    "    df['speed_y'] = df['center_y'].diff()  # Difference between consecutive 'center_y' values\n",
    "\n",
    "    # Step 2: Calculate magnitude of the velocity vector (R vector speed)\n",
    "    df['speed_r'] = (df['speed_x'] ** 2 + df['speed_y'] ** 2) ** 0.5\n",
    "\n",
    "    # Create a column for saccade detection\n",
    "    df['is_saccade'] = df['speed_r'] > speed_threshold\n",
    "\n",
    "    # create a saccade_on_off indicator where 1 is rising edge and -1 is falling edge by subtracting a shifted binary mask\n",
    "    saccade_on_off = df.is_saccade.astype(int) - df.is_saccade.shift(periods=1, fill_value=False).astype(int)\n",
    "    saccade_on_inds = np.where(saccade_on_off == 1)[\n",
    "                          0] - 1  # notice the manual shift here, chosen to include the first (sometimes slower) eye frame, just before saccade threshold crossing\n",
    "    saccade_on_ms = df['ms_axis'].iloc[saccade_on_inds]\n",
    "    saccade_on_timestamps = df['OE_timestamp'].iloc[saccade_on_inds]\n",
    "    saccade_off_inds = np.where(saccade_on_off == -1)[0]\n",
    "    saccade_off_timestamps = df['OE_timestamp'].iloc[saccade_off_inds]\n",
    "    saccade_off_ms = df['ms_axis'].iloc[saccade_off_inds]\n",
    "\n",
    "    saccade_dict = {'saccade_start_ind': saccade_on_inds,\n",
    "                    'saccade_start_timestamp': saccade_on_timestamps.values,\n",
    "                    'saccade_end_ind': saccade_off_inds,\n",
    "                    'saccade_end_timestamp': saccade_off_timestamps.values,\n",
    "                    'saccade_on_ms': saccade_on_ms.values,\n",
    "                    'saccade_off_ms': saccade_off_ms.values}\n",
    "\n",
    "    saccade_events_df = pd.DataFrame.from_dict(saccade_dict)\n",
    "    saccade_events_df['length'] = saccade_events_df['saccade_end_ind'] - saccade_events_df['saccade_start_ind']\n",
    "    # Drop columns used for intermediate steps\n",
    "    df = df.drop(['is_saccade'], axis=1)\n",
    "\n",
    "    distances = []\n",
    "    angles = []\n",
    "    speed_list = []\n",
    "    diameter_list = []\n",
    "    for index, row in tqdm.tqdm(saccade_events_df.iterrows()):\n",
    "        saccade_samples = df.loc[(df['OE_timestamp'] >= row['saccade_start_timestamp']) &\n",
    "                                 (df['OE_timestamp'] <= row['saccade_end_timestamp'])]\n",
    "        distance_traveled = saccade_samples['speed_r'].sum()\n",
    "        if speed_profile:\n",
    "            saccade_speed_profile = saccade_samples['speed_r'].values\n",
    "            speed_list.append(saccade_speed_profile)\n",
    "        saccade_diameter_profile = saccade_samples['pupil_diameter'].values\n",
    "        diameter_list.append(saccade_diameter_profile)\n",
    "        # Calculate angle from initial position to endpoint\n",
    "        initial_position = saccade_samples.iloc[0][['center_x', 'center_y']]\n",
    "        endpoint = saccade_samples.iloc[-1][['center_x', 'center_y']]\n",
    "        overall_angle = np.arctan2(endpoint['center_y'] - initial_position['center_y'],\n",
    "                                   endpoint['center_x'] - initial_position['center_x'])\n",
    "\n",
    "        angles.append(overall_angle)\n",
    "        distances.append(distance_traveled)\n",
    "\n",
    "    saccade_events_df['magnitude_raw'] = np.array(distances)\n",
    "    saccade_events_df['magnitude'] = np.array(distances) * magnitude_calib\n",
    "    saccade_events_df['angle'] = np.where(np.isnan(angles), angles, np.rad2deg(\n",
    "        angles) % 360)  # Convert radians to degrees and ensure result is in [0, 360)\n",
    "    start_ts = saccade_events_df['saccade_start_timestamp'].values\n",
    "    end_ts = saccade_events_df['saccade_end_timestamp'].values\n",
    "    saccade_start_df = df[df['OE_timestamp'].isin(start_ts)]\n",
    "    saccade_end_df = df[df['OE_timestamp'].isin(end_ts)]\n",
    "    start_x_coord = saccade_start_df['center_x']\n",
    "    start_y_coord = saccade_start_df['center_y']\n",
    "    end_x_coord = saccade_end_df['center_x']\n",
    "    end_y_coord = saccade_end_df['center_y']\n",
    "    saccade_events_df['initial_x'] = start_x_coord.values\n",
    "    saccade_events_df['initial_y'] = start_y_coord.values\n",
    "    saccade_events_df['end_x'] = end_x_coord.values\n",
    "    saccade_events_df['end_y'] = end_y_coord.values\n",
    "    saccade_events_df['calib_dx'] = (saccade_events_df['end_x'].values - saccade_events_df[\n",
    "        'initial_x'].values) * magnitude_calib\n",
    "    saccade_events_df['calib_dy'] = (saccade_events_df['end_y'].values - saccade_events_df[\n",
    "        'initial_y'].values) * magnitude_calib\n",
    "    if speed_profile:\n",
    "        saccade_events_df['speed_profile'] = speed_list\n",
    "    saccade_events_df['diameter_profile'] = diameter_list\n",
    "    if bokeh_verify_threshold:\n",
    "        bokeh_plotter(data_list=[df.speed_r], label_list=['Pupil Velocity'], peaks=saccade_on_inds)\n",
    "\n",
    "    return df, saccade_events_df\n",
    "\n",
    "\n",
    "# create a multi-animal block_collection:\n",
    "\n",
    "def create_block_collections(animals, block_lists, experiment_path, bad_blocks=None):\n",
    "    \"\"\"\n",
    "    Create block collections and a block dictionary from multiple animals and their respective block lists.\n",
    "\n",
    "    Parameters:\n",
    "    - animals: list of str, names of the animals.\n",
    "    - block_lists: list of lists of int, block numbers corresponding to each animal.\n",
    "    - experiment_path: pathlib.Path, path to the experiment directory.\n",
    "    - bad_blocks: list of int, blocks to exclude. Default is an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - block_collection: list of BlockSync objects for all specified blocks.\n",
    "    - block_dict: dictionary where keys are block numbers as strings and values are BlockSync objects.\n",
    "    \"\"\"\n",
    "    import UtilityFunctions_newOE as uf\n",
    "\n",
    "    if bad_blocks is None:\n",
    "        bad_blocks = []\n",
    "\n",
    "    block_collection = []\n",
    "    block_dict = {}\n",
    "\n",
    "    for animal, blocks in zip(animals, block_lists):\n",
    "        # Generate blocks for the current animal\n",
    "        current_blocks = uf.block_generator(\n",
    "            block_numbers=blocks,\n",
    "            experiment_path=experiment_path,\n",
    "            animal=animal,\n",
    "            bad_blocks=bad_blocks\n",
    "        )\n",
    "        # Add to collection and dictionary\n",
    "        block_collection.extend(current_blocks)\n",
    "        for b in current_blocks:\n",
    "            block_dict[f\"{animal}_block_{b.block_num}\"] = b\n",
    "\n",
    "    return block_collection, block_dict\n",
    "\n",
    "\n",
    "def load_self_kerr_refs(block, filename: str = \"self_kerr_refs.csv\") -> bool:\n",
    "    \"\"\"\n",
    "    Load Kerr reference coordinates from the analysis folder CSV and set them on `block`.\n",
    "\n",
    "    Reads a single-row CSV with columns:\n",
    "        kerr_ref_r_x, kerr_ref_r_y, kerr_ref_l_x, kerr_ref_l_y\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if refs were loaded and applied, False if the file was missing or empty.\n",
    "    \"\"\"\n",
    "    path = pathlib.Path(block.analysis_path) / filename\n",
    "    if not path.exists():\n",
    "        print(f\"No Kerr refs file found at: {path}\")\n",
    "        return False\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    if df.empty:\n",
    "        print(f\"Kerr refs file is empty: {path}\")\n",
    "        return False\n",
    "\n",
    "    row = df.iloc[0]\n",
    "\n",
    "    # Helper to safely set attribute if value is finite\n",
    "    def _set_attr(name):\n",
    "        if name in row and pd.notna(row[name]):\n",
    "            try:\n",
    "                setattr(block, name, int(round(float(row[name]))))\n",
    "            except (ValueError, TypeError):\n",
    "                # keep existing value if conversion fails\n",
    "                pass\n",
    "\n",
    "    for col in (\"kerr_ref_r_x\", \"kerr_ref_r_y\", \"kerr_ref_l_x\", \"kerr_ref_l_y\"):\n",
    "        _set_attr(col)\n",
    "\n",
    "    print(f\"Kerr refs loaded from: {path}\")\n",
    "    return True"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T15:43:07.510152Z",
     "start_time": "2025-11-04T15:43:07.060954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#animals = ['PV_62', 'PV_126', 'PV_57']\n",
    "#block_lists = [[24, 26, 38], [7, 8, 9, 10, 11, 12], [7, 8, 9, 12, 13]]\n",
    "#experiment_path = pathlib.Path(r\"Z:\\Nimrod\\experiments\")\n",
    "animals = ['PV_62']\n",
    "block_lists = [[24,26]]\n",
    "experiment_path = pathlib.Path(r\"Z:\\Nimrod\\experiments\")\n",
    "\n",
    "bad_blocks = [0]  # Example of bad blocks\n",
    "\n",
    "block_collection, block_dict = create_block_collections(\n",
    "    animals=animals,\n",
    "    block_lists=block_lists,\n",
    "    experiment_path=experiment_path,\n",
    "    bad_blocks=bad_blocks\n",
    ")"
   ],
   "id": "9bed13ed9213f953",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiated block number 024 at Path: Z:\\Nimrod\\experiments\\PV_62\\2023_04_27\\block_024, new OE version\n",
      "Found the sample rate for block 024 in the xml file, it is 20000 Hz\n",
      "created the .oe_rec attribute as an open ephys recording obj with get_data functionality\n",
      "retrieving zertoh sample number for block 024\n",
      "got it!\n",
      "instantiated block number 026 at Path: Z:\\Nimrod\\experiments\\PV_62\\2023_04_27\\block_026, new OE version\n",
      "Found the sample rate for block 026 in the xml file, it is 20000 Hz\n",
      "created the .oe_rec attribute as an open ephys recording obj with get_data functionality\n",
      "retrieving zertoh sample number for block 026\n",
      "got it!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-04T15:43:13.309150Z",
     "start_time": "2025-11-04T15:43:08.265063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for block in block_collection:\n",
    "    block.parse_open_ephys_events()\n",
    "    block.get_eye_brightness_vectors()\n",
    "    #block.create_eye_brightness_df(threshold_value=20)\n",
    "    block.handle_eye_videos()\n",
    "    # if the code fails here, go to manual synchronization\n",
    "    block.calibrate_pixel_size(10)\n",
    "    load_eye_data_2d_w_rotation_matrix(block)\n",
    "\n"
   ],
   "id": "df2ca2a9a0bd6247",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running parse_open_ephys_events...\n",
      "block 024 has a parsed events file, reading...\n",
      "Getting eye brightness values for block 024...\n",
      "Found an existing file!\n",
      "Eye brightness vectors generation complete.\n",
      "handling eye video files\n",
      "converting videos...\n",
      "converting files: ['Z:\\\\Nimrod\\\\experiments\\\\PV_62\\\\2023_04_27\\\\block_024\\\\eye_videos\\\\LE\\\\230427_pv62_trial2_640x480_60hz_experiment_1_recording_0\\\\230427_pv62_trial2.h264', 'Z:\\\\Nimrod\\\\experiments\\\\PV_62\\\\2023_04_27\\\\block_024\\\\eye_videos\\\\RE\\\\230427_pv62_trial2_640x480_60hz_experiment_1_recording_0\\\\230427_pv62_trial2.h264'] \n",
      " avoiding conversion on files: ['Z:\\\\Nimrod\\\\experiments\\\\PV_62\\\\2023_04_27\\\\block_024\\\\eye_videos\\\\LE\\\\230427_pv62_trial2_640x480_60hz_experiment_1_recording_0\\\\230427_pv62_trial2_LE.mp4', 'Z:\\\\Nimrod\\\\experiments\\\\PV_62\\\\2023_04_27\\\\block_024\\\\eye_videos\\\\LE\\\\230427_pv62_trial2_640x480_60hz_experiment_1_recording_0\\\\prev_analysis\\\\stabilized_demo_LE.mp4', 'Z:\\\\Nimrod\\\\experiments\\\\PV_62\\\\2023_04_27\\\\block_024\\\\eye_videos\\\\RE\\\\230427_pv62_trial2_640x480_60hz_experiment_1_recording_0\\\\230427_pv62_trial2.mp4']\n",
      "The file Z:\\Nimrod\\experiments\\PV_62\\2023_04_27\\block_024\\eye_videos\\RE\\230427_pv62_trial2_640x480_60hz_experiment_1_recording_0\\230427_pv62_trial2.mp4 already exists, no conversion necessary\n",
      "Validating videos...\n",
      "something wrong with the inspection, numbers of files does not match:\n",
      "videos_to_inspect = ['Z:\\\\Nimrod\\\\experiments\\\\PV_62\\\\2023_04_27\\\\block_024\\\\eye_videos\\\\LE\\\\230427_pv62_trial2_640x480_60hz_experiment_1_recording_0\\\\230427_pv62_trial2_LE.mp4', 'Z:\\\\Nimrod\\\\experiments\\\\PV_62\\\\2023_04_27\\\\block_024\\\\eye_videos\\\\LE\\\\230427_pv62_trial2_640x480_60hz_experiment_1_recording_0\\\\prev_analysis\\\\stabilized_demo_LE.mp4', 'Z:\\\\Nimrod\\\\experiments\\\\PV_62\\\\2023_04_27\\\\block_024\\\\eye_videos\\\\RE\\\\230427_pv62_trial2_640x480_60hz_experiment_1_recording_0\\\\230427_pv62_trial2.mp4']\n",
      "timestamps_to_inspect = ['Z:\\\\Nimrod\\\\experiments\\\\PV_62\\\\2023_04_27\\\\block_024\\\\eye_videos\\\\LE\\\\230427_pv62_trial2_640x480_60hz_experiment_1_recording_0\\\\230427_pv62_trial2_timestamps.csv', 'Z:\\\\Nimrod\\\\experiments\\\\PV_62\\\\2023_04_27\\\\block_024\\\\eye_videos\\\\RE\\\\230427_pv62_trial2_640x480_60hz_experiment_1_recording_0\\\\230427_pv62_trial2_timestamps.csv']\n",
      "got the calibration values from the analysis folder\n",
      "running parse_open_ephys_events...\n",
      "block 026 has a parsed events file, reading...\n",
      "Getting eye brightness values for block 026...\n",
      "Found an existing file!\n",
      "Eye brightness vectors generation complete.\n",
      "handling eye video files\n",
      "converting videos...\n",
      "converting files: ['Z:\\\\Nimrod\\\\experiments\\\\PV_62\\\\2023_04_27\\\\block_026\\\\eye_videos\\\\LE\\\\230427_pv62_trial4_640x480_60hz_experiment_1_recording_0\\\\230427_pv62_trial4.h264', 'Z:\\\\Nimrod\\\\experiments\\\\PV_62\\\\2023_04_27\\\\block_026\\\\eye_videos\\\\RE\\\\230427_pv62_trial4_640x480_60hz_experiment_1_recording_0\\\\230427_pv62_trial4.h264'] \n",
      " avoiding conversion on files: ['Z:\\\\Nimrod\\\\experiments\\\\PV_62\\\\2023_04_27\\\\block_026\\\\eye_videos\\\\LE\\\\230427_pv62_trial4_640x480_60hz_experiment_1_recording_0\\\\230427_pv62_trial4_LE.mp4', 'Z:\\\\Nimrod\\\\experiments\\\\PV_62\\\\2023_04_27\\\\block_026\\\\eye_videos\\\\RE\\\\230427_pv62_trial4_640x480_60hz_experiment_1_recording_0\\\\230427_pv62_trial4.mp4']\n",
      "The file Z:\\Nimrod\\experiments\\PV_62\\2023_04_27\\block_026\\eye_videos\\RE\\230427_pv62_trial4_640x480_60hz_experiment_1_recording_0\\230427_pv62_trial4.mp4 already exists, no conversion necessary\n",
      "Validating videos...\n",
      "The video named 230427_pv62_trial4_LE.mp4 has reported 107642 frames and has 107642 frames, it has dropped 0 frames\n",
      "The video named 230427_pv62_trial4.mp4 has reported 107640 frames and has 107640 frames, it has dropped 0 frames\n",
      "got the calibration values from the analysis folder\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CAREFUL! THIS CELL USES THE ROTATION MATRICES TO ROTATE THE DATA AND REFERENCE POINTS TOGETHER BEFORE DEGREE CONVERSION:\n",
    "\n",
    "def apply_rotation(df: pd.DataFrame, rot_mat: np.ndarray, rot_angle: float) -> pd.DataFrame:\n",
    "    df2 = df.copy()\n",
    "    pts = df2[['center_x', 'center_y']].values.reshape(-1, 1, 2).astype(np.float32)\n",
    "    pts2 = cv2.transform(pts, rot_mat.astype(np.float32))\n",
    "    df2['center_x'] = pts2[:, 0, 0]\n",
    "    df2['center_y'] = pts2[:, 0, 1]\n",
    "    df2['phi'] = (df2['phi'] + rot_angle) % 360\n",
    "    return df2\n",
    "\n",
    "for block in block_collection:\n",
    "    # Rotate left eye data in memory\n",
    "    if hasattr(block, 'left_eye_data') and block.left_eye_data is not None:\n",
    "        R_L = np.array(block.left_rotation_matrix, dtype=np.float32)\n",
    "        ang_L = float(block.left_rotation_angle)\n",
    "        block.left_eye_data = apply_rotation(block.left_eye_data, R_L, ang_L)\n",
    "\n",
    "    # Rotate right eye data in memory\n",
    "    if hasattr(block, 'right_eye_data') and block.right_eye_data is not None:\n",
    "        R_R = np.array(block.right_rotation_matrix, dtype=np.float32)\n",
    "        ang_R = float(block.right_rotation_angle)\n",
    "        block.right_eye_data = apply_rotation(block.right_eye_data, R_R, ang_R)\n",
    "\n",
    "    print(f\"Applied rotation to block {block.animal_call}-{block.block_num}\")"
   ],
   "id": "d42f9ba7204d2475",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T15:47:31.120071Z",
     "start_time": "2025-11-04T15:47:31.095043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for block in block_collection:\n",
    "    #load_self_kerr_refs(block)\n",
    "    #block.load_best_reference(r'Z:\\Nimrod\\experiments\\cross_animals_data\\kerr_reference_all_animals_current_25_05_12.csv')"
   ],
   "id": "f8d93e2512f05e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found reference file and loaded points 289.0 261.0 316.0 385.0\n",
      "found reference file and loaded points 288.0 282.0 326.0 368.0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This one rotates the reference points\n",
    "for block in block_collection:\n",
    "    # Rotate left reference point\n",
    "    if hasattr(block, 'kerr_ref_l_x') and block.kerr_ref_l_x is not None:\n",
    "        x0, y0 = block.kerr_ref_l_x, block.kerr_ref_l_y\n",
    "        pt = np.array([[[x0, y0]]], dtype=np.float32)\n",
    "        R_L = np.array(block.left_rotation_matrix, dtype=np.float32)\n",
    "        pt_rot = cv2.transform(pt, R_L)\n",
    "        # Update in-place\n",
    "        block.kerr_ref_l_x = float(pt_rot[0, 0, 0])\n",
    "        block.kerr_ref_l_y = float(pt_rot[0, 0, 1])\n",
    "\n",
    "    # Rotate right reference point\n",
    "    if hasattr(block, 'kerr_ref_r_x') and block.kerr_ref_r_x is not None:\n",
    "        x0, y0 = block.kerr_ref_r_x, block.kerr_ref_r_y\n",
    "        pt = np.array([[[x0, y0]]], dtype=np.float32)\n",
    "        R_R = np.array(block.right_rotation_matrix, dtype=np.float32)\n",
    "        pt_rot = cv2.transform(pt, R_R)\n",
    "        block.kerr_ref_r_x = float(pt_rot[0, 0, 0])\n",
    "        block.kerr_ref_r_y = float(pt_rot[0, 0, 1])\n",
    "\n",
    "    print(f\"Block {block.animal_call}-{block.block_num}:\")\n",
    "    print(f\"  Rotated left ref -> ({block.kerr_ref_l_x:.2f}, {block.kerr_ref_l_y:.2f})\")\n",
    "    print(f\"  Rotated right ref-> ({block.kerr_ref_r_x:.2f}, {block.kerr_ref_r_y:.2f})\")\n"
   ],
   "id": "1f61ea7c53ec508",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T16:03:09.949354Z",
     "start_time": "2025-11-04T16:03:04.610964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "name_tag = 'raw_verified'\n",
    "for block in block_collection:\n",
    "    # Here is where the conversion happens:\n",
    "    block.calculate_kerr_angles(name_tag=name_tag)\n"
   ],
   "id": "5bd4d6b74fcdef0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on Block 024\n",
      "Left eye\n",
      "Left eye\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MarkS3\\PycharmProjects\\pythonProject3\\BlockSync_current.py:3002: RuntimeWarning: invalid value encountered in arcsin\n",
      "  comp_t = np.arcsin((bPC_values[valid_positions] - bEC) / (np.cos(comp_p) * f_z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished successfully and saved to Z:\\Nimrod\\experiments\\PV_62\\2023_04_27\\block_024\\analysis with tag= raw_verified\n",
      "working on Block 026\n",
      "Left eye\n",
      "Left eye\n",
      "finished successfully and saved to Z:\\Nimrod\\experiments\\PV_62\\2023_04_27\\block_026\\analysis with tag= raw_verified\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-04T16:03:10.713577Z",
     "start_time": "2025-11-04T16:03:09.966388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load and combine the eye data with the angle calculation\n",
    "name_tag = 'raw_verified'\n",
    "def append_angle_data(eye_df, new_df):\n",
    "    \"\"\"\n",
    "    Appends the angle columns (phi and theta) from new_df to eye_df.\n",
    "    The function renames 'phi' to 'k_phi' and 'theta' to 'k_theta', then merges\n",
    "    on the shared 'OE_timestamp' column.\n",
    "\n",
    "    Parameters:\n",
    "    - eye_df: pandas DataFrame containing the eye tracking data.\n",
    "    - new_df: pandas DataFrame containing the new kinematics data with columns\n",
    "              'phi' and 'theta' along with 'OE_timestamp' (and possibly others).\n",
    "\n",
    "    Returns:\n",
    "    - merged_df: pandas DataFrame resulting from merging the new kinematics data\n",
    "                 into eye_df.\n",
    "    \"\"\"\n",
    "    # Select the necessary columns and rename them\n",
    "    angle_data = new_df[['OE_timestamp', 'phi', 'theta']].rename(\n",
    "        columns={'phi': 'k_phi', 'theta': 'k_theta'}\n",
    "    )\n",
    "\n",
    "    # Merge on OE_timestamp using a left join to preserve all rows in eye_df\n",
    "    merged_df = pd.merge(eye_df, angle_data, on='OE_timestamp', how='left')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "for block in block_collection:\n",
    "    print(block)\n",
    "    try:\n",
    "        left_angles = pd.read_csv([i for i in block.analysis_path.iterdir() if (f'left_kerr_angle_{name_tag}.csv' in str(i))][0])\n",
    "        right_angles = pd.read_csv([i for i in block.analysis_path.iterdir() if (f'right_kerr_angle_{name_tag}.csv' in str(i))][0])\n",
    "    except IndexError:\n",
    "        print(f'{block} has a problem, files missing')\n",
    "\n",
    "    block.left_eye_data = append_angle_data(block.left_eye_data,left_angles)\n",
    "    block.right_eye_data = append_angle_data(block.right_eye_data,right_angles)\n",
    "\n"
   ],
   "id": "ec5a9a95a3f2e2a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PV_62, block 024, on 2023-04-27_11-22-56\n",
      "PV_62, block 026, on 2023-04-27_12-21-41\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bc60603dd86a25fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T16:03:11.040836Z",
     "start_time": "2025-11-04T16:03:11.030815Z"
    }
   },
   "cell_type": "code",
   "source": "name_tag",
   "id": "f3095edcfcd21cba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raw_verified'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-04T16:03:34.526085Z",
     "start_time": "2025-11-04T16:03:14.822547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def export_eye_data_w_angles(block, name_tag='0'):\n",
    "    block.right_eye_data.to_csv(block.analysis_path / f'right_eye_data_{name_tag}.csv')\n",
    "    block.left_eye_data.to_csv(block.analysis_path / f'left_eye_data_{name_tag}.csv')\n",
    "\n",
    "for block in block_collection:\n",
    "    export_eye_data_w_angles(block, name_tag='degrees_raw_verified')"
   ],
   "id": "e9225ab14832669c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:44:12.771378Z",
     "start_time": "2025-10-24T13:44:12.761407Z"
    }
   },
   "cell_type": "code",
   "source": "block_collection",
   "id": "621935018e3c344",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BlockSync object for animal PV_126 with \n",
       " block_num 007 at date PV126_Trial16_wake3_2024-07-18_12-49-12]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T18:39:43.924627Z",
     "start_time": "2025-10-26T18:39:14.830524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "name_tag = 'degrees_raw_verified'\n",
    "for block in block_collection:\n",
    "    print(block.analysis_path / f'right_eye_data_{name_tag}.csv')\n",
    "    block.right_eye_data.to_csv(block.analysis_path / f'right_eye_data_{name_tag}.csv')\n",
    "    block.left_eye_data.to_csv(block.analysis_path / f'left_eye_data_{name_tag}.csv')"
   ],
   "id": "c186d1d4d0854ce7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:\\Nimrod\\experiments\\PV_126\\2024_07_18\\block_007\\analysis\\right_eye_data_degrees_raw_verified.csv\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c468abc84bc61b54"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
