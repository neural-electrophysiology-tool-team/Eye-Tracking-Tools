{
 "cells": [
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "from itertools import cycle\n",
    "import pickle\n",
    "import pathlib\n",
    "import math\n",
    "import tqdm\n",
    "import scipy.io\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.io\n",
    "import h5py\n",
    "import re\n",
    "from lxml import etree as ET\n",
    "import scipy.signal as sig\n",
    "import pandas as pd\n",
    "from scipy.stats import kde\n",
    "from BlockSync_current import BlockSync\n",
    "import UtilityFunctions_newOE as uf\n",
    "from scipy import signal\n",
    "import bokeh\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "rcParams['pdf.fonttype'] = 42  # Ensure fonts are embedded and editable\n",
    "rcParams['ps.fonttype'] = 42  # Ensure compatibility with vector outputs\n",
    "\n",
    "\n",
    "def bokeh_plotter(data_list, x_axis_list=None, label_list=None,\n",
    "                  plot_name='default',\n",
    "                  x_axis_label='X', y_axis_label='Y',\n",
    "                  peaks=None, peaks_list=False, export_path=False):\n",
    "    \"\"\"Generates an interactive Bokeh plot for the given data vector.\n",
    "    Args:\n",
    "        data_list (list or array): The data to be plotted.\n",
    "        label_list (list of str): The labels of the data vectors\n",
    "        plot_name (str, optional): The title of the plot. Defaults to 'default'.\n",
    "        x_axis (str, optional): The label for the x-axis. Defaults to 'X'.\n",
    "        y_axis (str, optional): The label for the y-axis. Defaults to 'Y'.\n",
    "        peaks (list or array, optional): Indices of peaks to highlight on the plot. Defaults to None.\n",
    "        export_path (False or str): when set to str, will output the resulting html fig\n",
    "    \"\"\"\n",
    "    color_cycle = cycle(bokeh.palettes.Category10_10)\n",
    "    fig = bokeh.plotting.figure(title=f'bokeh explorer: {plot_name}',\n",
    "                                x_axis_label=x_axis_label,\n",
    "                                y_axis_label=y_axis_label,\n",
    "                                plot_width=1500,\n",
    "                                plot_height=700)\n",
    "\n",
    "    for i, data_vector in enumerate(data_list):\n",
    "\n",
    "        color = next(color_cycle)\n",
    "\n",
    "        if x_axis_list is None:\n",
    "            x_axis = range(len(data_vector))\n",
    "        elif len(x_axis_list) == len(data_list):\n",
    "            print('x_axis manually set')\n",
    "            x_axis = x_axis_list[i]\n",
    "        else:\n",
    "            raise Exception('problem with x_axis_list input - should be either None, or a list with the same length as data_list')\n",
    "        if label_list is None:\n",
    "            fig.line(x_axis, data_vector, line_color=color, legend_label=f\"Line {i+1}\")\n",
    "        elif len(label_list) == len(data_list):\n",
    "            fig.line(range(len(data_vector)), data_vector, line_color=color, legend_label=f\"{label_list[i]}\")\n",
    "        if peaks is not None and peaks_list is True:\n",
    "            fig.circle(peaks[i], data_vector[peaks[i]], size=10, color=color)\n",
    "\n",
    "    if peaks is not None and peaks_list is False:\n",
    "        fig.circle(peaks, data_vector[peaks], size=10, color='red')\n",
    "\n",
    "    if export_path is not False:\n",
    "        print(f'exporting to {export_path}')\n",
    "        bokeh.io.output.output_file(filename=str(export_path / f'{plot_name}.html'), title=f'{plot_name}')\n",
    "    bokeh.plotting.show(fig)\n",
    "\n",
    "\n",
    "def load_eye_data_2d_w_rotation_matrix(block):\n",
    "    \"\"\"\n",
    "    This function checks if the eye dataframes and rotation dict object exist, then imports them\n",
    "    :param block: The current blocksync class with verifiec re/le dfs\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        block.left_eye_data = pd.read_csv(block.analysis_path / 'left_eye_data.csv', index_col=0, engine='python')\n",
    "        block.right_eye_data = pd.read_csv(block.analysis_path / 'right_eye_data.csv', index_col=0, engine='python')\n",
    "    except FileNotFoundError:\n",
    "        print('eye_data files not found, run the pipeline!')\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(block.analysis_path / 'rotate_eye_data_params.pkl', 'rb') as file:\n",
    "            rotation_dict = pickle.load(file)\n",
    "            block.left_rotation_matrix = rotation_dict['left_rotation_matrix']\n",
    "            block.right_rotation_matrix = rotation_dict['right_rotation_matrix']\n",
    "            block.left_rotation_angle = rotation_dict['left_rotation_angle']\n",
    "            block.right_rotation_angle = rotation_dict['right_rotation_angle']\n",
    "    except FileNotFoundError:\n",
    "        print('No rotation matrix file, create it')\n",
    "\n",
    "\n",
    "def create_saccade_events_df(eye_data_df, speed_threshold, bokeh_verify_threshold=False, magnitude_calib=1, speed_profile=True):\n",
    "    \"\"\"\n",
    "    Detects saccade events in eye tracking data and computes relevant metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - eye_data_df (pd.DataFrame): Input DataFrame containing eye tracking data.\n",
    "    - speed_threshold (float): Threshold for saccade detection based on speed.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): Modified input DataFrame with added columns for speed and saccade detection.\n",
    "    - saccade_events_df (pd.DataFrame): DataFrame containing information about detected saccade events.\n",
    "\n",
    "    Steps:\n",
    "    1. Calculate speed components ('speed_x', 'speed_y') based on differences in 'center_x' and 'center_y'.\n",
    "    2. Compute the magnitude of the velocity vector ('speed_r').\n",
    "    3. Create a binary column ('is_saccade') indicating saccade events based on the speed threshold.\n",
    "    4. Determine saccade onset and offset indices and timestamps.\n",
    "    5. Create a DataFrame ('saccade_events_df') with columns:\n",
    "        - 'saccade_start_ind': Indices of saccade onset.\n",
    "        - 'saccade_start_timestamp': Timestamps corresponding to saccade onset.\n",
    "        - 'saccade_end_ind': Indices of saccade offset.\n",
    "        - 'saccade_end_timestamp': Timestamps corresponding to saccade offset.\n",
    "        - 'length': Duration of each saccade event.\n",
    "    6. Calculate distance traveled and angles for each saccade event.\n",
    "    7. Append additional columns to 'saccade_events_df':\n",
    "        - 'magnitude': Magnitude of the distance traveled during each saccade.\n",
    "        - 'angle': Angle of the saccade vector in degrees.\n",
    "        - 'initial_x', 'initial_y': Initial coordinates of the saccade.\n",
    "        - 'end_x', 'end_y': End coordinates of the saccade.\n",
    "\n",
    "    Note: The original 'eye_data_df' is not modified; modified data is returned as 'df'.\n",
    "    \"\"\"\n",
    "    df = eye_data_df\n",
    "    df['speed_x'] = df['center_x'].diff()  # Difference between consecutive 'center_x' values\n",
    "    df['speed_y'] = df['center_y'].diff()  # Difference between consecutive 'center_y' values\n",
    "\n",
    "    # Step 2: Calculate magnitude of the velocity vector (R vector speed)\n",
    "    df['speed_r'] = (df['speed_x']**2 + df['speed_y']**2)**0.5\n",
    "\n",
    "    # Create a column for saccade detection\n",
    "    df['is_saccade'] = df['speed_r'] > speed_threshold\n",
    "\n",
    "    # create a saccade_on_off indicator where 1 is rising edge and -1 is falling edge by subtracting a shifted binary mask\n",
    "    saccade_on_off = df.is_saccade.astype(int) - df.is_saccade.shift(periods=1,fill_value=False).astype(int)\n",
    "    saccade_on_inds = np.where(saccade_on_off == 1)[0] - 1 # notice the manual shift here, chosen to include the first (sometimes slower) eye frame, just before saccade threshold crossing\n",
    "    saccade_on_ms = df['ms_axis'].iloc[saccade_on_inds]\n",
    "    saccade_on_timestamps = df['OE_timestamp'].iloc[saccade_on_inds]\n",
    "    saccade_off_inds = np.where(saccade_on_off == -1)[0]\n",
    "    saccade_off_timestamps = df['OE_timestamp'].iloc[saccade_off_inds]\n",
    "    saccade_off_ms = df['ms_axis'].iloc[saccade_off_inds]\n",
    "\n",
    "    saccade_dict = {'saccade_start_ind' :  saccade_on_inds ,\n",
    "                    'saccade_start_timestamp': saccade_on_timestamps.values,\n",
    "                    'saccade_end_ind':      saccade_off_inds,\n",
    "                    'saccade_end_timestamp':saccade_off_timestamps.values,\n",
    "                    'saccade_on_ms': saccade_on_ms.values,\n",
    "                    'saccade_off_ms': saccade_off_ms.values}\n",
    "\n",
    "    saccade_events_df = pd.DataFrame.from_dict(saccade_dict)\n",
    "    saccade_events_df['length'] = saccade_events_df['saccade_end_ind'] - saccade_events_df['saccade_start_ind']\n",
    "    # Drop columns used for intermediate steps\n",
    "    df = df.drop(['is_saccade'], axis=1)\n",
    "\n",
    "    distances = []\n",
    "    angles = []\n",
    "    speed_list = []\n",
    "    speed_list_calib = []\n",
    "    diameter_list = []\n",
    "    for index, row in tqdm.tqdm(saccade_events_df.iterrows()):\n",
    "        saccade_samples = df.loc[(df['OE_timestamp'] >= row['saccade_start_timestamp']) &\n",
    "                                 (df['OE_timestamp'] <= row['saccade_end_timestamp'])]\n",
    "        distance_traveled = saccade_samples['speed_r'].sum()\n",
    "        if speed_profile:\n",
    "            saccade_speed_profile = saccade_samples['speed_r'].values\n",
    "            saccade_calib_speed_profile = saccade_samples['speed_r'].values * magnitude_calib\n",
    "            speed_list.append(saccade_speed_profile)\n",
    "            speed_list_calib.append(saccade_calib_speed_profile)\n",
    "        saccade_diameter_profile = saccade_samples['pupil_diameter'].values\n",
    "        diameter_list.append(saccade_diameter_profile)\n",
    "        # Calculate angle from initial position to endpoint\n",
    "        initial_position = saccade_samples.iloc[0][['center_x', 'center_y']]\n",
    "        endpoint = saccade_samples.iloc[-1][['center_x', 'center_y']]\n",
    "        overall_angle = np.arctan2(endpoint['center_y'] - initial_position['center_y'],\n",
    "                           endpoint['center_x'] - initial_position['center_x'])\n",
    "\n",
    "        angles.append(overall_angle)\n",
    "        distances.append(distance_traveled)\n",
    "\n",
    "\n",
    "\n",
    "    saccade_events_df['magnitude_raw'] = np.array(distances)\n",
    "    saccade_events_df['magnitude'] = np.array(distances) * magnitude_calib\n",
    "    saccade_events_df['angle'] = np.where(np.isnan(angles), angles, np.rad2deg(angles) % 360) # Convert radians to degrees and ensure result is in [0, 360)\n",
    "    start_ts = saccade_events_df['saccade_start_timestamp'].values\n",
    "    end_ts = saccade_events_df['saccade_end_timestamp'].values\n",
    "    saccade_start_df = df[df['OE_timestamp'].isin(start_ts)]\n",
    "    saccade_end_df = df[df['OE_timestamp'].isin(end_ts)]\n",
    "    start_x_coord = saccade_start_df['center_x']\n",
    "    start_y_coord = saccade_start_df['center_y']\n",
    "    end_x_coord = saccade_end_df['center_x']\n",
    "    end_y_coord = saccade_end_df['center_y']\n",
    "    saccade_events_df['initial_x'] = start_x_coord.values\n",
    "    saccade_events_df['initial_y'] = start_y_coord.values\n",
    "    saccade_events_df['end_x'] = end_x_coord.values\n",
    "    saccade_events_df['end_y'] = end_y_coord.values\n",
    "    saccade_events_df['calib_dx'] = (saccade_events_df['end_x'].values - saccade_events_df['initial_x'].values) * magnitude_calib\n",
    "    saccade_events_df['calib_dy'] = (saccade_events_df['end_y'].values - saccade_events_df['initial_y'].values) * magnitude_calib\n",
    "    if speed_profile:\n",
    "        saccade_events_df['speed_profile'] = speed_list\n",
    "        saccade_events_df['speed_profile_calib'] = speed_list_calib\n",
    "    saccade_events_df['diameter_profile'] = diameter_list\n",
    "    if bokeh_verify_threshold:\n",
    "        bokeh_plotter(data_list=[df.speed_r], label_list=['Pupil Velocity'], peaks=saccade_on_inds)\n",
    "\n",
    "    return df, saccade_events_df\n"
   ],
   "id": "995927d1ed6780e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# create a multi-animal block_collection:\n",
    "\n",
    "def create_block_collections(animals, block_lists, experiment_path, bad_blocks=None):\n",
    "    \"\"\"\n",
    "    Create block collections and a block dictionary from multiple animals and their respective block lists.\n",
    "\n",
    "    Parameters:\n",
    "    - animals: list of str, names of the animals.\n",
    "    - block_lists: list of lists of int, block numbers corresponding to each animal.\n",
    "    - experiment_path: pathlib.Path, path to the experiment directory.\n",
    "    - bad_blocks: list of int, blocks to exclude. Default is an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - block_collection: list of BlockSync objects for all specified blocks.\n",
    "    - block_dict: dictionary where keys are block numbers as strings and values are BlockSync objects.\n",
    "    \"\"\"\n",
    "    import UtilityFunctions_newOE as uf\n",
    "\n",
    "    if bad_blocks is None:\n",
    "        bad_blocks = []\n",
    "\n",
    "    block_collection = []\n",
    "    block_dict = {}\n",
    "\n",
    "    for animal, blocks in zip(animals, block_lists):\n",
    "        # Generate blocks for the current animal\n",
    "        current_blocks = uf.block_generator(\n",
    "            block_numbers=blocks,\n",
    "            experiment_path=experiment_path,\n",
    "            animal=animal,\n",
    "            bad_blocks=bad_blocks\n",
    "        )\n",
    "        # Add to collection and dictionary\n",
    "        block_collection.extend(current_blocks)\n",
    "        for b in current_blocks:\n",
    "            block_dict[f\"{animal}_block_{b.block_num}\"] = b\n",
    "\n",
    "    return block_collection, block_dict\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "1fb642b30f741984",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "animals = ['PV_106','PV_143','PV_62','PV_126', 'PV_57']\n",
    "block_lists = [[8,9,10,11,12],[1,2,3,4],[24, 26, 38],[7, 8, 9, 10, 11, 12], [7, 8, 9, 12, 13]]\n",
    "\n",
    "experiment_path = pathlib.Path(r\"Z:\\Nimrod\\experiments\")\n",
    "bad_blocks = [0]  # Example of bad blocks\n",
    "\n",
    "block_collection, block_dict = create_block_collections(\n",
    "    animals=animals,\n",
    "    block_lists=block_lists,\n",
    "    experiment_path=experiment_path,\n",
    "    bad_blocks=bad_blocks)"
   ],
   "id": "6803a7068959ac0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "for block in block_collection:\n",
    "    block.parse_open_ephys_events()\n",
    "    block.get_eye_brightness_vectors()\n",
    "    block.synchronize_block()\n",
    "    block.create_eye_brightness_df(threshold_value=20)\n",
    "\n",
    "    # if the code fails here, go to manual synchronization\n",
    "    block.import_manual_sync_df()\n",
    "    block.read_dlc_data()\n",
    "    block.calibrate_pixel_size(10)\n",
    "    load_eye_data_2d_w_rotation_matrix(block) #should be integrated again... later\n",
    "\n",
    "# # NOTICE THE DIRECTIONALITY FLIP!!!\n",
    "#     block.left_eye_data = horizontal_flip_eye_data(block.left_eye_data.copy(),640)\n",
    "#     block.right_eye_data = horizontal_flip_eye_data(block.right_eye_data.copy(),640)\n",
    "    # calibrate pupil diameter:\n",
    "for block in block_collection:\n",
    "    block.left_eye_data = pd.read_csv(block.analysis_path / f'left_eye_data_degrees_raw_verified.csv')\n",
    "    block.right_eye_data = pd.read_csv(block.analysis_path / 'right_eye_data_degrees_raw_verified.csv')\n",
    "\n"
   ],
   "id": "2d7ac76096300a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " for block in block_collection:\n",
    "    if 'pupil_diameter' not in block.left_eye_data.columns:\n",
    "        block.left_eye_data['pupil_diameter_pixels'] = block.left_eye_data.major_ax\n",
    "        block.right_eye_data['pupil_diameter_pixels'] = block.right_eye_data.major_ax\n",
    "        block.left_eye_data['pupil_diameter'] = block.left_eye_data['pupil_diameter_pixels'] * block.L_pix_size\n",
    "        block.right_eye_data['pupil_diameter'] = block.right_eye_data['pupil_diameter_pixels'] * block.R_pix_size"
   ],
   "id": "ce66f600798501c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_saccade_events_with_direction_segmentation_robust(\n",
    "        eye_data_df,\n",
    "        speed_threshold,  # angular speed threshold in degrees/frame\n",
    "        directional_delta_threshold_deg=25,  # threshold for change in instantaneous angle (degrees)\n",
    "        magnitude_calib=1,\n",
    "        speed_profile=True,\n",
    "        min_subsaccade_samples=2,\n",
    "        min_net_disp=0.5  # minimal net angular displacement (in degrees) for a segment to be valid\n",
    "):\n",
    "    \"\"\"\n",
    "    Detects and segments saccade events in eye tracking data using angular speed and directional changes.\n",
    "    This robust version avoids producing segments with near-zero net displacement by:\n",
    "      1. Defining the saccade onset as the first frame where the angular speed exceeds the threshold.\n",
    "      2. Segmenting the event based on sustained directional changes.\n",
    "      3. Discarding segments whose overall net angular displacement (computed from k_phi and k_theta)\n",
    "         is below a user-specified minimal value.\n",
    "\n",
    "    Parameters:\n",
    "      - eye_data_df (pd.DataFrame): DataFrame with columns including:\n",
    "            'center_x', 'center_y', 'k_phi', 'k_theta', 'OE_timestamp', 'ms_axis', 'pupil_diameter'.\n",
    "      - speed_threshold (float): Angular speed threshold (degrees/frame) for detection.\n",
    "      - directional_delta_threshold_deg (float): Angular change threshold to determine segmentation boundaries.\n",
    "      - magnitude_calib (float): Calibration factor (not applied to angular measures).\n",
    "      - speed_profile (bool): Whether to record speed profiles.\n",
    "      - min_subsaccade_samples (int): Minimum number of samples required for a valid segment.\n",
    "      - min_net_disp (float): Minimal net angular displacement (in degrees) required for a segment to be kept.\n",
    "\n",
    "    Returns:\n",
    "      - df (pd.DataFrame): The input DataFrame with added computed columns.\n",
    "      - saccade_events_df (pd.DataFrame): DataFrame listing detected and segmented saccade events, with metrics.\n",
    "    \"\"\"\n",
    "    # Make a copy so as not to modify the original DataFrame.\n",
    "    df = eye_data_df.copy()\n",
    "\n",
    "    ### 1. Compute Frame-to-Frame Differences\n",
    "    df[\"speed_x\"] = df[\"center_x\"].diff()\n",
    "    df[\"speed_y\"] = df[\"center_y\"].diff()\n",
    "    df[\"speed_r\"] = np.sqrt(df[\"speed_x\"] ** 2 + df[\"speed_y\"] ** 2)\n",
    "\n",
    "    # Angular differences (k_phi and k_theta are in degrees)\n",
    "    df[\"angular_speed_phi\"] = df[\"k_phi\"].diff()\n",
    "    df[\"angular_speed_theta\"] = df[\"k_theta\"].diff()\n",
    "    df[\"angular_speed_r\"] = np.sqrt(df[\"angular_speed_phi\"] ** 2 + df[\"angular_speed_theta\"] ** 2)\n",
    "\n",
    "    ### 2. Saccade Detection Based on Angular Speed\n",
    "    # Mark frames where the instantaneous angular speed exceeds the threshold.\n",
    "    df[\"is_saccade_angle\"] = df[\"angular_speed_r\"] > speed_threshold\n",
    "\n",
    "    # Identify transitions to detect onsets and offsets.\n",
    "    saccade_on_off = df[\"is_saccade_angle\"].astype(int) - df[\"is_saccade_angle\"].shift(1, fill_value=0).astype(int)\n",
    "    # Use the first frame above threshold as onset\n",
    "    saccade_on_inds = np.where(saccade_on_off == 1)[0]\n",
    "    saccade_off_inds = np.where(saccade_on_off == -1)[0]\n",
    "\n",
    "    # Handle mismatches: if a saccade starts but does not end, drop the last onset.\n",
    "    if len(saccade_on_inds) > len(saccade_off_inds):\n",
    "        saccade_on_inds = saccade_on_inds[:-1]\n",
    "\n",
    "    saccade_events = []\n",
    "\n",
    "    ### 3. Process Each Detected Saccade for Segmentation\n",
    "    for start_ind, end_ind in zip(saccade_on_inds, saccade_off_inds):\n",
    "        saccade_df = df.iloc[start_ind:end_ind + 1].copy()\n",
    "        if saccade_df.empty or len(saccade_df) < min_subsaccade_samples:\n",
    "            continue\n",
    "\n",
    "        # Compute instantaneous angles (for both pixel- and angular-based estimates)\n",
    "        saccade_df[\"inst_angle_pixel\"] = np.degrees(np.arctan2(saccade_df[\"speed_y\"], saccade_df[\"speed_x\"]))\n",
    "        saccade_df[\"inst_angle_deg\"] = np.degrees(\n",
    "            np.arctan2(saccade_df[\"angular_speed_theta\"], saccade_df[\"angular_speed_phi\"]))\n",
    "\n",
    "        # Define helper function for minimal angular difference (handling circularity)\n",
    "        minimal_angle_diff_deg = lambda a, b: ((a - b + 180) % 360) - 180\n",
    "\n",
    "        angles = saccade_df[\"inst_angle_deg\"].values\n",
    "        # Compute consecutive differences\n",
    "        angle_diffs = np.array([minimal_angle_diff_deg(angles[i + 1], angles[i]) for i in range(len(angles) - 1)])\n",
    "\n",
    "        # Identify candidate segmentation boundaries when the absolute change exceeds threshold.\n",
    "        candidate_boundaries = np.where(np.abs(angle_diffs) > directional_delta_threshold_deg)[0].tolist()\n",
    "\n",
    "        # Always include the first and last frame of the saccade.\n",
    "        boundaries = [0] + candidate_boundaries + [len(saccade_df) - 1]\n",
    "\n",
    "        # Process each segment defined by these boundaries.\n",
    "        for i in range(len(boundaries) - 1):\n",
    "            seg_start = boundaries[i]\n",
    "            seg_end = boundaries[i + 1]\n",
    "            subsaccade = saccade_df.iloc[seg_start: seg_end + 1]\n",
    "            if len(subsaccade) < min_subsaccade_samples:\n",
    "                continue\n",
    "\n",
    "            # Compute net angular displacement using the angular positions (k_phi and k_theta)\n",
    "            initial_pos_angle = subsaccade.iloc[0][[\"k_phi\", \"k_theta\"]]\n",
    "            final_pos_angle = subsaccade.iloc[-1][[\"k_phi\", \"k_theta\"]]\n",
    "            net_disp = np.sqrt((final_pos_angle[\"k_phi\"] - initial_pos_angle[\"k_phi\"]) ** 2 +\n",
    "                               (final_pos_angle[\"k_theta\"] - initial_pos_angle[\"k_theta\"]) ** 2)\n",
    "\n",
    "            # Only record segments whose net displacement is above min_net_disp.\n",
    "            if net_disp < min_net_disp:\n",
    "                continue\n",
    "\n",
    "            # Timing and indices\n",
    "            sub_start_timestamp = subsaccade[\"OE_timestamp\"].iloc[0]\n",
    "            sub_end_timestamp = subsaccade[\"OE_timestamp\"].iloc[-1]\n",
    "            sub_start_ms = subsaccade[\"ms_axis\"].iloc[0]\n",
    "            sub_end_ms = subsaccade[\"ms_axis\"].iloc[-1]\n",
    "            sub_length = subsaccade.index[-1] - subsaccade.index[0]\n",
    "\n",
    "            # Pixel-based metrics\n",
    "            magnitude_raw_pixel = subsaccade[\"speed_r\"].sum()\n",
    "            magnitude_pixel = magnitude_raw_pixel * magnitude_calib\n",
    "\n",
    "            # Angular-based metric: sum of instantaneous angular speeds\n",
    "            magnitude_raw_angular = subsaccade[\"angular_speed_r\"].sum()\n",
    "\n",
    "            # Overall angular-based angle (from start to end)\n",
    "            overall_angle_deg = (np.degrees(np.arctan2(\n",
    "                final_pos_angle[\"k_theta\"] - initial_pos_angle[\"k_theta\"],\n",
    "                final_pos_angle[\"k_phi\"] - initial_pos_angle[\"k_phi\"]\n",
    "            )) % 360)\n",
    "\n",
    "            # (Optional) Capture speed profiles and other details\n",
    "            speed_profile_pixel = subsaccade[\"speed_r\"].values if speed_profile else None\n",
    "            speed_profile_pixel_calib = (speed_profile_pixel * magnitude_calib) if speed_profile else None\n",
    "            speed_profile_angular = subsaccade[\"angular_speed_r\"].values if speed_profile else None\n",
    "            diameter_profile = subsaccade[\"pupil_diameter\"].values\n",
    "\n",
    "            saccade_events.append({\n",
    "                \"saccade_start_ind\": subsaccade.index[0],\n",
    "                \"saccade_end_ind\": subsaccade.index[-1],\n",
    "                \"saccade_start_timestamp\": sub_start_timestamp,\n",
    "                \"saccade_end_timestamp\": sub_end_timestamp,\n",
    "                \"saccade_on_ms\": sub_start_ms,\n",
    "                \"saccade_off_ms\": sub_end_ms,\n",
    "                \"length\": sub_length,\n",
    "                \"magnitude_raw_pixel\": magnitude_raw_pixel,\n",
    "                \"magnitude_pixel\": magnitude_pixel,\n",
    "                \"magnitude_raw_angular\": magnitude_raw_angular,\n",
    "                \"overall_angle_deg\": overall_angle_deg,\n",
    "                \"net_angular_disp\": net_disp,\n",
    "                \"speed_profile_pixel\": speed_profile_pixel,\n",
    "                \"speed_profile_pixel_calib\": speed_profile_pixel_calib,\n",
    "                \"speed_profile_angular\": speed_profile_angular,\n",
    "                \"diameter_profile\": diameter_profile,\n",
    "                \"theta_init_pos\": initial_pos_angle[\"k_theta\"],\n",
    "                \"theta_end_pos\": final_pos_angle[\"k_theta\"],\n",
    "                \"phi_init_pos\": initial_pos_angle[\"k_phi\"],\n",
    "                \"phi_end_pos\": final_pos_angle[\"k_phi\"]\n",
    "            })\n",
    "\n",
    "    # Convert the list to a DataFrame.\n",
    "    saccade_events_df = pd.DataFrame(saccade_events)\n",
    "\n",
    "    # Optionally remove intermediate column\n",
    "    df.drop([\"is_saccade_angle\"], axis=1, inplace=True)\n",
    "\n",
    "    # Calculate delta columns for convenience.\n",
    "    if not saccade_events_df.empty:\n",
    "        saccade_events_df['delta_theta'] = saccade_events_df['theta_end_pos'] - saccade_events_df['theta_init_pos']\n",
    "        saccade_events_df['delta_phi'] = saccade_events_df['phi_end_pos'] - saccade_events_df['phi_init_pos']\n",
    "\n",
    "    return df, saccade_events_df\n"
   ],
   "id": "50d13d86ac1e35fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "for block in block_collection:\n",
    "    print(block)\n",
    "    block.left_eye_data, block.l_saccade_df = create_saccade_events_with_direction_segmentation_robust(\n",
    "        block.left_eye_data,\n",
    "        0.8,  # angular speed threshold in degrees/frame\n",
    "        directional_delta_threshold_deg=90,  # threshold for change in instantaneous angle (degrees)\n",
    "        magnitude_calib=1,\n",
    "        speed_profile=True,\n",
    "        min_subsaccade_samples=2)\n",
    "    block.right_eye_data, block.r_saccade_df = create_saccade_events_with_direction_segmentation_robust(\n",
    "        block.right_eye_data,\n",
    "        0.8,  # angular speed threshold in degrees/frame\n",
    "        directional_delta_threshold_deg=90,  # threshold for change in instantaneous angle (degrees)\n",
    "        magnitude_calib=1,\n",
    "        speed_profile=True,\n",
    "        min_subsaccade_samples=2)\n"
   ],
   "id": "bac9943b9ce958ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sanity_check_speed_r_distributions(block_collection, bins=100, xlim=None):\n",
    "    \"\"\"\n",
    "    Quick sanity check: plot histograms of speed_r for each block and each eye.\n",
    "\n",
    "    Args:\n",
    "        block_collection (list): List of BlockSync objects.\n",
    "        bins (int): Number of bins for histograms (if xlim given, span is aligned to it).\n",
    "        xlim (tuple): Optional (min,max) x-axis limits in deg/frame.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(len(block_collection), 2, figsize=(8, 3*len(block_collection)), dpi=120)\n",
    "    if len(block_collection) == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for i, block in enumerate(block_collection):\n",
    "        # --- Left eye ---\n",
    "        axL = axs[i][0]\n",
    "        if hasattr(block, \"left_eye_data\") and \"speed_r\" in block.left_eye_data.columns:\n",
    "            data = block.left_eye_data[\"speed_r\"].dropna().to_numpy()\n",
    "            if xlim:\n",
    "                bins_edges = np.linspace(xlim[0], xlim[1], bins+1)\n",
    "            else:\n",
    "                bins_edges = bins\n",
    "            axL.hist(data, bins=bins_edges, color=\"blue\", alpha=0.7, log=True)\n",
    "            axL.set_title(f\"Block {block.block_num} – Left Eye\")\n",
    "        else:\n",
    "            axL.set_title(f\"Block {block.block_num} – Left Eye (no data)\")\n",
    "        if xlim: axL.set_xlim(xlim)\n",
    "        axL.set_xlabel(\"speed_r [deg/frame]\")\n",
    "        axL.set_ylabel(\"Count (log)\")\n",
    "\n",
    "        # --- Right eye ---\n",
    "        axR = axs[i][1]\n",
    "        if hasattr(block, \"right_eye_data\") and \"speed_r\" in block.right_eye_data.columns:\n",
    "            data = block.right_eye_data[\"speed_r\"].dropna().to_numpy()\n",
    "            if xlim:\n",
    "                bins_edges = np.linspace(xlim[0], xlim[1], bins+1)\n",
    "            else:\n",
    "                bins_edges = bins\n",
    "            axR.hist(data, bins=bins_edges, color=\"red\", alpha=0.7, log=True)\n",
    "            axR.set_title(f\"Block {block.block_num} – Right Eye\")\n",
    "        else:\n",
    "            axR.set_title(f\"Block {block.block_num} – Right Eye (no data)\")\n",
    "        if xlim: axR.set_xlim(xlim)\n",
    "        axR.set_xlabel(\"speed_r [deg/frame]\")\n",
    "        axR.set_ylabel(\"Count (log)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sanity_check_speed_r_distributions(block_collection, bins=80, xlim=(0, 10))"
   ],
   "id": "12ec2fe4c963c8fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_OKABE_ITO = [\n",
    "    \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\",\n",
    "    \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#999999\"\n",
    "]\n",
    "\n",
    "def plot_animal_speed_r_distributions(\n",
    "    block_collection,\n",
    "    bins=100,\n",
    "    xlim=None,                       # (xmin, xmax); also sets the bin span if provided\n",
    "    normalize='pmf',                 # 'pmf' (sum=1 over bins) or 'pdf' (area=1)\n",
    "    combine_eyes=True,               # pool L+R per animal\n",
    "    eye=None,                        # if combine_eyes=False, choose 'L' or 'R'\n",
    "    color_map=None,                  # optional dict {'PV_62': '#...', ...}\n",
    "    linewidth=2.0,\n",
    "    title=\"speed_r distribution per animal (deg/frame)\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Aggregate speed_r across blocks (and eyes) per animal and plot per-animal probability curves.\n",
    "\n",
    "    Args:\n",
    "        block_collection: iterable of BlockSync objects; must have .animal_call, .left_eye_data/.right_eye_data with 'speed_r'.\n",
    "        bins: number of bins (if xlim is given, span is aligned to it).\n",
    "        xlim: optional (xmin, xmax) to set axis limits AND bin edges.\n",
    "        normalize: 'pmf' (probabilities sum to 1) or 'pdf' (probability density).\n",
    "        combine_eyes: if True, pool left and right eyes per animal.\n",
    "        eye: if combine_eyes is False, choose which eye to plot: 'L' or 'R'.\n",
    "        color_map: optional fixed colors per animal; else uses Okabe–Ito palette.\n",
    "        linewidth: line width for traces.\n",
    "        title: plot title.\n",
    "    \"\"\"\n",
    "    # --- collect data per animal ---\n",
    "    by_animal = {}\n",
    "    for block in block_collection:\n",
    "        animal = getattr(block, 'animal_call', None)\n",
    "        if animal is None:\n",
    "            continue\n",
    "        # fetch data arrays safely\n",
    "        L = None\n",
    "        R = None\n",
    "        if hasattr(block, \"left_eye_data\") and \"speed_r\" in block.left_eye_data.columns:\n",
    "            L = block.left_eye_data[\"speed_r\"].dropna().to_numpy()\n",
    "        if hasattr(block, \"right_eye_data\") and \"speed_r\" in block.right_eye_data.columns:\n",
    "            R = block.right_eye_data[\"speed_r\"].dropna().to_numpy()\n",
    "\n",
    "        if combine_eyes:\n",
    "            arrs = [a for a in (L, R) if a is not None and a.size]\n",
    "            if not arrs:\n",
    "                continue\n",
    "            data = np.concatenate(arrs)\n",
    "        else:\n",
    "            if eye not in ('L', 'R'):\n",
    "                raise ValueError(\"When combine_eyes=False you must set eye to 'L' or 'R'.\")\n",
    "            src = L if eye == 'L' else R\n",
    "            if src is None or src.size == 0:\n",
    "                continue\n",
    "            data = src\n",
    "\n",
    "        if data.size:\n",
    "            by_animal.setdefault(animal, []).append(data)\n",
    "\n",
    "    # concatenate across blocks per animal\n",
    "    for a in list(by_animal.keys()):\n",
    "        by_animal[a] = np.concatenate(by_animal[a]) if len(by_animal[a]) else np.array([])\n",
    "\n",
    "    # guard: nothing to plot\n",
    "    if not by_animal or all(v.size == 0 for v in by_animal.values()):\n",
    "        print(\"No speed_r data found to plot.\")\n",
    "        return\n",
    "\n",
    "    # --- decide on bin edges ---\n",
    "    if xlim is not None:\n",
    "        xmin, xmax = xlim\n",
    "        bin_edges = np.linspace(xmin, xmax, bins + 1)\n",
    "    else:\n",
    "        # robust global span from all animals (clip extremes a bit)\n",
    "        all_vals = np.concatenate([v for v in by_animal.values() if v.size])\n",
    "        low, high = np.nanpercentile(all_vals, [0.5, 99.5])\n",
    "        if not np.isfinite(low) or not np.isfinite(high) or low >= high:\n",
    "            low, high = float(np.nanmin(all_vals)), float(np.nanmax(all_vals))\n",
    "        bin_edges = np.linspace(low, high, bins + 1)\n",
    "\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "\n",
    "    # --- colors ---\n",
    "    animals = sorted(by_animal.keys())\n",
    "    if color_map is None:\n",
    "        palette = (_OKABE_ITO * ((len(animals) // len(_OKABE_ITO)) + 1))[:len(animals)]\n",
    "        colors = {a: c for a, c in zip(animals, palette)}\n",
    "    else:\n",
    "        colors = {a: color_map.get(a, _OKABE_ITO[i % len(_OKABE_ITO)]) for i, a in enumerate(animals)}\n",
    "\n",
    "    # --- plot ---\n",
    "    fig, ax = plt.subplots(figsize=(6.5, 4.0), dpi=140)\n",
    "\n",
    "    for animal in animals:\n",
    "        vals = by_animal[animal]\n",
    "        if vals.size == 0:\n",
    "            continue\n",
    "        counts, _ = np.histogram(vals, bins=bin_edges, density=(normalize == 'pdf'))\n",
    "        if normalize == 'pmf':\n",
    "            total = counts.sum()\n",
    "            y = counts / total if total > 0 else counts\n",
    "        else:\n",
    "            y = counts  # pdf\n",
    "\n",
    "        # step plot looks nice for hist-based curves; or use regular line\n",
    "        ax.plot(bin_centers, y, label=animal, color=colors[animal], lw=linewidth)\n",
    "\n",
    "    ax.set_xlabel(\"speed_r [deg/frame]\")\n",
    "    ax.set_ylabel(\"Probability\" if normalize == 'pmf' else \"Probability density\")\n",
    "    ax.set_title(title)\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "\n",
    "    # Legend placed to the right, outside the axes\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), borderaxespad=0.0, title=\"Animal\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig, ax\n",
    "plot_animal_speed_r_distributions(block_collection, bins=120, xlim=(0, 1), normalize='pdf', combine_eyes=False, eye=\"R\")\n"
   ],
   "id": "b4f4f7e05fcca9e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def filter_saccade_events(saccade_events_df, min_length=1, min_disp_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Filters saccade events based on the number of frames (length) and overall angular displacement.\n",
    "\n",
    "    Parameters:\n",
    "      - saccade_events_df: DataFrame produced by create_saccade_events_with_direction_segmentation_updated.\n",
    "        It should include at least the following columns:\n",
    "          'length', 'theta_init_pos', 'theta_end_pos', 'phi_init_pos', 'phi_end_pos',\n",
    "          or alternatively 'delta_theta' and 'delta_phi'.\n",
    "      - min_length: Minimum number of frames for a saccade event (default is 1).\n",
    "      - min_disp_threshold: Minimum overall angular displacement (in degrees) required for an event.\n",
    "\n",
    "    Returns:\n",
    "      - filtered_df: A DataFrame containing only saccade events with length >= min_length\n",
    "                     and overall displacement >= min_disp_threshold.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original dataframe.\n",
    "    filtered_df = saccade_events_df.copy()\n",
    "\n",
    "    # Ensure that we have the delta columns.\n",
    "    if 'delta_theta' not in filtered_df.columns or 'delta_phi' not in filtered_df.columns:\n",
    "        # Compute delta_theta and delta_phi from the initial and final positions.\n",
    "        filtered_df['delta_theta'] = filtered_df['theta_end_pos'] - filtered_df['theta_init_pos']\n",
    "        filtered_df['delta_phi'] = filtered_df['phi_end_pos'] - filtered_df['phi_init_pos']\n",
    "\n",
    "    # Compute the overall angular displacement.\n",
    "    filtered_df['overall_disp'] = np.sqrt(filtered_df['delta_theta']**2 + filtered_df['delta_phi']**2)\n",
    "\n",
    "    # Filter out events with length below min_length and overall displacement below threshold.\n",
    "    filtered_df = filtered_df[(filtered_df['length'] >= min_length) &\n",
    "                              (filtered_df['overall_disp'] >= min_disp_threshold)]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "for block in block_collection:\n",
    "    print(f'{block} before = {len(block.l_saccade_df)+len(block.r_saccade_df)}')\n",
    "    block.l_saccade_df = filter_saccade_events(block.l_saccade_df,min_length=1,min_disp_threshold=0.23)\n",
    "    block.r_saccade_df = filter_saccade_events(block.r_saccade_df,min_length=1,min_disp_threshold=0.23)\n",
    "    print(f'{block} after = {len(block.l_saccade_df)+len(block.r_saccade_df)}')"
   ],
   "id": "3dedec74971fc100",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "PV_62, block 023, on 2023-04-27_10-52-42 before = 1322\n",
    "PV_62, block 023, on 2023-04-27_10-52-42 after = 1044\n",
    "PV_62, block 024, on 2023-04-27_11-22-56 before = 1322\n",
    "PV_62, block 024, on 2023-04-27_11-22-56 after = 1044\n",
    "PV_62, block 026, on 2023-04-27_12-21-41 before = 1338\n",
    "PV_62, block 026, on 2023-04-27_12-21-41 after = 1066\n",
    "PV_126, block 007, on PV126_Trial16_wake3_2024-07-18_12-49-12 before = 2672\n",
    "PV_126, block 007, on PV126_Trial16_wake3_2024-07-18_12-49-12 after = 2353\n",
    "PV_126, block 008, on PV126_Trial16_wake4_2024-07-18_13-24-41 before = 524\n",
    "PV_126, block 008, on PV126_Trial16_wake4_2024-07-18_13-24-41 after = 451\n",
    "PV_126, block 009, on PV126_Trial18_wake5_2024-07-18_14-39-15 before = 802\n",
    "PV_126, block 009, on PV126_Trial18_wake5_2024-07-18_14-39-15 after = 687\n",
    "PV_126, block 010, on PV126_Trial19_wake6_2024-07-18_15-24-57 before = 564\n",
    "PV_126, block 010, on PV126_Trial19_wake6_2024-07-18_15-24-57 after = 492\n",
    "PV_126, block 011, on PV126_Trial115_eyeTracking_w7 before = 3197\n",
    "PV_126, block 011, on PV126_Trial115_eyeTracking_w7 after = 2827\n",
    "PV_126, block 012, on PV126_Trial116_eyeTracking_h8 before = 208\n",
    "PV_126, block 012, on PV126_Trial116_eyeTracking_h8 after = 174\n"
   ],
   "id": "6ee1ecacb848effb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### over here, I get the lizard movements binary from mark's analysis\n",
    "import os\n",
    "def block_get_lizard_movement(block):\n",
    "    # collect accelerometer data\n",
    "    # path definition\n",
    "    p = block.oe_path / 'analysis'\n",
    "    analysis_list = os.listdir(p)\n",
    "    correct_analysis = [i for i in analysis_list if block.animal_call in i][0]\n",
    "    p = p / str(correct_analysis)\n",
    "    mat_path = p / 'lizMov.mat'\n",
    "    print(f'path to mat file is {mat_path}')\n",
    "    # read mat file\n",
    "    try:\n",
    "        mat_data = h5py.File(str(mat_path), 'r')\n",
    "        mat_dict = {'t_mov_ms': mat_data['t_mov_ms'][:],\n",
    "                    'movAll': mat_data['movAll'][:]}\n",
    "\n",
    "        acc_df = pd.DataFrame(data=np.array([mat_dict['t_mov_ms'][:, 0], mat_dict['movAll'][:, 0]]).T,\n",
    "                              columns=['t_mov_ms', 'movAll'])\n",
    "        mat_data.close()\n",
    "        block.liz_mov_df = acc_df\n",
    "        print(f'liz_mov_df created for {block}')\n",
    "    except FileNotFoundError:\n",
    "        print('mat file does not exist - run the matlab getLizMovement function')\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# this cell now labels the saccades as with/without head movements\n",
    "def get_head_mov_col(df, mov_times):\n",
    "    head_mov_bool = np.zeros(len(df), dtype=bool)  # Initialize array of False\n",
    "\n",
    "    for i, saccade in enumerate(df.itertuples()):\n",
    "        saccade_start = saccade.saccade_on_ms\n",
    "        saccade_end = saccade.saccade_off_ms\n",
    "\n",
    "        overlapping_mov_times = mov_times[np.logical_and(mov_times >= saccade_start, mov_times <= saccade_end)]\n",
    "\n",
    "        if overlapping_mov_times.size > 0:\n",
    "            head_mov_bool[i] = True\n",
    "\n",
    "    df['head_movement'] = head_mov_bool\n",
    "    return df\n",
    "\n",
    "def label_saccade_movements(block):\n",
    "    mov_times = block.liz_mov_df.t_mov_ms.values\n",
    "    block.l_saccade_df = get_head_mov_col(block.l_saccade_df,mov_times=mov_times)\n",
    "    block.r_saccade_df = get_head_mov_col(block.r_saccade_df,mov_times=mov_times)\n",
    "\n",
    "# Create a list to store blocks where movement data exists\n",
    "block_collection_w_mov = []\n",
    "\n",
    "for block in block_collection:\n",
    "    try:\n",
    "        block_get_lizard_movement(block)  # Try loading movement data\n",
    "        label_saccade_movements(block)    # Try labeling saccades\n",
    "\n",
    "        # If both steps succeed, add block to the valid collection\n",
    "        block_collection_w_mov.append(block)\n",
    "\n",
    "    except (FileNotFoundError, OSError) as e:\n",
    "        print(f\"Skipping block {block}: {str(e)}\")  # Notify which block failed\n"
   ],
   "id": "d4437fae26d3cd22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "# add annotations for a joint dataframe:\n",
    "for block in block_collection:\n",
    "    block.r_saccade_df['eye'] = 'R'\n",
    "    block.r_saccade_df['block'] = block.block_num\n",
    "    block.r_saccade_df['animal'] = block.animal_call\n",
    "    block.l_saccade_df['eye'] = 'L'\n",
    "    block.l_saccade_df['block'] = block.block_num\n",
    "    block.l_saccade_df['animal'] = block.animal_call\n",
    "    block.all_saccade_df = pd.concat([block.l_saccade_df,block.r_saccade_df])"
   ],
   "id": "5eba41faee81ee2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "saccade_df_list = []\n",
    "for block in block_collection:\n",
    "    saccade_df_list.append(block.all_saccade_df)\n",
    "saccade_collection = pd.concat(saccade_df_list)"
   ],
   "id": "3367910146bc1118",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "def get_closest_diff_ind(timestamp, timeseries):\n",
    "    \"\"\"\n",
    "    This function extracts a frame from a series so that it is as close as possible to a given timestamp\n",
    "    :param timestamp: The time to match a frame to\n",
    "    :param timeseries: The time frames series to look at for a match\n",
    "    :param report_acc: if set to 1, will report the accuracy of the match\n",
    "    :return: index_of_lowest_diff , accuracy of match (if requested)\n",
    "    \"\"\"\n",
    "    array = np.abs(timeseries - timestamp)\n",
    "    index_of_lowest_diff = np.argmin(array)\n",
    "    lowest_diff_val = timeseries[index_of_lowest_diff]\n",
    "    return index_of_lowest_diff, lowest_diff_val\n",
    "\n",
    "def find_synced_saccades(df, diff_threshold=680):\n",
    "    synced_saccades = []\n",
    "    non_synced_saccades = []\n",
    "    l_df = df.query('eye == \"L\"')\n",
    "    r_df = df.query('eye == \"R\"')\n",
    "    for i, row in tqdm.tqdm(l_df.iterrows()):\n",
    "        l_timestamp = row['saccade_start_timestamp']\n",
    "        ind_min_diff, r_timestamp = get_closest_diff_ind(l_timestamp, r_df['saccade_start_timestamp'].values)\n",
    "        #print(i,ind_lowest_diff)\n",
    "        time_diff = np.abs(l_timestamp - r_timestamp)\n",
    "        if time_diff < diff_threshold:\n",
    "            synced_saccades.append((row, r_df.iloc[ind_min_diff]))  # Collect synchronized rows\n",
    "        else:\n",
    "            non_synced_saccades.append(row)  # Collect non-synchronized rows\n",
    "    # Create DataFrame with multi-index\n",
    "    multi_index = pd.MultiIndex.from_tuples([(i, 'L') for i in range(len(synced_saccades))] + [(i, 'R') for i in range(len(synced_saccades))], names=['Main', 'Sub'])\n",
    "    synced_df = pd.DataFrame(index=multi_index, columns=df.columns)\n",
    "    # Populate DataFrame\n",
    "    for idx, (l_row, r_row) in enumerate(synced_saccades):\n",
    "        synced_df.loc[(idx, 'L')] = l_row\n",
    "        synced_df.loc[(idx, 'R')] = r_row\n",
    "    r_non_synced_leftovers = r_df[~r_df['saccade_start_timestamp'].isin(synced_df.query('eye == \"R\"')['saccade_start_timestamp'].values)]\n",
    "    print(len(r_non_synced_leftovers),len(r_df))\n",
    "    # Create DataFrame for non-synced saccades\n",
    "    non_synced_df = pd.DataFrame(non_synced_saccades, columns=df.columns)\n",
    "    non_synced_df = pd.concat([non_synced_df,r_non_synced_leftovers])\n",
    "\n",
    "    return synced_df, non_synced_df\n",
    "\n",
    "synced_df_list = []\n",
    "non_synced_df_list = []\n",
    "for saccade_df in saccade_df_list:\n",
    "    # Find synced saccades:\n",
    "    synced_df, non_synced_df = find_synced_saccades(saccade_df.dropna(), diff_threshold=680)\n",
    "    if len(non_synced_df.dropna()) + len(synced_df.dropna()) == len(saccade_df.dropna()):\n",
    "        print('got them all')\n",
    "\n",
    "    synced_df_list.append(synced_df)\n",
    "    non_synced_df_list.append(non_synced_df)\n"
   ],
   "id": "1ef01c535f4f85d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "def combine_synced_dataframes(dataframes):\n",
    "    combined_dfs = []\n",
    "    start_index = 0\n",
    "    for df in dataframes:\n",
    "        num_rows = len(df) // 2  # Assuming each dataframe contains pairs of rows\n",
    "        main_index = pd.MultiIndex.from_tuples([(i + start_index, 'L') for i in range(num_rows)] + [(i + start_index, 'R') for i in range(num_rows)], names=['Main', 'Sub'])\n",
    "        df.index = main_index\n",
    "        combined_dfs.append(df)\n",
    "        start_index += num_rows\n",
    "    combined_df = pd.concat(combined_dfs)\n",
    "    combined_df.reset_index(inplace=True)\n",
    "    return combined_df\n",
    "\n",
    "synced_saccade_collection = combine_synced_dataframes(synced_df_list)\n",
    "non_synced_saccade_collection = pd.concat(non_synced_df_list)\n"
   ],
   "id": "658e316e9bef5349",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "all_saccade_collection = pd.concat([synced_saccade_collection,non_synced_saccade_collection])\n",
   "id": "afe21b88a353fb78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "562791a740ce6eea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "def export_inter_saccade_intervals_density_traces_from_blocks(\n",
    "    block_collection,\n",
    "    figure_size=(1.8, 1.2),\n",
    "    export_path=None,\n",
    "    plot_name='ISI_histogram.pdf',\n",
    "    num_bins=20,\n",
    "    high_ms=20000.0,\n",
    "    pair_threshold_ms=60.0,\n",
    "    min_isi_ms=None,          # if None -> 0.75 * frame_ms\n",
    "    first_bin_factor=0.5,     # first real edge = 0.5×Δt\n",
    "    add_leading_zero_bin=True,\n",
    "    font_family='Arial',\n",
    "    color_template=\"okabeito\",\n",
    "    custom_colors=None,\n",
    "    color_map=None,           # {'PV_62':'#...', ...}\n",
    "    xlim=None,\n",
    "    avg_label=\"All (combined)\",\n",
    "    avg_color=\"k\",\n",
    "    avg_linewidth=2.0,\n",
    "    avg_linestyle=\"-\",\n",
    "    export_pickle_path=None,  # if None and export_path is set, will save next to the PDF\n",
    "    pickle_name=\"ISI_histogram_plotdata.pkl\"\n",
    "):\n",
    "    # Collect & dedupe\n",
    "    events = _collect_events_from_blocks(block_collection)\n",
    "    if events.empty:\n",
    "        raise ValueError(\"No saccade events found.\")\n",
    "    dedup = _dedupe_lr_pairs(events, pair_threshold_ms=pair_threshold_ms)\n",
    "\n",
    "    # Frame interval & ISIs\n",
    "    frame_ms = _estimate_frame_interval_ms(block_collection)\n",
    "    min_isi_eff = (0.75*frame_ms) if (min_isi_ms is None) else float(min_isi_ms)\n",
    "    isi_all = _compute_blockwise_isis(dedup, min_isi_ms=min_isi_eff)\n",
    "    animals = sorted(dedup['animal'].dropna().unique())\n",
    "\n",
    "    # Colors\n",
    "    if color_map is None:\n",
    "        cmap = build_color_map(animals, template=color_template, custom=custom_colors, order=animals)\n",
    "    else:\n",
    "        fallback = build_color_map(animals, template=color_template, custom=custom_colors, order=animals)\n",
    "        cmap = {a: color_map.get(a, fallback[a]) for a in animals}\n",
    "\n",
    "    # Global binning\n",
    "    low_ms = max(1e-6, first_bin_factor*frame_ms)\n",
    "    bins = _snapped_log_bins(num_bins, frame_ms, low_ms, float(high_ms), add_leading_zero_bin)\n",
    "    bin_centers = np.sqrt(bins[:-1]*bins[1:])\n",
    "\n",
    "    # Style & axes\n",
    "    plt.rcParams['font.family'] = 'sans-serif'; plt.rcParams['font.sans-serif'] = [font_family]\n",
    "    fig, ax = plt.subplots(figsize=figure_size, dpi=300)\n",
    "    legend_handles, legend_labels = [], []\n",
    "\n",
    "    # Helper: fold probability outside xlim into edge bins\n",
    "    def _fold_counts_to_xlim(counts, edges, xlim_tuple):\n",
    "        \"\"\"\n",
    "        counts: per-bin probabilities (sum == 1)\n",
    "        edges:  bin edges (len = len(counts) + 1)\n",
    "        xlim_tuple: (low, high)\n",
    "        Returns: disp_counts (renorm), disp_edges, disp_centers, idx_lo, idx_hi\n",
    "        \"\"\"\n",
    "        low, high = float(xlim_tuple[0]), float(xlim_tuple[1])\n",
    "        if not (np.isfinite(low) and np.isfinite(high) and low < high):\n",
    "            centers = np.sqrt(edges[:-1]*edges[1:])\n",
    "            return counts, edges, centers, 0, len(counts)-1\n",
    "\n",
    "        idx_lo = np.searchsorted(edges, low, side='right') - 1\n",
    "        idx_lo = max(0, min(idx_lo, len(counts)-1))\n",
    "        idx_hi = np.searchsorted(edges, high, side='left') - 1\n",
    "        idx_hi = max(0, min(idx_hi, len(counts)-1))\n",
    "\n",
    "        counts_fold = counts.copy()\n",
    "\n",
    "        # Left tail -> first displayed bin\n",
    "        left_mask = edges[1:] <= low\n",
    "        left_mass = counts_fold[left_mask].sum()\n",
    "        counts_fold[left_mask] = 0.0\n",
    "        counts_fold[idx_lo] += left_mass\n",
    "\n",
    "        # Right tail -> last displayed bin\n",
    "        right_mask = edges[:-1] >= high\n",
    "        right_mass = counts_fold[right_mask].sum()\n",
    "        counts_fold[right_mask] = 0.0\n",
    "        counts_fold[idx_hi] += right_mass\n",
    "\n",
    "        disp_counts = counts_fold[idx_lo:idx_hi+1]\n",
    "        disp_edges  = edges[idx_lo:idx_hi+2]\n",
    "        s = disp_counts.sum()\n",
    "        if s > 0:\n",
    "            disp_counts = disp_counts / s\n",
    "        disp_centers = np.sqrt(disp_edges[:-1]*disp_edges[1:])\n",
    "        return disp_counts, disp_edges, disp_centers, idx_lo, idx_hi\n",
    "\n",
    "    # --- Per-animal plotting + accumulate total counts ---\n",
    "    total_counts = np.zeros(len(bins)-1, dtype=float)\n",
    "    valid_any = False\n",
    "\n",
    "    # For pickle: store plot-ready x/y and also raw hist/density per animal\n",
    "    plot_data = {\n",
    "        \"params\": {\n",
    "            \"figure_size\": figure_size, \"num_bins\": num_bins, \"high_ms\": high_ms,\n",
    "            \"pair_threshold_ms\": pair_threshold_ms, \"min_isi_ms\": min_isi_ms,\n",
    "            \"first_bin_factor\": first_bin_factor, \"add_leading_zero_bin\": add_leading_zero_bin,\n",
    "            \"xlim\": xlim, \"font_family\": font_family, \"color_template\": color_template\n",
    "        },\n",
    "        \"frame_ms\": frame_ms,\n",
    "        \"bins\": bins,\n",
    "        \"animals\": animals,\n",
    "        \"color_map\": cmap,\n",
    "        \"per_animal\": {},   # {animal: {\"hist\":..., \"density\":..., \"x\":..., \"y\":...}}\n",
    "        \"combined\": {}      # {\"total_counts\":..., \"total_density\":..., \"x\":..., \"y\":...}\n",
    "    }\n",
    "\n",
    "    for a in animals:\n",
    "        isi = isi_all.get(a, np.array([]))\n",
    "        if isi.size == 0:\n",
    "            continue\n",
    "\n",
    "        hist, _ = np.histogram(isi, bins=bins)\n",
    "        if hist.sum() == 0:\n",
    "            continue\n",
    "        valid_any = True\n",
    "        total_counts += hist\n",
    "\n",
    "        dens = hist.astype(float) / hist.sum()\n",
    "\n",
    "        if xlim is not None:\n",
    "            disp_dens, disp_edges, disp_centers, idx_lo, idx_hi = _fold_counts_to_xlim(dens, bins, xlim)\n",
    "            x_plot, y_plot = disp_centers, disp_dens\n",
    "        else:\n",
    "            if add_leading_zero_bin:\n",
    "                x_plot = np.r_[bins[0], bin_centers[1:]]\n",
    "                y_plot = np.r_[0.0, dens[1:]]\n",
    "            else:\n",
    "                x_plot, y_plot = bin_centers, dens\n",
    "\n",
    "        # Save per-animal plot data\n",
    "        plot_data[\"per_animal\"][a] = {\n",
    "            \"hist\": hist,\n",
    "            \"density\": dens,\n",
    "            \"x\": x_plot,\n",
    "            \"y\": y_plot\n",
    "        }\n",
    "\n",
    "        h, = ax.plot(x_plot, y_plot, linewidth=1.2, color=cmap[a], label=str(a))\n",
    "        legend_handles.append(h); legend_labels.append(str(a))\n",
    "\n",
    "    # --- Combined \"All (combined)\" average trace ---\n",
    "    if valid_any and total_counts.sum() > 0:\n",
    "        total_dens = total_counts / total_counts.sum()\n",
    "\n",
    "        if xlim is not None:\n",
    "            disp_dens_all, disp_edges_all, disp_centers_all, _, _ = _fold_counts_to_xlim(total_dens, bins, xlim)\n",
    "            x_all, y_all = disp_centers_all, disp_dens_all\n",
    "        else:\n",
    "            if add_leading_zero_bin:\n",
    "                x_all = np.r_[bins[0], bin_centers[1:]]\n",
    "                y_all = np.r_[0.0, total_dens[1:]]\n",
    "            else:\n",
    "                x_all, y_all = bin_centers, total_dens\n",
    "\n",
    "        # Save combined plot data\n",
    "        plot_data[\"combined\"] = {\n",
    "            \"total_counts\": total_counts,\n",
    "            \"total_density\": total_dens,\n",
    "            \"x\": x_all,\n",
    "            \"y\": y_all,\n",
    "            \"label\": avg_label\n",
    "        }\n",
    "\n",
    "        h_all, = ax.plot(x_all, y_all, avg_linestyle, color=avg_color, linewidth=avg_linewidth,\n",
    "                         label=avg_label, zorder=10)\n",
    "        legend_handles.insert(0, h_all); legend_labels.insert(0, avg_label)\n",
    "\n",
    "    # Axes cosmetics\n",
    "    ax.set_xscale('log')\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim[0], xlim[1])\n",
    "    else:\n",
    "        ax.set_xlim(bins[0], bins[-1])\n",
    "\n",
    "    ax.set_xlabel('ISI [ms]', fontsize=10); ax.set_ylabel('Probability', fontsize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8, length=5, width=1, direction='out')\n",
    "    ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "    ax.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out = {\n",
    "        'dedup_events': dedup,\n",
    "        'isi_all': isi_all,\n",
    "        'frame_ms': frame_ms,\n",
    "        'bins': bins,\n",
    "        'color_map': cmap,\n",
    "        'combined': plot_data[\"combined\"]\n",
    "    }\n",
    "\n",
    "    # Export: figure + legend + pickle\n",
    "    outdir = None\n",
    "    if export_path is not None:\n",
    "        ts = _dt.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        outdir = Path(export_path) / f\"inter_saccade_export_{ts}\"\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(outdir/plot_name, format='pdf', bbox_inches='tight', dpi=300)\n",
    "\n",
    "        if legend_handles:\n",
    "            fig_leg = plt.figure(figsize=(2.0, 0.28*max(1,len(legend_labels))+0.4), dpi=300)\n",
    "            fig_leg.legend(legend_handles, legend_labels, loc='center', frameon=False, ncol=1, prop={'size':8})\n",
    "            fig_leg.savefig(outdir/('legend_'+plot_name), format='pdf', bbox_inches='tight', dpi=300)\n",
    "            plt.close(fig_leg)\n",
    "\n",
    "    # Decide pickle path and write\n",
    "    if export_pickle_path is not None:\n",
    "        export_pickle_path = Path(export_pickle_path)\n",
    "        export_pickle_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(export_pickle_path, \"wb\") as f:\n",
    "            pickle.dump(plot_data, f)\n",
    "        print(f\"Plot data (.pickle) saved to: {export_pickle_path}\")\n",
    "    elif outdir is not None:\n",
    "        pkl_path = outdir / pickle_name\n",
    "        with open(pkl_path, \"wb\") as f:\n",
    "            pickle.dump(plot_data, f)\n",
    "        print(f\"Plot data (.pickle) saved to: {pkl_path}\")\n",
    "\n",
    "    if outdir is not None:\n",
    "        print(\"Exported plot + separate legend to:\", outdir)\n",
    "\n",
    "    plt.show()\n",
    "    return out\n"
   ],
   "id": "33fd64bd6df42297",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save next to the PDF (inside the timestamped folder)\n",
    "export_inter_saccade_intervals_density_traces_from_blocks(\n",
    "    block_collection,\n",
    "    export_path=r\"Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_4\\material\\Inter_saccade_intervals_histogram\",\n",
    "    color_map=color_map,\n",
    "    xlim=(100, 20000),\n",
    "    add_leading_zero_bin=False,\n",
    "    num_bins=25\n",
    ")\n"
   ],
   "id": "1f67737d783f75ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- drop-in replacements for the two helpers + the plotting function ---\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import datetime as _dt\n",
    "\n",
    "def _collect_events_from_blocks(block_collection):\n",
    "    rows = []\n",
    "    for b in block_collection:\n",
    "        animal = getattr(b, 'animal_call', None)\n",
    "        blockn = getattr(b, 'block_num', None)\n",
    "        for eye, sdf in (('L', getattr(b, 'l_saccade_df', None)),\n",
    "                         ('R', getattr(b, 'r_saccade_df', None))):\n",
    "            if sdf is None or len(sdf) == 0:\n",
    "                continue\n",
    "            s_on = pd.to_numeric(sdf.get('saccade_on_ms', pd.Series([], dtype='float64')), errors='coerce')\n",
    "            hm = sdf['head_movement'] if 'head_movement' in sdf.columns else pd.Series(False, index=sdf.index)\n",
    "            df_tmp = pd.DataFrame({\n",
    "                'animal': animal, 'block': blockn, 'eye': eye,\n",
    "                'saccade_on_ms': s_on, 'head_movement': hm.astype(bool)\n",
    "            }).dropna(subset=['saccade_on_ms'])\n",
    "            rows.append(df_tmp)\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=['animal','block','eye','saccade_on_ms','head_movement'])\n",
    "    return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "def _estimate_frame_interval_ms(block_collection, fallback_ms=16.67):\n",
    "    dts = []\n",
    "    for b in block_collection:\n",
    "        for df in [getattr(b, 'left_eye_data', None), getattr(b, 'right_eye_data', None)]:\n",
    "            if df is None or 'ms_axis' not in df.columns: continue\n",
    "            ms = pd.to_numeric(df['ms_axis'], errors='coerce').dropna().to_numpy()\n",
    "            if ms.size > 3:\n",
    "                dt = np.diff(ms); dt = dt[(dt > 0) & np.isfinite(dt)]\n",
    "                if dt.size: dts.append(np.median(dt))\n",
    "    return float(np.median(dts)) if dts else float(fallback_ms)\n",
    "\n",
    "def _dedupe_lr_pairs(events_df, pair_threshold_ms=60.0):\n",
    "    \"\"\"\n",
    "    PER-BLOCK pairwise matching of L and R onsets:\n",
    "      - Sort L and R streams; greedily pair when |L - R| <= threshold (emit one event at min time, HM = OR).\n",
    "      - Unmatched events pass through unchanged.\n",
    "    Returns: ['animal','block','event_time_ms','head_movement'].\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for (animal, block), g in events_df.groupby(['animal','block'], dropna=False):\n",
    "        L = g[g['eye'] == 'L'].sort_values('saccade_on_ms')\n",
    "        R = g[g['eye'] == 'R'].sort_values('saccade_on_ms')\n",
    "        i = j = 0\n",
    "        lt, rt = L['saccade_on_ms'].to_numpy(dtype=float), R['saccade_on_ms'].to_numpy(dtype=float)\n",
    "        lhm = L['head_movement'].to_numpy(dtype=bool) if len(L) else np.array([], bool)\n",
    "        rhm = R['head_movement'].to_numpy(dtype=bool) if len(R) else np.array([], bool)\n",
    "\n",
    "        ev_t, ev_hm = [], []\n",
    "        while i < len(lt) and j < len(rt):\n",
    "            dt = lt[i] - rt[j]\n",
    "            if abs(dt) <= pair_threshold_ms:\n",
    "                ev_t.append(min(lt[i], rt[j])); ev_hm.append(bool(lhm[i] or rhm[j]))\n",
    "                i += 1; j += 1\n",
    "            elif lt[i] < rt[j] - pair_threshold_ms:\n",
    "                ev_t.append(lt[i]); ev_hm.append(bool(lhm[i])); i += 1\n",
    "            elif rt[j] < lt[i] - pair_threshold_ms:\n",
    "                ev_t.append(rt[j]); ev_hm.append(bool(rhm[j])); j += 1\n",
    "            else:\n",
    "                # should not happen, but safeguard\n",
    "                ev_t.append(min(lt[i], rt[j])); ev_hm.append(bool(lhm[i] if lt[i]<=rt[j] else rhm[j]))\n",
    "                if lt[i] <= rt[j]: i += 1\n",
    "                else: j += 1\n",
    "        # leftovers\n",
    "        while i < len(lt):\n",
    "            ev_t.append(lt[i]); ev_hm.append(bool(lhm[i])); i += 1\n",
    "        while j < len(rt):\n",
    "            ev_t.append(rt[j]); ev_hm.append(bool(rhm[j])); j += 1\n",
    "\n",
    "        out.append(pd.DataFrame({'animal': animal, 'block': block,\n",
    "                                 'event_time_ms': ev_t, 'head_movement': ev_hm}))\n",
    "    return (pd.concat(out, ignore_index=True)\n",
    "            if out else pd.DataFrame(columns=['animal','block','event_time_ms','head_movement']))\n",
    "\n",
    "def _compute_blockwise_isis(dedup_df, min_isi_ms):\n",
    "    isi_all = defaultdict(list); isi_head = defaultdict(list)\n",
    "    for (animal, block), g in dedup_df.groupby(['animal','block'], dropna=False):\n",
    "        g = g.sort_values('event_time_ms')\n",
    "        t = g['event_time_ms'].to_numpy()\n",
    "        if t.size > 1:\n",
    "            d = np.diff(t); d = d[d >= min_isi_ms]\n",
    "            if d.size: isi_all[animal].append(d)\n",
    "        th = g.loc[g['head_movement'], 'event_time_ms'].to_numpy()\n",
    "        if th.size > 1:\n",
    "            dh = np.diff(th); dh = dh[dh >= min_isi_ms]\n",
    "            if dh.size: isi_head[animal].append(dh)\n",
    "    isi_all = {a: (np.concatenate(v) if v else np.array([], float)) for a, v in isi_all.items()}\n",
    "    isi_head = {a: (np.concatenate(v) if v else np.array([], float)) for a, v in isi_head.items()}\n",
    "    for a in dedup_df['animal'].dropna().unique():\n",
    "        isi_all.setdefault(a, np.array([], float)); isi_head.setdefault(a, np.array([], float))\n",
    "    return isi_all, isi_head\n",
    "\n",
    "def export_inter_saccade_intervals_density_traces_from_blocks(\n",
    "    block_collection,\n",
    "    figure_size=(1.8, 1.2),\n",
    "    export_path=None,\n",
    "    plot_name='ISI_histogram.pdf',\n",
    "    num_bins=20,\n",
    "    high_ms=20000.0,\n",
    "    pair_threshold_ms=60.0,   # <= merges only L/R duplicates (about ~3–4 frames), not same-eye sequences\n",
    "    min_isi_ms=None,          # if None -> 0.75 * frame_ms\n",
    "    first_bin_factor=0.5,     # first bin low edge = factor * frame_ms (so 1-frame ISI falls inside)\n",
    "    font_family='Arial'\n",
    "):\n",
    "    # collect & dedupe with L/R pairing\n",
    "    events = _collect_events_from_blocks(block_collection)\n",
    "    if events.empty:\n",
    "        raise ValueError(\"No saccade events found.\")\n",
    "    dedup = _dedupe_lr_pairs(events, pair_threshold_ms=pair_threshold_ms)\n",
    "\n",
    "    # frame interval and binning\n",
    "    frame_ms = _estimate_frame_interval_ms(block_collection)  # ~16–17 ms\n",
    "    min_isi_eff = (0.75 * frame_ms) if (min_isi_ms is None) else float(min_isi_ms)\n",
    "\n",
    "    low_ms = max(1e-6, first_bin_factor * frame_ms)\n",
    "    bins = np.geomspace(low_ms, float(high_ms), int(num_bins))\n",
    "    bin_centers = np.sqrt(bins[:-1] * bins[1:])\n",
    "\n",
    "    # compute ISIs\n",
    "    isi_all, _ = _compute_blockwise_isis(dedup, min_isi_ms=min_isi_eff)\n",
    "    animals = sorted([a for a in dedup['animal'].dropna().unique()])\n",
    "\n",
    "    # style + colors\n",
    "    plt.rcParams['font.family'] = 'sans-serif'; plt.rcParams['font.sans-serif'] = [font_family]\n",
    "    OI = ['#0072B2','#D55E00','#009E73','#CC79A7','#F0E442','#56B4E9','#E69F00','#000000']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figure_size, dpi=300)\n",
    "    legend_handles, legend_labels = [], []\n",
    "\n",
    "    for i, a in enumerate(animals):\n",
    "        isi = isi_all.get(a, np.array([]))\n",
    "        if isi.size == 0: continue\n",
    "        hist, _ = np.histogram(isi, bins=bins)\n",
    "        dens = hist / hist.sum() if hist.sum() > 0 else hist\n",
    "        h, = ax.plot(bin_centers, dens, linewidth=1.2, color=OI[i % len(OI)], label=str(a))\n",
    "        legend_handles.append(h); legend_labels.append(str(a))\n",
    "\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlim(bins[0], bins[-1])  # remove extra empty left margin\n",
    "    #ax.set_xlabel('ISI [ms]', fontsize=10); ax.set_ylabel('Probability', fontsize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "    ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "    plt.grid(False); plt.tight_layout()\n",
    "\n",
    "    out = {'dedup_events': dedup, 'isi_all': isi_all, 'frame_ms': frame_ms, 'bins': bins}\n",
    "\n",
    "    if export_path is not None:\n",
    "        ts = _dt.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        outdir = Path(export_path) / f\"inter_saccade_export_{ts}\"\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # main plot WITHOUT legend\n",
    "        fig.savefig(outdir / plot_name, format='pdf', bbox_inches='tight', dpi=300)\n",
    "\n",
    "        # separate legend\n",
    "        if legend_handles:\n",
    "            fig_leg = plt.figure(figsize=(2.2, 0.28*max(1,len(legend_labels)) + 0.4), dpi=300)\n",
    "            fig_leg.legend(legend_handles, legend_labels, loc='center', frameon=False, ncol=1,\n",
    "                           prop={'size': 8})\n",
    "            fig_leg.savefig(outdir / ('legend_' + plot_name), format='pdf', bbox_inches='tight', dpi=300)\n",
    "            plt.close(fig_leg)\n",
    "\n",
    "        # reproducibility: dumps\n",
    "        pd.DataFrame(dedup).to_csv(outdir / \"dedup_events.csv\", index=False)\n",
    "        isi_rows = [{'animal': a, 'isi_ms': float(v)} for a in animals for v in isi_all.get(a, [])]\n",
    "        pd.DataFrame(isi_rows).to_csv(outdir / \"isi_all_longform.csv\", index=False)\n",
    "        with open(outdir / \"binning_info.txt\", 'w') as f:\n",
    "            f.write(f\"Estimated frame interval (ms): {frame_ms:.3f}\\n\")\n",
    "            f.write(f\"First bin low edge (ms): {bins[0]:.3f}\\n\")\n",
    "            f.write(f\"Min ISI enforced (ms): {min_isi_eff:.3f}\\n\")\n",
    "            f.write(f\"Pair L/R threshold (ms): {pair_threshold_ms:.3f}\\n\")\n",
    "        print(\"Exported plot, legend, and data to:\", outdir)\n",
    "\n",
    "    plt.show()\n",
    "    return out\n"
   ],
   "id": "57fdadd92307cb93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "export_path = r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_4\\material\\Inter_saccade_intervals_histogram'\n",
    "out = export_inter_saccade_intervals_density_traces_from_blocks(\n",
    "    block_collection,\n",
    "    figure_size=(1.7, 1.3),\n",
    "    export_path=None,\n",
    "    num_bins=20,\n",
    "    plot_name='ISI_histogram.pdf'\n",
    ")"
   ],
   "id": "d8c55d0c1bb3dc7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def export_inter_saccade_intervals_density_traces_from_blocks(\n",
    "    block_collection,\n",
    "    figure_size=(1.8, 1.2),\n",
    "    export_path=None,\n",
    "    plot_name='ISI_histogram.pdf',\n",
    "    num_bins=20,\n",
    "    high_ms=20000.0,\n",
    "    pair_threshold_ms=60.0,\n",
    "    min_isi_ms=None,          # if None -> 0.75 * frame_ms\n",
    "    first_bin_factor=0.5,     # first real edge = 0.5×Δt\n",
    "    add_leading_zero_bin=True,\n",
    "    font_family='Arial',\n",
    "    color_template=\"okabeito\",\n",
    "    custom_colors=None,\n",
    "    color_map=None,           # {'PV_62':'#...', ...}\n",
    "    xlim=None,\n",
    "    avg_label=\"All (combined)\",\n",
    "    avg_color=\"k\",\n",
    "    avg_linewidth=2.0,\n",
    "    avg_linestyle=\"-\"\n",
    "):\n",
    "    # Collect & dedupe\n",
    "    events = _collect_events_from_blocks(block_collection)\n",
    "    if events.empty:\n",
    "        raise ValueError(\"No saccade events found.\")\n",
    "    dedup = _dedupe_lr_pairs(events, pair_threshold_ms=pair_threshold_ms)\n",
    "\n",
    "    # Frame interval & ISIs\n",
    "    frame_ms = _estimate_frame_interval_ms(block_collection)\n",
    "    min_isi_eff = (0.75*frame_ms) if (min_isi_ms is None) else float(min_isi_ms)\n",
    "    isi_all = _compute_blockwise_isis(dedup, min_isi_ms=min_isi_eff)\n",
    "    animals = sorted(dedup['animal'].dropna().unique())\n",
    "\n",
    "    # Colors\n",
    "    if color_map is None:\n",
    "        cmap = build_color_map(animals, template=color_template, custom=custom_colors, order=animals)\n",
    "    else:\n",
    "        fallback = build_color_map(animals, template=color_template, custom=custom_colors, order=animals)\n",
    "        cmap = {a: color_map.get(a, fallback[a]) for a in animals}\n",
    "\n",
    "    # Global binning\n",
    "    low_ms = max(1e-6, first_bin_factor*frame_ms)\n",
    "    bins = _snapped_log_bins(num_bins, frame_ms, low_ms, float(high_ms), add_leading_zero_bin)\n",
    "    bin_centers = np.sqrt(bins[:-1]*bins[1:])\n",
    "\n",
    "    # Style & axes\n",
    "    plt.rcParams['font.family'] = 'sans-serif'; plt.rcParams['font.sans-serif'] = [font_family]\n",
    "    fig, ax = plt.subplots(figsize=figure_size, dpi=300)\n",
    "    legend_handles, legend_labels = [], []\n",
    "\n",
    "    # Helper: fold probability outside xlim into edge bins\n",
    "    def _fold_counts_to_xlim(counts, edges, xlim_tuple):\n",
    "        \"\"\"\n",
    "        counts: per-bin probabilities (sum == 1)\n",
    "        edges:  bin edges (len = len(counts) + 1)\n",
    "        xlim_tuple: (low, high)\n",
    "        Returns: disp_counts (renorm), disp_edges, disp_centers, idx_lo, idx_hi\n",
    "        \"\"\"\n",
    "        low, high = float(xlim_tuple[0]), float(xlim_tuple[1])\n",
    "        if not (np.isfinite(low) and np.isfinite(high) and low < high):\n",
    "            centers = np.sqrt(edges[:-1]*edges[1:])\n",
    "            return counts, edges, centers, 0, len(counts)-1\n",
    "\n",
    "        idx_lo = np.searchsorted(edges, low, side='right') - 1\n",
    "        idx_lo = max(0, min(idx_lo, len(counts)-1))\n",
    "\n",
    "        idx_hi = np.searchsorted(edges, high, side='left') - 1\n",
    "        idx_hi = max(0, min(idx_hi, len(counts)-1))\n",
    "\n",
    "        counts_fold = counts.copy()\n",
    "\n",
    "        # Left tail -> first displayed bin\n",
    "        left_mask = edges[1:] <= low\n",
    "        left_mass = counts_fold[left_mask].sum()\n",
    "        counts_fold[left_mask] = 0.0\n",
    "        counts_fold[idx_lo] += left_mass\n",
    "\n",
    "        # Right tail -> last displayed bin\n",
    "        right_mask = edges[:-1] >= high\n",
    "        right_mass = counts_fold[right_mask].sum()\n",
    "        counts_fold[right_mask] = 0.0\n",
    "        counts_fold[idx_hi] += right_mass\n",
    "\n",
    "        disp_counts = counts_fold[idx_lo:idx_hi+1]\n",
    "        disp_edges  = edges[idx_lo:idx_hi+2]\n",
    "        s = disp_counts.sum()\n",
    "        if s > 0:\n",
    "            disp_counts = disp_counts / s\n",
    "        disp_centers = np.sqrt(disp_edges[:-1]*disp_edges[1:])\n",
    "        return disp_counts, disp_edges, disp_centers, idx_lo, idx_hi\n",
    "\n",
    "    # --- Per-animal plotting + accumulate total counts ---\n",
    "    total_counts = np.zeros(len(bins)-1, dtype=float)\n",
    "    valid_any = False\n",
    "\n",
    "    for a in animals:\n",
    "        isi = isi_all.get(a, np.array([]))\n",
    "        if isi.size == 0:\n",
    "            continue\n",
    "\n",
    "        hist, _ = np.histogram(isi, bins=bins)\n",
    "        if hist.sum() == 0:\n",
    "            continue\n",
    "        valid_any = True\n",
    "        total_counts += hist\n",
    "\n",
    "        dens = hist.astype(float) / hist.sum()\n",
    "\n",
    "        if xlim is not None:\n",
    "            disp_dens, disp_edges, disp_centers, idx_lo, idx_hi = _fold_counts_to_xlim(dens, bins, xlim)\n",
    "            x_plot, y_plot = disp_centers, disp_dens\n",
    "        else:\n",
    "            if add_leading_zero_bin:\n",
    "                x_plot = np.r_[bins[0], bin_centers[1:]]\n",
    "                y_plot = np.r_[0.0, dens[1:]]\n",
    "            else:\n",
    "                x_plot, y_plot = bin_centers, dens\n",
    "\n",
    "        h, = ax.plot(x_plot, y_plot, linewidth=1.2, color=cmap[a], label=str(a))\n",
    "        legend_handles.append(h); legend_labels.append(str(a))\n",
    "\n",
    "    # --- Combined \"All (combined)\" average trace ---\n",
    "    combined = {}\n",
    "    if valid_any and total_counts.sum() > 0:\n",
    "        total_dens = total_counts / total_counts.sum()\n",
    "\n",
    "        if xlim is not None:\n",
    "            disp_dens_all, disp_edges_all, disp_centers_all, _, _ = _fold_counts_to_xlim(total_dens, bins, xlim)\n",
    "            x_all, y_all = disp_centers_all, disp_dens_all\n",
    "        else:\n",
    "            if add_leading_zero_bin:\n",
    "                x_all = np.r_[bins[0], bin_centers[1:]]\n",
    "                y_all = np.r_[0.0, total_dens[1:]]\n",
    "            else:\n",
    "                x_all, y_all = bin_centers, total_dens\n",
    "\n",
    "        h_all, = ax.plot(x_all, y_all, avg_linestyle, color=avg_color, linewidth=avg_linewidth, label=avg_label, zorder=10)\n",
    "        legend_handles.insert(0, h_all); legend_labels.insert(0, avg_label)\n",
    "\n",
    "        combined = {\n",
    "            'total_counts': total_counts,\n",
    "            'total_density': total_dens,\n",
    "            'display_x': x_all,\n",
    "            'display_y': y_all\n",
    "        }\n",
    "\n",
    "    # Axes cosmetics\n",
    "    ax.set_xscale('log')\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim[0], xlim[1])\n",
    "    else:\n",
    "        ax.set_xlim(bins[0], bins[-1])\n",
    "\n",
    "    ax.set_xlabel('ISI [ms]', fontsize=10); ax.set_ylabel('Probability', fontsize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8, length=5, width=1, direction='out')\n",
    "    ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "    ax.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out = {\n",
    "        'dedup_events': dedup,\n",
    "        'isi_all': isi_all,\n",
    "        'frame_ms': frame_ms,\n",
    "        'bins': bins,\n",
    "        'color_map': cmap,\n",
    "        'combined': combined\n",
    "    }\n",
    "\n",
    "    # Export main plot + separate legend\n",
    "    if export_path is not None:\n",
    "        ts = _dt.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        outdir = Path(export_path) / f\"inter_saccade_export_{ts}\"\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(outdir/plot_name, format='pdf', bbox_inches='tight', dpi=300)\n",
    "\n",
    "        if legend_handles:\n",
    "            fig_leg = plt.figure(figsize=(2.0, 0.28*max(1,len(legend_labels))+0.4), dpi=300)\n",
    "            fig_leg.legend(legend_handles, legend_labels, loc='center', frameon=False, ncol=1, prop={'size':8})\n",
    "            fig_leg.savefig(outdir/('legend_'+plot_name), format='pdf', bbox_inches='tight', dpi=300)\n",
    "            plt.close(fig_leg)\n",
    "        print(\"Exported plot + separate legend to:\", outdir)\n",
    "\n",
    "    plt.show()\n",
    "    return out\n"
   ],
   "id": "dfeea32a6b6dc757",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def export_inter_saccade_intervals_density_traces_from_blocks(\n",
    "    block_collection,\n",
    "    figure_size=(1.8, 1.2),\n",
    "    export_path=None,\n",
    "    plot_name='ISI_histogram.pdf',\n",
    "    num_bins=20,\n",
    "    high_ms=20000.0,\n",
    "    pair_threshold_ms=60.0,\n",
    "    min_isi_ms=None,          # if None -> 0.75 * frame_ms\n",
    "    first_bin_factor=0.5,     # first real edge = 0.5×Δt\n",
    "    add_leading_zero_bin=True,\n",
    "    font_family='Arial',\n",
    "    color_template=\"okabeito\",\n",
    "    custom_colors=None,\n",
    "    color_map=None,            # <-- pass a dict {'PV_62':'#...', ...} to lock colors\n",
    "    xlim=None\n",
    "):\n",
    "    # Collect & dedupe\n",
    "    events = _collect_events_from_blocks(block_collection)\n",
    "    if events.empty:\n",
    "        raise ValueError(\"No saccade events found.\")\n",
    "    dedup = _dedupe_lr_pairs(events, pair_threshold_ms=pair_threshold_ms)\n",
    "\n",
    "    # Frame interval & ISIs\n",
    "    frame_ms = _estimate_frame_interval_ms(block_collection)\n",
    "    min_isi_eff = (0.75*frame_ms) if (min_isi_ms is None) else float(min_isi_ms)\n",
    "    isi_all = _compute_blockwise_isis(dedup, min_isi_ms=min_isi_eff)\n",
    "    animals = sorted(dedup['animal'].dropna().unique())\n",
    "\n",
    "    # Colors\n",
    "    if color_map is None:\n",
    "        cmap = build_color_map(animals, template=color_template, custom=custom_colors, order=animals)\n",
    "    else:\n",
    "        # ensure every animal has a color; fall back to template if needed\n",
    "        fallback = build_color_map(animals, template=color_template, custom=custom_colors, order=animals)\n",
    "        cmap = {a: color_map.get(a, fallback[a]) for a in animals}\n",
    "\n",
    "    # Binning (global bins; we’ll fold mass at plot-time based on xlim)\n",
    "    low_ms = max(1e-6, first_bin_factor*frame_ms)\n",
    "    bins = _snapped_log_bins(num_bins, frame_ms, low_ms, float(high_ms), add_leading_zero_bin)\n",
    "    bin_centers = np.sqrt(bins[:-1]*bins[1:])\n",
    "\n",
    "    # Style & plot\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = [font_family]\n",
    "    fig, ax = plt.subplots(figsize=figure_size, dpi=300)\n",
    "    legend_handles, legend_labels = [], []\n",
    "\n",
    "    # Helper: fold all probability mass outside xlim into edge bins\n",
    "    def _fold_counts_to_xlim(counts, edges, xlim_tuple):\n",
    "        \"\"\"\n",
    "        counts: per-bin probabilities (sum == 1)\n",
    "        edges:  bin edges, len = len(counts) + 1\n",
    "        xlim_tuple: (low, high)\n",
    "        Returns: disp_counts (renormalized), disp_edges, disp_centers, idx_lo, idx_hi\n",
    "        \"\"\"\n",
    "        low, high = float(xlim_tuple[0]), float(xlim_tuple[1])\n",
    "        if not (np.isfinite(low) and np.isfinite(high) and low < high):\n",
    "            # invalid; just return original\n",
    "            centers = np.sqrt(edges[:-1]*edges[1:])\n",
    "            return counts, edges, centers, 0, len(counts)-1\n",
    "\n",
    "        # Index of first displayed bin (bin whose interval intersects [low, ...])\n",
    "        # We'll \"fold\" everything strictly below `low` into this bin.\n",
    "        # Use bin index where edges[idx] <= low < edges[idx+1]; clamp to valid range\n",
    "        idx_lo = np.searchsorted(edges, low, side='right') - 1\n",
    "        idx_lo = max(0, min(idx_lo, len(counts)-1))\n",
    "\n",
    "        # Index of last displayed bin (bin whose interval intersects [..., high])\n",
    "        # We'll fold everything strictly above `high` into this bin.\n",
    "        idx_hi = np.searchsorted(edges, high, side='left') - 1\n",
    "        idx_hi = max(0, min(idx_hi, len(counts)-1))\n",
    "\n",
    "        counts_fold = counts.copy()\n",
    "\n",
    "        # Mass below: bins with right edge <= low belong to the left fold mass\n",
    "        left_mask = edges[1:] <= low\n",
    "        left_mass = counts_fold[left_mask].sum()\n",
    "        # zero them out and add to idx_lo\n",
    "        counts_fold[left_mask] = 0.0\n",
    "        counts_fold[idx_lo] += left_mass\n",
    "\n",
    "        # Mass above: bins with left edge >= high belong to the right fold mass\n",
    "        right_mask = edges[:-1] >= high\n",
    "        right_mass = counts_fold[right_mask].sum()\n",
    "        counts_fold[right_mask] = 0.0\n",
    "        counts_fold[idx_hi] += right_mass\n",
    "\n",
    "        # Now restrict visible slice\n",
    "        disp_counts = counts_fold[idx_lo:idx_hi+1]\n",
    "        disp_edges = edges[idx_lo:idx_hi+2]  # +2 because edges are one longer\n",
    "        # Numerical safety: renormalize to sum=1\n",
    "        s = disp_counts.sum()\n",
    "        if s > 0:\n",
    "            disp_counts = disp_counts / s\n",
    "\n",
    "        disp_centers = np.sqrt(disp_edges[:-1]*disp_edges[1:])\n",
    "        return disp_counts, disp_edges, disp_centers, idx_lo, idx_hi\n",
    "\n",
    "    for a in animals:\n",
    "        isi = isi_all.get(a, np.array([]))\n",
    "        if isi.size == 0:\n",
    "            continue\n",
    "\n",
    "        # Histogram over global bins\n",
    "        hist, _ = np.histogram(isi, bins=bins)\n",
    "        if hist.sum() == 0:\n",
    "            continue\n",
    "        dens = hist.astype(float) / hist.sum()\n",
    "\n",
    "        if xlim is not None:\n",
    "            # Fold tails into the first/last visible bins\n",
    "            disp_dens, disp_edges, disp_centers, idx_lo, idx_hi = _fold_counts_to_xlim(dens, bins, xlim)\n",
    "            # Plot at bin centers within the displayed slice\n",
    "            x_plot, y_plot = disp_centers, disp_dens\n",
    "        else:\n",
    "            # default behavior (optionally with a dummy first point)\n",
    "            if add_leading_zero_bin:\n",
    "                x_plot = np.r_[bins[0], bin_centers[1:]]\n",
    "                y_plot = np.r_[0.0, dens[1:]]\n",
    "            else:\n",
    "                x_plot, y_plot = bin_centers, dens\n",
    "\n",
    "        h, = ax.plot(x_plot, y_plot, linewidth=1.2, color=cmap[a], label=str(a))\n",
    "        legend_handles.append(h)\n",
    "        legend_labels.append(str(a))\n",
    "\n",
    "    ax.set_xscale('log')\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim[0], xlim[1])\n",
    "    else:\n",
    "        ax.set_xlim(bins[0], bins[-1])\n",
    "\n",
    "    ax.set_xlabel('ISI [ms]', fontsize=10)\n",
    "    ax.set_ylabel('Probability', fontsize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8, length=5, width=1, direction='out')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out = {\n",
    "        'dedup_events': dedup,\n",
    "        'isi_all': isi_all,\n",
    "        'frame_ms': frame_ms,\n",
    "        'bins': bins,\n",
    "        'color_map': cmap\n",
    "    }\n",
    "\n",
    "    # Export main plot + separate legend\n",
    "    if export_path is not None:\n",
    "        ts = _dt.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        outdir = Path(export_path) / f\"inter_saccade_export_{ts}\"\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(outdir/plot_name, format='pdf', bbox_inches='tight', dpi=300)\n",
    "\n",
    "        if legend_handles:\n",
    "            fig_leg = plt.figure(figsize=(2.0, 0.28*max(1,len(legend_labels))+0.4), dpi=300)\n",
    "            fig_leg.legend(legend_handles, legend_labels, loc='center', frameon=False, ncol=1, prop={'size':8})\n",
    "            fig_leg.savefig(outdir/('legend_'+plot_name), format='pdf', bbox_inches='tight', dpi=300)\n",
    "            plt.close(fig_leg)\n",
    "        print(\"Exported plot + separate legend to:\", outdir)\n",
    "\n",
    "    plt.show()\n",
    "    return out\n"
   ],
   "id": "51e3c4130b5a22f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "out = export_inter_saccade_intervals_density_traces_from_blocks(\n",
    "    block_collection,\n",
    "    figure_size=(1.7, 1.7),\n",
    "    export_path=None,\n",
    "    plot_name='ISI_histogram.pdf',\n",
    "    first_bin_factor=0,\n",
    "    color_map=color_map,\n",
    "    xlim=(100, 20000),\n",
    "    add_leading_zero_bin=False,\n",
    "    num_bins=20\n",
    ")\n"
   ],
   "id": "7f04d8e9360452bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Universal color utilities\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "from pathlib import Path\n",
    "import datetime as _dt\n",
    "\n",
    "COLOR_TEMPLATES = {\n",
    "    # Colorblind-safe (Okabe–Ito)\n",
    "    \"okabeito\": [\"#0072B2\", \"#D55E00\", \"#009E73\", \"#CC79A7\",\n",
    "                 \"#F0E442\", \"#56B4E9\", \"#E69F00\", \"#000000\"],\n",
    "    # Tableau-10 as an alternative\n",
    "    \"tableau10\": [plt.get_cmap(\"tab10\")(i) for i in range(10)],\n",
    "}\n",
    "\n",
    "def build_color_map(labels, template=\"okabeito\", custom=None, order=None):\n",
    "    \"\"\"\n",
    "    Return dict {label -> color}. If you want stable per-animal colors across figures,\n",
    "    pass a canonical 'order' (list of all animals) or pass an explicit color_map to the\n",
    "    plotting functions.\n",
    "    \"\"\"\n",
    "    labs = list(order) if order is not None else list(labels)\n",
    "    base = list(custom) if custom is not None else list(COLOR_TEMPLATES.get(template, COLOR_TEMPLATES[\"okabeito\"]))\n",
    "    return {lab: base[i % len(base)] for i, lab in enumerate(labs)}\n",
    "\n",
    "def set_axes_color_cycle(ax, labels, color_map):\n",
    "    ax.set_prop_cycle(cycler(color=[color_map[l] for l in labels]))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ISI density traces from block_collection (accepts color_map)\n",
    "# ============================================================\n",
    "def _collect_events_from_blocks(block_collection):\n",
    "    rows = []\n",
    "    for b in block_collection:\n",
    "        animal = getattr(b, 'animal_call', None)\n",
    "        blockn = getattr(b, 'block_num', None)\n",
    "        for eye, sdf in (('L', getattr(b, 'l_saccade_df', None)),\n",
    "                         ('R', getattr(b, 'r_saccade_df', None))):\n",
    "            if sdf is None or len(sdf) == 0: continue\n",
    "            s_on = pd.to_numeric(sdf.get('saccade_on_ms', pd.Series([], dtype='float64')), errors='coerce')\n",
    "            hm = sdf['head_movement'] if 'head_movement' in sdf.columns else pd.Series(False, index=sdf.index)\n",
    "            rows.append(pd.DataFrame({\n",
    "                'animal': animal, 'block': blockn, 'eye': eye,\n",
    "                'saccade_on_ms': s_on, 'head_movement': hm.astype(bool)\n",
    "            }).dropna(subset=['saccade_on_ms']))\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(\n",
    "        columns=['animal','block','eye','saccade_on_ms','head_movement'])\n",
    "\n",
    "def _estimate_frame_interval_ms(block_collection, fallback_ms=16.67):\n",
    "    dts=[]\n",
    "    for b in block_collection:\n",
    "        for df in (getattr(b,'left_eye_data',None), getattr(b,'right_eye_data',None)):\n",
    "            if df is None or 'ms_axis' not in df.columns: continue\n",
    "            ms = pd.to_numeric(df['ms_axis'], errors='coerce').dropna().to_numpy()\n",
    "            if ms.size>3:\n",
    "                dt = np.diff(ms); dt = dt[(dt>0)&np.isfinite(dt)]\n",
    "                if dt.size: dts.append(np.median(dt))\n",
    "    return float(np.median(dts)) if dts else float(fallback_ms)\n",
    "\n",
    "def _dedupe_lr_pairs(events_df, pair_threshold_ms=60.0):\n",
    "    out=[]\n",
    "    for (animal,block), g in events_df.groupby(['animal','block'], dropna=False):\n",
    "        L = g[g['eye']=='L'].sort_values('saccade_on_ms')\n",
    "        R = g[g['eye']=='R'].sort_values('saccade_on_ms')\n",
    "        lt, rt = L['saccade_on_ms'].to_numpy(float), R['saccade_on_ms'].to_numpy(float)\n",
    "        lhm = L['head_movement'].to_numpy(bool) if len(L) else np.array([],bool)\n",
    "        rhm = R['head_movement'].to_numpy(bool) if len(R) else np.array([],bool)\n",
    "        i=j=0; ev_t=[]; ev_hm=[]\n",
    "        while i<len(lt) and j<len(rt):\n",
    "            dt = lt[i]-rt[j]\n",
    "            if abs(dt)<=pair_threshold_ms:\n",
    "                ev_t.append(min(lt[i],rt[j])); ev_hm.append(bool(lhm[i] or rhm[j])); i+=1; j+=1\n",
    "            elif lt[i] < rt[j]-pair_threshold_ms:\n",
    "                ev_t.append(lt[i]); ev_hm.append(bool(lhm[i])); i+=1\n",
    "            else:\n",
    "                ev_t.append(rt[j]); ev_hm.append(bool(rhm[j])); j+=1\n",
    "        while i<len(lt): ev_t.append(lt[i]); ev_hm.append(bool(lhm[i])); i+=1\n",
    "        while j<len(rt): ev_t.append(rt[j]); ev_hm.append(bool(rhm[j])); j+=1\n",
    "        out.append(pd.DataFrame({'animal':animal,'block':block,'event_time_ms':ev_t,'head_movement':ev_hm}))\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame(\n",
    "        columns=['animal','block','event_time_ms','head_movement'])\n",
    "\n",
    "def _compute_blockwise_isis(dedup_df, min_isi_ms):\n",
    "    isi_all={}\n",
    "    for (animal,block), g in dedup_df.groupby(['animal','block'], dropna=False):\n",
    "        t = np.sort(g['event_time_ms'].to_numpy())\n",
    "        if t.size>1:\n",
    "            d = np.diff(t); d = d[np.isfinite(d) & (d>=min_isi_ms)]\n",
    "            if d.size: isi_all.setdefault(animal,[]).append(d)\n",
    "    return {a:(np.concatenate(v) if v else np.array([],float)) for a,v in isi_all.items()}\n",
    "\n",
    "def _snapped_log_bins(num_bins, frame_ms, low_ms, high_ms, add_leading_zero_bin=True):\n",
    "    # snap edges to (k+0.5)*Δt so each bin contains ≥1 realizable ISI (k*Δt)\n",
    "    raw = np.geomspace(low_ms, high_ms, int(num_bins)+1)\n",
    "    k_edges = np.floor(raw/frame_ms) + 0.5\n",
    "    kmax = int(np.floor(high_ms/frame_ms))\n",
    "    k_edges = np.clip(k_edges, 0.5, kmax+0.5)\n",
    "    k_edges = np.unique(k_edges)\n",
    "    if add_leading_zero_bin and (k_edges.size==0 or not np.isclose(k_edges[0],0.5)):\n",
    "        k_edges = np.r_[0.25, k_edges]          # dummy zero-count bin\n",
    "    if k_edges[-1] < (kmax+0.5):\n",
    "        k_edges = np.r_[k_edges, (kmax+0.5)]\n",
    "    return k_edges * frame_ms\n",
    "\n",
    "def export_inter_saccade_intervals_density_traces_from_blocks(\n",
    "    block_collection,\n",
    "    figure_size=(1.8, 1.2),\n",
    "    export_path=None,\n",
    "    plot_name='ISI_histogram.pdf',\n",
    "    num_bins=20,\n",
    "    high_ms=20000.0,\n",
    "    pair_threshold_ms=60.0,\n",
    "    min_isi_ms=None,          # if None -> 0.75 * frame_ms\n",
    "    first_bin_factor=0.5,     # first real edge = 0.5×Δt\n",
    "    add_leading_zero_bin=True,\n",
    "    font_family='Arial',\n",
    "    color_template=\"okabeito\",\n",
    "    custom_colors=None,\n",
    "    color_map=None,            # <-- pass a dict {'PV_62':'#...', ...} to lock colors\n",
    "    xlim=None\n",
    "):\n",
    "    # Collect & dedupe\n",
    "    events = _collect_events_from_blocks(block_collection)\n",
    "    if events.empty: raise ValueError(\"No saccade events found.\")\n",
    "    dedup = _dedupe_lr_pairs(events, pair_threshold_ms=pair_threshold_ms)\n",
    "\n",
    "    # Frame interval & ISIs\n",
    "    frame_ms = _estimate_frame_interval_ms(block_collection)\n",
    "    min_isi_eff = (0.75*frame_ms) if (min_isi_ms is None) else float(min_isi_ms)\n",
    "    isi_all = _compute_blockwise_isis(dedup, min_isi_ms=min_isi_eff)\n",
    "    animals = sorted(dedup['animal'].dropna().unique())\n",
    "\n",
    "    # Colors\n",
    "    if color_map is None:\n",
    "        cmap = build_color_map(animals, template=color_template, custom=custom_colors, order=animals)\n",
    "    else:\n",
    "        # ensure every animal has a color; fall back to template if needed\n",
    "        fallback = build_color_map(animals, template=color_template, custom=custom_colors, order=animals)\n",
    "        cmap = {a: color_map.get(a, fallback[a]) for a in animals}\n",
    "\n",
    "    # Binning\n",
    "    low_ms = max(1e-6, first_bin_factor*frame_ms)\n",
    "    bins = _snapped_log_bins(num_bins, frame_ms, low_ms, float(high_ms), add_leading_zero_bin)\n",
    "    bin_centers = np.sqrt(bins[:-1]*bins[1:])\n",
    "\n",
    "    # Style & plot\n",
    "    plt.rcParams['font.family'] = 'sans-serif'; plt.rcParams['font.sans-serif'] = [font_family]\n",
    "    fig, ax = plt.subplots(figsize=figure_size, dpi=300)\n",
    "    legend_handles, legend_labels = [], []\n",
    "\n",
    "    for a in animals:\n",
    "        isi = isi_all.get(a, np.array([]))\n",
    "        if isi.size==0: continue\n",
    "        hist, _ = np.histogram(isi, bins=bins)\n",
    "        dens = hist/hist.sum() if hist.sum()>0 else hist\n",
    "        if add_leading_zero_bin:\n",
    "            x_plot = np.r_[bins[0], bin_centers[1:]]; y_plot = np.r_[0.0, dens[1:]]\n",
    "        else:\n",
    "            x_plot, y_plot = bin_centers, dens\n",
    "        h, = ax.plot(x_plot, y_plot, linewidth=1.2, color=cmap[a], label=str(a))\n",
    "        legend_handles.append(h); legend_labels.append(str(a))\n",
    "\n",
    "    ax.set_xscale('log');\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim[0],xlim[1])\n",
    "    else:\n",
    "        ax.set_xlim(bins[0], bins[-1])\n",
    "\n",
    "    ax.set_xlabel('ISI [ms]', fontsize=10); ax.set_ylabel('Probability', fontsize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8, length=5, width=1, direction='out')\n",
    "    ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "    ax.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out = {'dedup_events': dedup, 'isi_all': isi_all, 'frame_ms': frame_ms, 'bins': bins, 'color_map': cmap}\n",
    "\n",
    "    # Export main plot + separate legend\n",
    "    if export_path is not None:\n",
    "        ts = _dt.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        outdir = Path(export_path) / f\"inter_saccade_export_{ts}\"\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(outdir/plot_name, format='pdf', bbox_inches='tight', dpi=300)\n",
    "\n",
    "        if legend_handles:\n",
    "            fig_leg = plt.figure(figsize=(2.0, 0.28*max(1,len(legend_labels))+0.4), dpi=300)\n",
    "            fig_leg.legend(legend_handles, legend_labels, loc='center', frameon=False, ncol=1, prop={'size':8})\n",
    "            fig_leg.savefig(outdir/('legend_'+plot_name), format='pdf', bbox_inches='tight', dpi=300)\n",
    "            plt.close(fig_leg)\n",
    "        print(\"Exported plot + separate legend to:\", outdir)\n",
    "\n",
    "    plt.show()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "36399adda4f9591f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "color_map =  {\n",
    "    'PV_106': '#0072B2',\n",
    "    'PV_126': '#D55E00',\n",
    "    'PV_143': '#009E73',\n",
    "    'PV_57': '#CC79A7',\n",
    "    'PV_62': '#F0E442'}\n",
    "export_path = r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_4\\material\\Inter_saccade_intervals_histogram'\n",
    "out = export_inter_saccade_intervals_density_traces_from_blocks(\n",
    "    block_collection,\n",
    "    figure_size=(1.7, 1.7),\n",
    "    export_path=None,\n",
    "    plot_name='ISI_histogram.pdf',\n",
    "    first_bin_factor=0,\n",
    "    color_map=color_map,\n",
    "    xlim=(100,20000),\n",
    "    add_leading_zero_bin=False,\n",
    "    num_bins=25\n",
    "    )"
   ],
   "id": "c705ef3dc3b55d98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "export_path = r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_4\\material\\Inter_saccade_intervals_histogram'\n",
    "out = export_inter_saccade_intervals_density_traces_from_blocks(\n",
    "    block_collection,\n",
    "    figure_size=(2.5, 1.7),\n",
    "    export_path=None,\n",
    "    num_bins=20,\n",
    "    high_ms=20000,\n",
    "    add_leading_zero_bin=True,   # keep this\n",
    "    first_bin_factor=0.5         # keeps first real edge at 0.5×Δt\n",
    ")\n",
    "\n",
    "\n",
    "# create two plots - one log scale for higher start threshold (so the peak at 100ms is gone), another lin scale for lower ISI values (which will not be inflated)."
   ],
   "id": "29c490b3c9e98b23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "def plot_saccade_periodogram_from_out(out,\n",
    "                                      fs_hz=1000,\n",
    "                                      fmax_hz=20.0,\n",
    "                                      nperseg=None,\n",
    "                                      detrend='constant',\n",
    "                                      logy=True,\n",
    "                                      figure_size=(2.5, 1.7),\n",
    "                                      color_template=\"okabeito\",\n",
    "                                      custom_colors=None,\n",
    "                                      log_bin_x=True,\n",
    "                                      num_log_bins=40,\n",
    "                                      agg_func='median'):\n",
    "    \"\"\"\n",
    "    Build a binary event train per block, estimate PSD with Welch per block,\n",
    "    average PSDs across blocks per animal, and (optionally) re-bin frequencies\n",
    "    into logarithmically spaced bins.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    out : dict\n",
    "        Must include 'dedup_events' DataFrame with ['animal','block','event_time_ms'].\n",
    "    log_bin_x : bool\n",
    "        If True, compress frequency axis into logarithmic bins.\n",
    "    num_log_bins : int\n",
    "        Number of bins to use on the log frequency axis.\n",
    "    agg_func : str\n",
    "        'mean' or 'median' across Welch frequencies falling into the same bin.\n",
    "    \"\"\"\n",
    "    dedup = out['dedup_events']\n",
    "    animals = sorted(dedup['animal'].dropna().unique())\n",
    "\n",
    "    # --- build colormap (fallback if missing) ---\n",
    "    cmap = out.get('color_map', None)\n",
    "    if cmap is None:\n",
    "        COLOR_TEMPLATES = {\n",
    "            \"okabeito\": [\"#0072B2\", \"#D55E00\", \"#009E73\", \"#CC79A7\",\n",
    "                         \"#F0E442\", \"#56B4E9\", \"#E69F00\", \"#000000\"],\n",
    "            \"tableau10\": [plt.get_cmap(\"tab10\")(i) for i in range(10)],\n",
    "        }\n",
    "        base = list(custom_colors) if custom_colors is not None else COLOR_TEMPLATES[color_template]\n",
    "        cmap = {lab: base[i % len(base)] for i, lab in enumerate(animals)}\n",
    "\n",
    "    per_animal_psd = {}\n",
    "    f_ref = None\n",
    "\n",
    "    for animal in animals:\n",
    "        block_psds = []\n",
    "        for block_id, g_block in dedup[dedup['animal'] == animal].groupby('block', dropna=False):\n",
    "            t_ms = np.asarray(g_block['event_time_ms'], dtype=float)\n",
    "            if t_ms.size < 2: continue\n",
    "\n",
    "            t0 = np.nanmin(t_ms)\n",
    "            tt = t_ms - t0\n",
    "            n_samples = int(np.ceil(tt.max() * fs_hz / 1000.0)) + 1\n",
    "            if n_samples < int(2*fs_hz): continue\n",
    "\n",
    "            idx = np.unique(np.clip(np.round(tt * fs_hz / 1000.0).astype(int), 0, n_samples-1))\n",
    "            x = np.zeros(n_samples, dtype=np.float32); x[idx] = 1.0\n",
    "\n",
    "            use_nperseg = int(nperseg) if nperseg else min(n_samples, int(8*fs_hz))\n",
    "            use_nperseg = min(use_nperseg, n_samples)\n",
    "\n",
    "            f, Pxx = signal.welch(x, fs=fs_hz, nperseg=use_nperseg,\n",
    "                                  noverlap=use_nperseg//2, detrend=detrend)\n",
    "\n",
    "            if f_ref is None:\n",
    "                f_ref = f\n",
    "            block_psds.append(Pxx)\n",
    "\n",
    "        if block_psds:\n",
    "            per_animal_psd[animal] = np.mean(np.vstack(block_psds), axis=0)\n",
    "\n",
    "    if not per_animal_psd:\n",
    "        raise ValueError(\"No PSDs computed.\")\n",
    "\n",
    "    # --- log re-binning of frequency axis ---\n",
    "    if log_bin_x:\n",
    "        fmin = np.min(f_ref[1:])  # skip DC\n",
    "        fmax = fmax_hz\n",
    "        log_edges = np.geomspace(fmin, fmax, num_log_bins+1)\n",
    "        f_bins = np.sqrt(log_edges[:-1] * log_edges[1:])  # geometric centers\n",
    "\n",
    "        per_animal_binned = {}\n",
    "        for animal, Pxx in per_animal_psd.items():\n",
    "            binned = []\n",
    "            for i in range(len(log_edges)-1):\n",
    "                mask = (f_ref >= log_edges[i]) & (f_ref < log_edges[i+1])\n",
    "                if not np.any(mask):\n",
    "                    binned.append(np.nan)\n",
    "                else:\n",
    "                    vals = Pxx[mask]\n",
    "                    if agg_func == 'median':\n",
    "                        binned.append(np.median(vals))\n",
    "                    else:\n",
    "                        binned.append(np.mean(vals))\n",
    "            per_animal_binned[animal] = np.array(binned)\n",
    "        f_plot = f_bins\n",
    "        psd_to_plot = per_animal_binned\n",
    "    else:\n",
    "        f_plot = f_ref\n",
    "        psd_to_plot = per_animal_psd\n",
    "\n",
    "    # --- plot ---\n",
    "    fig, ax = plt.subplots(figsize=figure_size, dpi=300)\n",
    "    for animal, Pxx in psd_to_plot.items():\n",
    "        if logy:\n",
    "            ax.semilogy(f_plot, Pxx, lw=1.2, color=cmap[animal], label=str(animal))\n",
    "        else:\n",
    "            ax.plot(f_plot, Pxx, lw=1.2, color=cmap[animal], label=str(animal))\n",
    "\n",
    "    ax.set_xscale('log' if log_bin_x else 'linear')\n",
    "    ax.set_xlim(f_plot[0], fmax_hz)\n",
    "    ax.set_xlabel(\"Frequency [Hz]\", fontsize=10)\n",
    "    ax.set_ylabel(\"Power spectral density\", fontsize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8, length=5, width=1, direction='out')\n",
    "    ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "    #ax.legend(frameon=False, fontsize=8, ncol=1)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    return {'f': f_plot, 'Pxx': psd_to_plot, 'fs_hz': fs_hz, 'color_map': cmap}\n",
    "plot_saccade_periodogram_from_out(out,log_bin_x=False)\n",
    "\n",
    "#This is not optimal, make a binary series of saccade events - within it the periodogram will tell me which frequencies are constructing the signal - this could help with te"
   ],
   "id": "564924965646c4d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_bin_edges_vs_framegrid(out,\n",
    "                                k_show_max=None,           # e.g., 40 (defaults to cover your last bin)\n",
    "                                zoom_low_ms=5, zoom_high_ms=300,   # low-range zoom for readability\n",
    "                                annotate_bins=True,\n",
    "                                show_table_first_n=12):     # how many bins to list in the printed table\n",
    "    \"\"\"\n",
    "    Visualize snapped histogram edges relative to the frame quantization grid (k * frame_ms).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    out : dict\n",
    "        The return dict from export_inter_saccade_intervals_density_traces_from_blocks(...).\n",
    "        Must contain keys: 'bins' (array of edges, ms) and 'frame_ms' (float).\n",
    "    k_show_max : int or None\n",
    "        Highest integer k to plot as grid markers (k * frame_ms). If None, inferred from bins.\n",
    "    zoom_low_ms, zoom_high_ms : float\n",
    "        X-limits for a zoomed low-ISI panel (ms).\n",
    "    annotate_bins : bool\n",
    "        If True, labels the first few bins with their index.\n",
    "    show_table_first_n : int\n",
    "        Prints a table for the first N bins with their [low, high) and the integer k values inside.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    table_df : pd.DataFrame\n",
    "        A dataframe summarizing the first `show_table_first_n` bins and their covered k’s.\n",
    "    \"\"\"\n",
    "    bins = np.asarray(out['bins'], dtype=float)\n",
    "    frame_ms = float(out['frame_ms'])\n",
    "    if bins.ndim != 1 or bins.size < 2:\n",
    "        raise ValueError(\"`out['bins']` must be a 1D array of bin edges (len >= 2).\")\n",
    "\n",
    "    # Decide how many k grid points to draw\n",
    "    if k_show_max is None:\n",
    "        k_show_max = int(np.floor(bins[-1] / frame_ms))\n",
    "\n",
    "    # Build figure: full range + zoom\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8.0, 2.6), dpi=300, constrained_layout=True, sharey=True)\n",
    "    ax_full, ax_zoom = axes\n",
    "\n",
    "    for ax, (xmin, xmax) in zip(\n",
    "        (ax_full, ax_zoom),\n",
    "        ((bins[0], bins[-1]), (zoom_low_ms, zoom_high_ms))\n",
    "    ):\n",
    "        # Log x-axis for both panels\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(0, 1)  # dummy y, we’re just showing guides\n",
    "\n",
    "        # Alternating shaded bins for readability\n",
    "        for i in range(len(bins) - 1):\n",
    "            left, right = bins[i], bins[i + 1]\n",
    "            ax.axvspan(left, right, color='white' if i % 2 == 0 else '0.15', alpha=0.15, lw=0)\n",
    "\n",
    "        # Bin edges as vertical lines\n",
    "        for i, x in enumerate(bins):\n",
    "            ax.axvline(x, color='0.5', lw=0.8, alpha=0.9)\n",
    "            if annotate_bins:\n",
    "                # Only annotate if the text will fall within the x-limits\n",
    "                if x > xmin and x < xmax:\n",
    "                    ax.text(x, 0.92, f\"{i}\", rotation=90, va='top', ha='center', fontsize=7)\n",
    "\n",
    "        # k·Δt grid markers\n",
    "        k_vals = np.arange(1, k_show_max + 1)\n",
    "        grid_ms = k_vals * frame_ms\n",
    "        in_range = (grid_ms >= xmin) & (grid_ms <= xmax)\n",
    "        ax.plot(grid_ms[in_range], np.full(in_range.sum(), 0.5), 'o', ms=3, alpha=0.9, label='k·Δt')\n",
    "        # thin helper stems to x-axis for visual alignment\n",
    "        for xk in grid_ms[in_range]:\n",
    "            ax.vlines(xk, 0.35, 0.65, color='C0', lw=0.6, alpha=0.6)\n",
    "\n",
    "        ax.set_xlabel('Time [ms]', fontsize=9)\n",
    "        ax.tick_params(axis='both', which='both', labelsize=8, length=4, width=0.8, direction='out')\n",
    "        ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    ax_full.set_title('Bin edges vs frame grid (full range)', fontsize=9)\n",
    "    ax_zoom.set_title(f'Zoomed ({zoom_low_ms:g}–{zoom_high_ms:g} ms)', fontsize=9)\n",
    "\n",
    "    # ---- Build a small table for the first N bins: which integer k land inside [edge_i, edge_{i+1})\n",
    "    def ks_in_bin(left, right, frame_ms):\n",
    "        # k is inside if k*Δt ∈ [left, right)\n",
    "        k_min = int(np.ceil(left / frame_ms))\n",
    "        k_max = int(np.floor((np.nextafter(right, -np.inf)) / frame_ms))\n",
    "        if k_max < k_min:\n",
    "            return []\n",
    "        return list(range(k_min, k_max + 1))\n",
    "\n",
    "    rows = []\n",
    "    N = min(show_table_first_n, len(bins) - 1)\n",
    "    for i in range(N):\n",
    "        left, right = bins[i], bins[i + 1]\n",
    "        ks = ks_in_bin(left, right, frame_ms)\n",
    "        rows.append({\n",
    "            'bin_idx': i,\n",
    "            'left_ms': left,\n",
    "            'right_ms': right,\n",
    "            'width_ms': right - left,\n",
    "            'k_values_in_bin': ks,\n",
    "            'num_k': len(ks)\n",
    "        })\n",
    "\n",
    "    table_df = pd.DataFrame(rows)\n",
    "    # Pretty print\n",
    "    with pd.option_context('display.max_colwidth', None, 'display.precision', 3):\n",
    "        print(\"\\nEarly-bin coverage relative to frame grid (k·Δt):\")\n",
    "        print(table_df)\n",
    "\n",
    "    plt.show()\n",
    "    return table_df\n",
    "# Now visualize bin edges vs the 60 Hz grid\n",
    "_ = plot_bin_edges_vs_framegrid(out,\n",
    "                                k_show_max=None,         # auto from last edge\n",
    "                                zoom_low_ms=5,\n",
    "                                zoom_high_ms=300,\n",
    "                                annotate_bins=True,\n",
    "                                show_table_first_n=14)"
   ],
   "id": "9ebefb606e1ff43f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# for this - send the ISI vector to Mark for a check ^^",
   "id": "4265adc3f9348371",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Single cell: robust HMM fitting over saccade_collection with Main/Sub pairing and required features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "def fit_hmm_on_saccade_collection(\n",
    "    saccade_collection,\n",
    "    n_states=3,\n",
    "    pair_threshold_ms=25.0,\n",
    "    # prefer pixel magnitude if present; fallbacks handled automatically\n",
    "    magnitude_preference=(\"magnitude_pixel\", \"magnitude_raw_pixel\", \"magnitude_raw_angular\",\n",
    "                          \"net_angular_disp\", \"overall_disp\", \"overall_angle_deg\", \"length\"),\n",
    "    # columns expected in the input; these are auto-detected if missing\n",
    "    time_col_candidates=(\"saccade_on_ms\", \"event_time_ms\", \"saccade_start_ms\", \"saccade_start_timestamp\"),\n",
    "    head_flag_col_candidates=(\"head_movement\", \"head_motion\", \"head_move\"),\n",
    "    eye_col_candidates=(\"eye\", \"Eye\", \"EYE\"),\n",
    "    animal_col_candidates=(\"animal\", \"Animal\", \"subject\"),\n",
    "    block_col_candidates=(\"block\", \"Block\", \"block_num\"),\n",
    "    random_state=0,\n",
    "    max_iter=100\n",
    "):\n",
    "    \"\"\"\n",
    "    saccade_collection : pd.DataFrame or list-like of DataFrames\n",
    "        Each row is a saccade event. Must include at least:\n",
    "        - Main/Sub pairing columns ('Main','Sub') to group L/R events of the same saccade\n",
    "        - Animal and block identifiers (auto-detected)\n",
    "        - At least one magnitude column (auto-detected)\n",
    "        - A time column in ms if available (auto-detected)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : GaussianHMM\n",
    "    events_with_states : pd.DataFrame  # per-collapsed-event features + state\n",
    "    feature_names : list[str]\n",
    "    scaler : StandardScaler\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- 1) Normalize input to a single DataFrame ----------\n",
    "    if isinstance(saccade_collection, list):\n",
    "        df = pd.concat([x.copy() for x in saccade_collection], ignore_index=True)\n",
    "    elif isinstance(saccade_collection, pd.DataFrame):\n",
    "        df = saccade_collection.copy()\n",
    "    else:\n",
    "        raise TypeError(\"saccade_collection must be a DataFrame or a list of DataFrames.\")\n",
    "\n",
    "    # Be safe about column names (strip spaces)\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "    # ---------- 2) Auto-detect critical columns ----------\n",
    "    def first_present(cands):\n",
    "        for c in cands:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    time_col = first_present(time_col_candidates)\n",
    "    if time_col is None:\n",
    "        raise ValueError(f\"No acceptable time column found. Tried: {time_col_candidates}\")\n",
    "\n",
    "    eye_col = first_present(eye_col_candidates)\n",
    "    if eye_col is None:\n",
    "        # If truly missing, treat everything as one eye (monocular)\n",
    "        eye_col = \"__synthetic_eye\"\n",
    "        df[eye_col] = \"unknown\"\n",
    "\n",
    "    head_col = first_present(head_flag_col_candidates)\n",
    "    # make a safe boolean column for head movement\n",
    "    if head_col is None:\n",
    "        df[\"__head_flag__\"] = False\n",
    "        head_col = \"__head_flag__\"\n",
    "    else:\n",
    "        # robust coercion to bool avoiding NaN->int issues\n",
    "        df[\"__head_flag__\"] = pd.to_numeric(df[head_col], errors=\"coerce\").fillna(0.0) != 0.0\n",
    "        head_col = \"__head_flag__\"\n",
    "\n",
    "    animal_col = first_present(animal_col_candidates)\n",
    "    if animal_col is None:\n",
    "        animal_col = \"__animal__\"\n",
    "        df[animal_col] = \"animal\"\n",
    "\n",
    "    block_col = first_present(block_col_candidates)\n",
    "    if block_col is None:\n",
    "        block_col = \"__block__\"\n",
    "        df[block_col] = \"block\"\n",
    "\n",
    "    # Pick magnitude column\n",
    "    mag_col = None\n",
    "    for c in magnitude_preference:\n",
    "        if c in df.columns:\n",
    "            mag_col = c\n",
    "            break\n",
    "    if mag_col is None:\n",
    "        raise ValueError(f\"Could not find a magnitude-like column. Tried: {magnitude_preference}\")\n",
    "\n",
    "    # Try to ensure time is numeric (ms if you have it; otherwise it can be any consistent unit)\n",
    "    df[time_col] = pd.to_numeric(df[time_col], errors=\"coerce\")\n",
    "    # sort for ISI computation\n",
    "    df.sort_values([animal_col, block_col, time_col], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # ---------- 3) Collapse L/R pairs using Main/Sub if present ----------\n",
    "    have_main_sub = (\"Main\" in df.columns) and (\"Sub\" in df.columns)\n",
    "\n",
    "    def collapse_pairs_using_main_sub(g):\n",
    "        \"\"\"\n",
    "        Within an (animal, block) chunk, group by Main (saccade id).\n",
    "        Collapse multi-eye events into one row:\n",
    "          - time = min time in group\n",
    "          - magnitude = max magnitude in group (robust)\n",
    "          - head_movement = any eye True\n",
    "          - synced = True if >=2 unique eyes, else False\n",
    "          - monocular/synchronized flag derived from synced\n",
    "        For lone events (only one eye for that Main), they still appear once (monocular).\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        for main_id, gg in g.groupby(\"Main\", dropna=False):\n",
    "            # Skip NaN Main if it looks invalid and there’s no Sub as well: treat each row as its own main\n",
    "            if pd.isna(main_id):\n",
    "                for _, r in gg.iterrows():\n",
    "                    rows.append({\n",
    "                        animal_col: r[animal_col],\n",
    "                        block_col: r[block_col],\n",
    "                        \"time_ms_like\": r[time_col],\n",
    "                        \"magnitude\": r.get(mag_col, np.nan),\n",
    "                        \"head_move\": bool(r[head_col]),\n",
    "                        \"synced\": False,  # no confirmed pairing\n",
    "                        \"n_eyes\": 1\n",
    "                    })\n",
    "                continue\n",
    "\n",
    "            t = np.nanmin(gg[time_col].values)\n",
    "            mag = np.nanmax(gg[mag_col].values) if mag_col in gg.columns else np.nan\n",
    "            head_any = bool(pd.Series(gg[head_col].values).fillna(False).any())\n",
    "            n_eyes = gg[eye_col].nunique() if eye_col in gg.columns else 1\n",
    "            synced = n_eyes >= 2\n",
    "            rows.append({\n",
    "                animal_col: gg[animal_col].iloc[0],\n",
    "                block_col: gg[block_col].iloc[0],\n",
    "                \"time_ms_like\": t,\n",
    "                \"magnitude\": mag,\n",
    "                \"head_move\": head_any,\n",
    "                \"synced\": synced,\n",
    "                \"n_eyes\": int(n_eyes)\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def collapse_pairs_by_nearest_lr(g, thr_ms=pair_threshold_ms):\n",
    "        \"\"\"\n",
    "        Fallback: if Main/Sub are missing, pair nearest L/R within a threshold.\n",
    "        Unpaired events become monocular.\n",
    "        \"\"\"\n",
    "        g = g.sort_values(time_col).reset_index(drop=True)\n",
    "        used = np.zeros(len(g), dtype=bool)\n",
    "        rows = []\n",
    "        for i, r in g.iterrows():\n",
    "            if used[i]:\n",
    "                continue\n",
    "            t_i = r[time_col]\n",
    "            eye_i = str(r[eye_col])\n",
    "            # look for nearest opposite eye within thr\n",
    "            opp = g[(~used) & (g.index != i) & (g[eye_col] != eye_i)]\n",
    "            if not opp.empty:\n",
    "                # nearest in time\n",
    "                j = (opp[time_col] - t_i).abs().idxmin()\n",
    "                dt = abs(opp.loc[j, time_col] - t_i)\n",
    "                if pd.notna(dt) and dt <= thr_ms:\n",
    "                    # make a pair\n",
    "                    mag = np.nanmax([r.get(mag_col, np.nan), opp.loc[j, mag_col] if mag_col in opp.columns else np.nan])\n",
    "                    head_any = bool(pd.Series([r[head_col], opp.loc[j, head_col]]).fillna(False).any())\n",
    "                    rows.append({\n",
    "                        animal_col: r[animal_col],\n",
    "                        block_col: r[block_col],\n",
    "                        \"time_ms_like\": min(t_i, opp.loc[j, time_col]),\n",
    "                        \"magnitude\": mag,\n",
    "                        \"head_move\": head_any,\n",
    "                        \"synced\": True,\n",
    "                        \"n_eyes\": 2\n",
    "                    })\n",
    "                    used[i] = True\n",
    "                    used[j] = True\n",
    "                    continue\n",
    "            # otherwise monocular\n",
    "            rows.append({\n",
    "                animal_col: r[animal_col],\n",
    "                block_col: r[block_col],\n",
    "                \"time_ms_like\": t_i,\n",
    "                \"magnitude\": r.get(mag_col, np.nan),\n",
    "                \"head_move\": bool(r[head_col]),\n",
    "                \"synced\": False,\n",
    "                \"n_eyes\": 1\n",
    "            })\n",
    "            used[i] = True\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    collapsed_list = []\n",
    "    for (ani, blk), g in df.groupby([animal_col, block_col], dropna=False):\n",
    "        if have_main_sub:\n",
    "            collapsed = collapse_pairs_using_main_sub(g)\n",
    "        else:\n",
    "            collapsed = collapse_pairs_by_nearest_lr(g)\n",
    "        collapsed_list.append(collapsed)\n",
    "\n",
    "    ev = pd.concat(collapsed_list, ignore_index=True)\n",
    "    ev.sort_values([\"time_ms_like\", animal_col, block_col], inplace=True)\n",
    "    ev.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # ---------- 4) Compute ISI per (animal, block) ----------\n",
    "    ev[\"ISI\"] = np.nan\n",
    "    for (ani, blk), g in ev.groupby([animal_col, block_col], dropna=False):\n",
    "        idx = g.index\n",
    "        t = g[\"time_ms_like\"].values\n",
    "        isi = np.diff(t, prepend=np.nan)\n",
    "        ev.loc[idx, \"ISI\"] = isi\n",
    "\n",
    "    # Some sessions might begin with NaN ISI → fill with median ISI of that (animal, block)\n",
    "    def _fill_group_nan(series):\n",
    "        if series.isna().all():\n",
    "            return series.fillna(0.0)  # degenerate case\n",
    "        return series.fillna(series.dropna().median())\n",
    "\n",
    "    ev[\"ISI\"] = ev.groupby([animal_col, block_col], dropna=False)[\"ISI\"].transform(_fill_group_nan)\n",
    "\n",
    "    # ---------- 5) Build feature matrix ----------\n",
    "    # Binary features as 0/1 floats (no forced int conversion)\n",
    "    ev[\"head_move_f\"] = (ev[\"head_move\"].fillna(False)).astype(float)\n",
    "    ev[\"synced_f\"] = (ev[\"synced\"].fillna(False)).astype(float)\n",
    "\n",
    "    # Magnitude may contain NaN → replace with group median, then global median\n",
    "    if ev[\"magnitude\"].isna().any():\n",
    "        ev[\"magnitude\"] = ev.groupby([animal_col, block_col], dropna=False)[\"magnitude\"].transform(\n",
    "            lambda s: s.fillna(s.median())\n",
    "        )\n",
    "        ev[\"magnitude\"] = ev[\"magnitude\"].fillna(ev[\"magnitude\"].median())\n",
    "\n",
    "    # Small epsilon to avoid zero-variance disasters\n",
    "    eps = 1e-9\n",
    "    X = ev[[\"ISI\", \"magnitude\", \"head_move_f\", \"synced_f\"]].astype(float).values + eps\n",
    "    feature_names = [\"ISI\", \"magnitude\", \"head_move\", \"synced\"]\n",
    "\n",
    "    # ---------- 6) Scale and fit HMM ----------\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # hmmlearn expects a single sequence by default; here we treat the entire set as one sequence.\n",
    "    model = GaussianHMM(\n",
    "        n_components=n_states,\n",
    "        covariance_type=\"full\",\n",
    "        n_iter=max_iter,\n",
    "        random_state=random_state,\n",
    "        verbose=False\n",
    "    )\n",
    "    model.fit(X_scaled)\n",
    "\n",
    "    # ---------- 7) Decode most likely states ----------\n",
    "    states = model.predict(X_scaled)\n",
    "    ev_out = ev.copy()\n",
    "    ev_out[\"state\"] = states\n",
    "\n",
    "    # Keep only the useful columns plus identifiers\n",
    "    keep_cols = [animal_col, block_col, \"time_ms_like\", \"ISI\", \"magnitude\", \"head_move\", \"synced\", \"state\"]\n",
    "    ev_out = ev_out[keep_cols].sort_values([\"time_ms_like\", animal_col, block_col]).reset_index(drop=True)\n",
    "\n",
    "    return model, ev_out, feature_names, scaler\n",
    "\n",
    "\n",
    "# Assuming you already have `all_saccade_collection` built as in your pipeline:\n",
    "model, events_with_states, feat_names, scaler = fit_hmm_on_saccade_collection(\n",
    "    all_saccade_collection,\n",
    "    n_states=3,\n",
    "    pair_threshold_ms=60.0\n",
    ")"
   ],
   "id": "d3802ebd32c24a39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Optional, Union, List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: only if you have sklearn; otherwise we'll fallback\n",
    "try:\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    _HAS_SK = True\n",
    "except Exception:\n",
    "    _HAS_SK = False\n",
    "\n",
    "\n",
    "def plot_hmm_saccade_summary(\n",
    "    events_with_states: pd.DataFrame,\n",
    "    time_col: str = \"time_ms_like\",\n",
    "    state_col: str = \"state\",\n",
    "    eye_col: Optional[str] = \"eye\",           # set to None if you don't have an eye column\n",
    "    # Confusion matrix (optional)\n",
    "    y_true: Optional[Union[np.ndarray, pd.Series]] = None,\n",
    "    y_pred: Optional[Union[np.ndarray, pd.Series]] = None,\n",
    "    true_labels: Optional[List[str]] = None,   # class order for CM rows\n",
    "    pred_labels: Optional[List[str]] = None,   # class order for CM cols\n",
    "    # Visual style\n",
    "    state_names: Optional[Dict[int, str]] = None,  # map numeric->name for legend\n",
    "    state_colors: Optional[Dict[int, str]] = None, # map state->color hex\n",
    "    figure_height: float = 8.0,\n",
    "    alpha_state_band: float = 0.15,\n",
    "    raster_tick_height: float = 0.8,\n",
    "):\n",
    "    \"\"\"\n",
    "    events_with_states must contain at least [time_col, state_col].\n",
    "    If eye_col is provided, it will create a 2-level raster (one row per eye label).\n",
    "    If y_true & y_pred are provided, the confusion matrix is plotted (top panel).\n",
    "\n",
    "    Typical usage:\n",
    "      plot_hmm_saccade_summary(events_df, time_col='event_time_ms', state_col='state', eye_col='eye',\n",
    "                               y_true=truth, y_pred=pred, state_names={0:'Fix',1:'Slow',2:'Fast'})\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- basic checks / prep ----\n",
    "    df = events_with_states.copy()\n",
    "    if time_col not in df.columns or state_col not in df.columns:\n",
    "        raise ValueError(f\"DataFrame must contain '{time_col}' and '{state_col}' columns.\")\n",
    "\n",
    "    # Ensure time is numeric\n",
    "    df[time_col] = pd.to_numeric(df[time_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[time_col, state_col]).sort_values(time_col)\n",
    "\n",
    "    # Normalize state to int codes (keeps mapping if already int)\n",
    "    if not np.issubdtype(df[state_col].dtype, np.integer):\n",
    "        df[\"_state_code\"], uniques = pd.factorize(df[state_col])\n",
    "        code_to_label = dict(enumerate(uniques.tolist()))\n",
    "    else:\n",
    "        df[\"_state_code\"] = df[state_col].astype(int)\n",
    "        code_to_label = {int(s): str(s) for s in sorted(df[\"_state_code\"].unique())}\n",
    "\n",
    "    # Apply provided state_names if given\n",
    "    if state_names is not None:\n",
    "        code_to_label = {int(k): state_names.get(int(k), code_to_label.get(int(k), str(k)))\n",
    "                         for k in code_to_label.keys()}\n",
    "\n",
    "    state_list = sorted(code_to_label.keys())\n",
    "\n",
    "    # Colors\n",
    "    if state_colors is None:\n",
    "        # Simple default palette\n",
    "        default_palette = [\"#4C78A8\", \"#F58518\", \"#54A24B\", \"#E45756\", \"#72B7B2\", \"#EECA3B\"]\n",
    "        state_colors = {s: default_palette[s % len(default_palette)] for s in state_list}\n",
    "    else:\n",
    "        # ensure all states have a color\n",
    "        for s in state_list:\n",
    "            state_colors.setdefault(s, \"#999999\")\n",
    "\n",
    "    # Build state segments (start/stop in time for each contiguous state)\n",
    "    t = df[time_col].to_numpy()\n",
    "    s = df[\"_state_code\"].to_numpy()\n",
    "\n",
    "    # breakpoints where state changes\n",
    "    change_idx = np.where(np.diff(s) != 0)[0] + 1\n",
    "    starts = np.r_[0, change_idx]\n",
    "    ends   = np.r_[change_idx, len(s)]\n",
    "\n",
    "    state_bands = [(t[si], t[ei-1] if ei-1 >= si else t[si], int(s[si])) for si, ei in zip(starts, ends)]\n",
    "    # If single sample segments, give minimal width\n",
    "    eps = (t[-1] - t[0]) * 1e-4 if len(t) > 1 else 1.0\n",
    "\n",
    "    # ---- figure layout ----\n",
    "    has_cm = (y_true is not None) and (y_pred is not None)\n",
    "    nrows = 3 if has_cm else 2\n",
    "    fig, axs = plt.subplots(nrows, 1, figsize=(12, figure_height), sharex=False,\n",
    "                            gridspec_kw={\"height_ratios\": [1.2, 1.0, 1.3] if has_cm else [1.0, 1.3]})\n",
    "    if nrows == 2:\n",
    "        ax_cm = None\n",
    "        ax_state, ax_raster = axs[0], axs[1]\n",
    "    else:\n",
    "        ax_cm, ax_state, ax_raster = axs[0], axs[1], axs[2]\n",
    "\n",
    "    # ---- (1) Confusion matrix (optional) ----\n",
    "    if has_cm:\n",
    "        y_true = np.asarray(y_true).ravel()\n",
    "        y_pred = np.asarray(y_pred).ravel()\n",
    "        if _HAS_SK:\n",
    "            # Labels / order\n",
    "            if true_labels is None:\n",
    "                true_labels = sorted(np.unique(y_true).tolist())\n",
    "            if pred_labels is None:\n",
    "                pred_labels = sorted(np.unique(y_pred).tolist())\n",
    "            cm = confusion_matrix(y_true, y_pred, labels=pred_labels)\n",
    "            im = ax_cm.imshow(cm, aspect=\"auto\", cmap=\"Blues\")\n",
    "            ax_cm.set_title(\"Confusion Matrix\")\n",
    "            ax_cm.set_xlabel(\"Predicted\")\n",
    "            ax_cm.set_ylabel(\"True\")\n",
    "            ax_cm.set_xticks(range(len(pred_labels)))\n",
    "            ax_cm.set_xticklabels([state_names.get(l, str(l)) if state_names else str(l) for l in pred_labels],\n",
    "                                  rotation=45, ha=\"right\")\n",
    "            ax_cm.set_yticks(range(len(true_labels)))\n",
    "            ax_cm.set_yticklabels([state_names.get(l, str(l)) if state_names else str(l) for l in true_labels])\n",
    "            # annotate cells\n",
    "            for i in range(cm.shape[0]):\n",
    "                for j in range(cm.shape[1]):\n",
    "                    ax_cm.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", fontsize=9)\n",
    "            fig.colorbar(im, ax=ax_cm, fraction=0.046, pad=0.04)\n",
    "        else:\n",
    "            ax_cm.text(0.5, 0.5, \"Install scikit‑learn for confusion matrix\\npip install scikit-learn\",\n",
    "                       ha=\"center\", va=\"center\", fontsize=11)\n",
    "            ax_cm.axis(\"off\")\n",
    "\n",
    "    # ---- (2) State sequence over time (colored spans) ----\n",
    "    ax_state.set_title(\"State sequence over time\")\n",
    "    ax_state.set_ylabel(\"State\")\n",
    "    # colored blocks\n",
    "    for t0, t1, st in state_bands:\n",
    "        if t1 <= t0:\n",
    "            t1 = t0 + eps\n",
    "        ax_state.axvspan(t0, t1, color=state_colors[st], alpha=alpha_state_band, lw=0)\n",
    "    # a thin line of state code vs time (to see edges)\n",
    "    ax_state.step(t, s, where=\"post\", color=\"#333333\", lw=1)\n",
    "    # nice y ticks with labels\n",
    "    ax_state.set_yticks(state_list)\n",
    "    ax_state.set_yticklabels([code_to_label[k] for k in state_list])\n",
    "\n",
    "    # ---- (3) Saccade raster with state overlay ----\n",
    "    ax_raster.set_title(\"Saccade raster (per eye) with state overlay\")\n",
    "    ax_raster.set_xlabel(\"Time (ms)\")\n",
    "    if eye_col is not None and eye_col in df.columns:\n",
    "        eye_levels = df[eye_col].astype(str).unique().tolist()\n",
    "        eye_levels.sort()\n",
    "        eye_to_row = {e: i for i, e in enumerate(eye_levels)}\n",
    "        # overlay state bands again for context\n",
    "        for t0, t1, st in state_bands:\n",
    "            if t1 <= t0:\n",
    "                t1 = t0 + eps\n",
    "            ax_raster.axvspan(t0, t1, color=state_colors[st], alpha=alpha_state_band, lw=0)\n",
    "\n",
    "        # draw ticks\n",
    "        for e in eye_levels:\n",
    "            row = eye_to_row[e]\n",
    "            times = df.loc[df[eye_col].astype(str) == e, time_col].to_numpy()\n",
    "            if times.size:\n",
    "                ax_raster.vlines(times, row - raster_tick_height/2, row + raster_tick_height/2, lw=1)\n",
    "        ax_raster.set_yticks(list(eye_to_row.values()))\n",
    "        ax_raster.set_yticklabels(eye_levels)\n",
    "    else:\n",
    "        # single-row raster if no eye column\n",
    "        times = df[time_col].to_numpy()\n",
    "        for t_i in times:\n",
    "            ax_raster.vlines(t_i, 0.5 - raster_tick_height/2, 0.5 + raster_tick_height/2, lw=1)\n",
    "        # overlay bands\n",
    "        for t0, t1, st in state_bands:\n",
    "            if t1 <= t0:\n",
    "                t1 = t0 + eps\n",
    "            ax_raster.axvspan(t0, t1, color=state_colors[st], alpha=alpha_state_band, lw=0)\n",
    "        ax_raster.set_yticks([0.5])\n",
    "        ax_raster.set_yticklabels([\"events\"])\n",
    "\n",
    "    # Legend for states\n",
    "    handles = [plt.Line2D([0], [0], color=state_colors[s], lw=6, label=code_to_label[s]) for s in state_list]\n",
    "    ax_state.legend(handles=handles, loc=\"upper right\", frameon=False, ncol=min(3, len(handles)))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, (ax_cm, ax_state, ax_raster) if has_cm else (None, ax_state, ax_raster)\n"
   ],
   "id": "5e931c862c3dc9e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def print_and_plot_hmm_transitions(model, state_labels=None, cmap=\"Blues\"):\n",
    "    \"\"\"\n",
    "    Print and plot the HMM state transition probabilities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : fitted hmmlearn model\n",
    "        Must have .transmat_ attribute.\n",
    "    state_labels : list of str or None\n",
    "        Labels to use for states (defaults to \"S0\", \"S1\", ...).\n",
    "    cmap : str\n",
    "        Matplotlib colormap for heatmap.\n",
    "    \"\"\"\n",
    "    n_states = model.transmat_.shape[0]\n",
    "    if state_labels is None:\n",
    "        state_labels = [f\"S{i}\" for i in range(n_states)]\n",
    "\n",
    "    # Put into DataFrame\n",
    "    trans_df = pd.DataFrame(model.transmat_,\n",
    "                            index=[f\"from {s}\" for s in state_labels],\n",
    "                            columns=[f\"to {s}\" for s in state_labels])\n",
    "\n",
    "    print(\"Transition probability matrix:\")\n",
    "    display(trans_df.round(3))\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(3 + n_states * 0.4, 2.5 + n_states * 0.4), dpi=150)\n",
    "    sns.heatmap(trans_df, annot=True, fmt=\".2f\", cmap=cmap, cbar=True,\n",
    "                linewidths=0.5, linecolor=\"gray\")\n",
    "    plt.title(\"HMM Transition Probabilities\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return trans_df\n",
    "\n",
    "\n",
    "# Example usage after fitting your HMM:\n",
    "transitions = print_and_plot_hmm_transitions(model)\n"
   ],
   "id": "6136ce99de181687",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "summary = events_with_states.groupby(\"state\").agg({\n",
    "    \"ISI\": [\"median\", \"mean\"],\n",
    "    \"magnitude\": [\"median\", \"mean\"],\n",
    "    \"head_move\": \"mean\",\n",
    "    \"synced\": \"mean\",\n",
    "    \"time_ms_like\": \"count\"\n",
    "})\n",
    "summary\n"
   ],
   "id": "6d38598cced6af0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# events_with_states must have \"event_time_ms\" and \"state\"\n",
    "# Minimal (no confusion matrix):\n",
    "fig, axes = plot_hmm_saccade_summary(\n",
    "    events_with_states,               # your DF\n",
    "    time_col=\"time_ms_like\",         # or whatever you use\n",
    "    state_col=\"state\",\n",
    "    eye_col=\"eye\",                    # or None if not present\n",
    "    state_names={0:\"fix\", 1:\"slow\", 2:\"fast\"}  # optional\n",
    ")\n"
   ],
   "id": "abcc80e89662781b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "events_with_states",
   "id": "4296730c7a5bf2cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This is for 2 deg/frame (~120 deg/sec speed thr)\n",
    "export_path = r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_4\\material\\Inter_saccade_intervals_histogram'\n",
    "out = export_inter_saccade_intervals_density_traces_from_blocks(\n",
    "    block_collection,\n",
    "    num_bins=30,\n",
    "    figure_size=(2.5, 1.7),\n",
    "    export_path=None,\n",
    "    plot_name='ISI_histogram.pdf')"
   ],
   "id": "632d6c544a4c5e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This is for 1 deg/frame (~60 deg/sec speed thr)\n",
    "export_path = r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_4\\material\\Inter_saccade_intervals_histogram'\n",
    "out = export_inter_saccade_intervals_density_traces_from_blocks(\n",
    "    block_collection,\n",
    "    num_bins=20,\n",
    "    figure_size=(2.5, 1.7),\n",
    "    export_path=None,\n",
    "    plot_name='ISI_histogram.pdf')"
   ],
   "id": "19cf6d097184b2eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === ISI traces (log-x) with x-only minor spikes, realizable bins, and a left-edge zero anchor ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogLocator, NullFormatter, NullLocator\n",
    "from pathlib import Path\n",
    "import datetime as _dt\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _collect_events_from_blocks(block_collection):\n",
    "    rows = []\n",
    "    for b in block_collection:\n",
    "        animal = getattr(b, 'animal_call', None)\n",
    "        blockn = getattr(b, 'block_num', None)\n",
    "        for eye, sdf in (('L', getattr(b, 'l_saccade_df', None)),\n",
    "                         ('R', getattr(b, 'r_saccade_df', None))):\n",
    "            if sdf is None or len(sdf) == 0:\n",
    "                continue\n",
    "            s_on = pd.to_numeric(sdf.get('saccade_on_ms', pd.Series([], dtype='float64')), errors='coerce')\n",
    "            hm = sdf['head_movement'] if 'head_movement' in sdf.columns else pd.Series(False, index=sdf.index)\n",
    "            rows.append(pd.DataFrame({\n",
    "                'animal': animal, 'block': blockn, 'eye': eye,\n",
    "                'saccade_on_ms': s_on, 'head_movement': hm.astype(bool)\n",
    "            }).dropna(subset=['saccade_on_ms']))\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(\n",
    "        columns=['animal','block','eye','saccade_on_ms','head_movement'])\n",
    "\n",
    "def _estimate_frame_interval_ms(block_collection, fallback_ms=16.67):\n",
    "    dts = []\n",
    "    for b in block_collection:\n",
    "        for df in [getattr(b, 'left_eye_data', None), getattr(b, 'right_eye_data', None)]:\n",
    "            if df is None or 'ms_axis' not in df.columns: continue\n",
    "            ms = pd.to_numeric(df['ms_axis'], errors='coerce').dropna().to_numpy()\n",
    "            if ms.size > 3:\n",
    "                dt = np.diff(ms); dt = dt[(dt > 0) & np.isfinite(dt)]\n",
    "                if dt.size: dts.append(np.median(dt))\n",
    "    return float(np.median(dts)) if dts else float(fallback_ms)\n",
    "\n",
    "def _dedupe_lr_pairs(events_df, pair_threshold_ms=60.0):\n",
    "    \"\"\"Greedy L/R pairing per (animal,block); unmatched events pass through.\"\"\"\n",
    "    out = []\n",
    "    for (animal, block), g in events_df.groupby(['animal','block'], dropna=False):\n",
    "        L = g[g['eye']=='L'].sort_values('saccade_on_ms')\n",
    "        R = g[g['eye']=='R'].sort_values('saccade_on_ms')\n",
    "        lt, rt = L['saccade_on_ms'].to_numpy(float), R['saccade_on_ms'].to_numpy(float)\n",
    "        lhm = L['head_movement'].to_numpy(bool) if len(L) else np.array([], bool)\n",
    "        rhm = R['head_movement'].to_numpy(bool) if len(R) else np.array([], bool)\n",
    "        i=j=0; ev_t=[]; ev_hm=[]\n",
    "        while i<len(lt) and j<len(rt):\n",
    "            dt = lt[i]-rt[j]\n",
    "            if abs(dt) <= pair_threshold_ms:\n",
    "                ev_t.append(min(lt[i], rt[j])); ev_hm.append(bool(lhm[i] or rhm[j])); i+=1; j+=1\n",
    "            elif lt[i] < rt[j] - pair_threshold_ms:\n",
    "                ev_t.append(lt[i]); ev_hm.append(bool(lhm[i])); i+=1\n",
    "            else:\n",
    "                ev_t.append(rt[j]); ev_hm.append(bool(rhm[j])); j+=1\n",
    "        while i<len(lt): ev_t.append(lt[i]); ev_hm.append(bool(lhm[i])); i+=1\n",
    "        while j<len(rt): ev_t.append(rt[j]); ev_hm.append(bool(rhm[j])); j+=1\n",
    "        out.append(pd.DataFrame({'animal':animal,'block':block,'event_time_ms':ev_t,'head_movement':ev_hm}))\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame(\n",
    "        columns=['animal','block','event_time_ms','head_movement'])\n",
    "\n",
    "def _compute_blockwise_isis(dedup_df, min_isi_ms):\n",
    "    isi_all = {}\n",
    "    for (animal, block), g in dedup_df.groupby(['animal','block'], dropna=False):\n",
    "        t = np.sort(g['event_time_ms'].to_numpy())\n",
    "        if t.size > 1:\n",
    "            d = np.diff(t); d = d[np.isfinite(d) & (d >= min_isi_ms)]\n",
    "            if d.size: isi_all.setdefault(animal, []).append(d)\n",
    "    return {a: (np.concatenate(v) if v else np.array([], float)) for a, v in isi_all.items()}\n",
    "\n",
    "def _snapped_log_bins(num_bins, frame_ms, low_ms, high_ms, add_leading_zero_bin=True):\n",
    "    \"\"\"\n",
    "    Log edges snapped to (k+0.5)*Δt so each bin spans ≥1 feasible ISI (k*Δt).\n",
    "    Optional leading dummy bin [0.25Δt, 0.5Δt] (zero by construction).\n",
    "    \"\"\"\n",
    "    raw = np.geomspace(low_ms, high_ms, int(num_bins)+1)\n",
    "    k_edges = np.floor(raw/frame_ms) + 0.5\n",
    "    kmax = int(np.floor(high_ms/frame_ms))\n",
    "    k_edges = np.clip(k_edges, 0.5, kmax+0.5)\n",
    "    k_edges = np.unique(k_edges)\n",
    "    if add_leading_zero_bin and (k_edges.size == 0 or not np.isclose(k_edges[0], 0.5)):\n",
    "        k_edges = np.r_[0.25, k_edges]  # dummy bin (center not used)\n",
    "    if k_edges[-1] < (kmax+0.5):\n",
    "        k_edges = np.r_[k_edges, (kmax+0.5)]\n",
    "    return k_edges * frame_ms\n",
    "\n",
    "# ---------- plotting ----------\n",
    "def export_inter_saccade_intervals_density_traces_from_blocks(\n",
    "    block_collection,\n",
    "    figure_size=(1.8, 1.2),\n",
    "    export_path=None,\n",
    "    plot_name='ISI_histogram.pdf',\n",
    "    num_bins=25,\n",
    "    high_ms=20000.0,\n",
    "    pair_threshold_ms=60.0,\n",
    "    min_isi_ms=None,          # if None -> 0.75 * frame_ms\n",
    "    first_bin_factor=0.5,     # first “real” edge = 0.5×Δt\n",
    "    add_leading_zero_bin=True,\n",
    "    font_family='Arial'\n",
    "):\n",
    "    # Build and dedupe\n",
    "    events = _collect_events_from_blocks(block_collection)\n",
    "    if events.empty:\n",
    "        raise ValueError(\"No saccade events found.\")\n",
    "    dedup = _dedupe_lr_pairs(events, pair_threshold_ms=pair_threshold_ms)\n",
    "\n",
    "    # Frame interval + ISIs\n",
    "    frame_ms = _estimate_frame_interval_ms(block_collection)\n",
    "    min_isi_eff = (0.75 * frame_ms) if (min_isi_ms is None) else float(min_isi_ms)\n",
    "    isi_all = _compute_blockwise_isis(dedup, min_isi_ms=min_isi_eff)\n",
    "    animals = sorted(dedup['animal'].dropna().unique())\n",
    "\n",
    "    # Bins\n",
    "    low_ms = max(1e-6, first_bin_factor * frame_ms)\n",
    "    bins = _snapped_log_bins(num_bins=num_bins, frame_ms=frame_ms,\n",
    "                             low_ms=low_ms, high_ms=float(high_ms),\n",
    "                             add_leading_zero_bin=add_leading_zero_bin)\n",
    "    bin_centers = np.sqrt(bins[:-1] * bins[1:])\n",
    "\n",
    "    # Style\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = [font_family]\n",
    "    OI = ['#0072B2','#D55E00','#009E73','#CC79A7','#F0E442','#56B4E9','#E69F00','#000000']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figure_size, dpi=300)\n",
    "    legend_handles, legend_labels = [], []\n",
    "\n",
    "    for i, a in enumerate(animals):\n",
    "        isi = isi_all.get(a, np.array([]))\n",
    "        if isi.size == 0:\n",
    "            continue\n",
    "        hist, _ = np.histogram(isi, bins=bins)\n",
    "        dens = hist / hist.sum() if hist.sum() > 0 else hist\n",
    "\n",
    "        # draw a left-edge zero, then jump to first *real* bin center\n",
    "        if add_leading_zero_bin:\n",
    "            x_plot = np.r_[bins[0], bin_centers[1:]]\n",
    "            y_plot = np.r_[0.0, dens[1:]]\n",
    "        else:\n",
    "            x_plot = bin_centers\n",
    "            y_plot = dens\n",
    "\n",
    "        h, = ax.plot(x_plot, y_plot, linewidth=1.2, color=OI[i % len(OI)], label=str(a))\n",
    "        legend_handles.append(h); legend_labels.append(str(a))\n",
    "\n",
    "    # Axes + ticks: x-only minor “spikes”\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlim(bins[0], bins[-1])\n",
    "    ax.set_xlabel('ISI [ms]', fontsize=10)\n",
    "    ax.set_ylabel('Probability', fontsize=10)\n",
    "\n",
    "    # Major ticks\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8, length=5, width=1, direction='out')\n",
    "    # X minor ticks (spikes), Y minor ticks OFF\n",
    "    ax.xaxis.set_minor_locator(LogLocator(base=10.0, subs=np.arange(2,10)*0.1))\n",
    "    ax.xaxis.set_minor_formatter(NullFormatter())\n",
    "    ax.tick_params(axis='x', which='minor', length=3, width=0.8, direction='out', bottom=True, top=False)\n",
    "    ax.yaxis.set_minor_locator(NullLocator())  # ensure no y-minor spikes\n",
    "\n",
    "    ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out = {'dedup_events': dedup, 'isi_all': isi_all, 'frame_ms': frame_ms, 'bins': bins}\n",
    "\n",
    "    # Export: main plot (no legend) + legend as separate figure (so size is unaffected)\n",
    "    if export_path is not None:\n",
    "        ts = _dt.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        outdir = Path(export_path) / f\"inter_saccade_export_{ts}\"\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        fig.savefig(outdir / plot_name, format='pdf', bbox_inches='tight', dpi=300)\n",
    "\n",
    "        if legend_handles:\n",
    "            fig_leg = plt.figure(figsize=(2.2, 0.28*max(1,len(legend_labels)) + 0.4), dpi=300)\n",
    "            fig_leg.legend(legend_handles, legend_labels, loc='center', frameon=False, ncol=1, prop={'size': 8})\n",
    "            fig_leg.savefig(outdir / ('legend_' + plot_name), format='pdf', bbox_inches='tight', dpi=300)\n",
    "            plt.close(fig_leg)\n",
    "\n",
    "        with open(outdir / \"binning_info.txt\", 'w') as f:\n",
    "            f.write(f\"Frame interval (ms): {frame_ms:.3f}\\n\")\n",
    "            f.write(f\"First edge (ms): {bins[0]:.3f}\\n\")\n",
    "            f.write(f\"Min ISI enforced (ms): {min_isi_eff:.3f}\\n\")\n",
    "            f.write(f\"Pair L/R threshold (ms): {pair_threshold_ms:.3f}\\n\")\n",
    "            f.write(f\"Leading zero anchor: {add_leading_zero_bin}\\n\")\n",
    "        print(\"Exported plot + separate legend to:\", outdir)\n",
    "\n",
    "    plt.show()\n",
    "    return out\n"
   ],
   "id": "1984f3083aa13c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "export_path",
   "id": "23d748527b64cda7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "rcParams['pdf.fonttype'] = 42  # Ensure fonts are embedded and editable\n",
    "rcParams['ps.fonttype'] = 42  # Ensure compatibility with vector outputs\n",
    "\n",
    "export_path = r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_4\\material\\Inter_saccade_intervals_histogram'\n",
    "out = export_inter_saccade_intervals_density_traces_from_blocks(\n",
    "    block_collection,\n",
    "    figure_size=(2.5, 1.7),\n",
    "    export_path=None,\n",
    "    plot_name='ISI_histogram.pdf'\n",
    ")"
   ],
   "id": "b650c0c29dc900f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_most_common_isi(block_collection,\n",
    "                            num_bins=50,\n",
    "                            high_ms=20000.0,\n",
    "                            pair_threshold_ms=60.0,\n",
    "                            min_isi_ms=None,\n",
    "                            first_bin_factor=0.5):\n",
    "    \"\"\"\n",
    "    Compute the most common inter-saccadic interval (ISI) pooled across all animals.\n",
    "\n",
    "    Args:\n",
    "        block_collection: list of BlockSync objects\n",
    "        num_bins (int): number of log-spaced bins\n",
    "        high_ms (float): upper edge for ISIs [ms]\n",
    "        pair_threshold_ms (float): threshold for L/R pairing\n",
    "        min_isi_ms (float or None): minimum ISI to keep; if None use 0.75*frame_ms\n",
    "        first_bin_factor (float): lowest edge = factor*frame_ms\n",
    "\n",
    "    Returns:\n",
    "        most_common_isi (float): ISI bin center with maximum probability\n",
    "        hist (np.ndarray): normalized pooled histogram\n",
    "        bin_centers (np.ndarray): corresponding bin centers [ms]\n",
    "    \"\"\"\n",
    "    # Collect + dedupe events\n",
    "    events = _collect_events_from_blocks(block_collection)\n",
    "    if events.empty:\n",
    "        raise ValueError(\"No saccade events found.\")\n",
    "    dedup = _dedupe_lr_pairs(events, pair_threshold_ms=pair_threshold_ms)\n",
    "\n",
    "    # Estimate frame interval and ISIs\n",
    "    frame_ms = _estimate_frame_interval_ms(block_collection)\n",
    "    min_isi_eff = (0.75 * frame_ms) if (min_isi_ms is None) else float(min_isi_ms)\n",
    "    isi_all = _compute_blockwise_isis(dedup, min_isi_ms=min_isi_eff)\n",
    "\n",
    "    # Pool across animals\n",
    "    pooled_isi = np.concatenate(list(isi_all.values())) if isi_all else np.array([], float)\n",
    "    if pooled_isi.size == 0:\n",
    "        raise ValueError(\"No valid ISIs after filtering.\")\n",
    "\n",
    "    # Define log bins (same as plotting function)\n",
    "    low_ms = max(1e-6, first_bin_factor * frame_ms)\n",
    "    bins = _snapped_log_bins(num_bins=num_bins, frame_ms=frame_ms,\n",
    "                             low_ms=low_ms, high_ms=float(high_ms),\n",
    "                             add_leading_zero_bin=False)\n",
    "    bin_centers = np.sqrt(bins[:-1] * bins[1:])\n",
    "\n",
    "    # Histogram + normalize\n",
    "    hist, _ = np.histogram(pooled_isi, bins=bins)\n",
    "    dens = hist / hist.sum() if hist.sum() > 0 else hist\n",
    "\n",
    "    # Find most common ISI\n",
    "    idx = np.argmax(dens)\n",
    "    most_common_isi = bin_centers[idx]\n",
    "\n",
    "    return most_common_isi, dens, bin_centers\n"
   ],
   "id": "afbee58686fdb504",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def plot_block_saccades_plotly(block_collection,\n",
    "                               block,\n",
    "                               pair_threshold_ms=60.0,\n",
    "                               short_isi_ms=60.0,\n",
    "                               smooth_win=5,\n",
    "                               font_family=\"Arial\",\n",
    "                               height=520,\n",
    "                               export_html=None):\n",
    "    \"\"\"\n",
    "    Interactive Plotly figure: right-eye speed + saccade event markers (L, R, LR) with\n",
    "    short-ISI highlights. Supports zoom/pan/hover and legend toggling.\n",
    "\n",
    "    Args:\n",
    "        block_collection: list of BlockSync objects (used only if we need frame stats)\n",
    "        block:           the BlockSync object to visualize\n",
    "        pair_threshold_ms (float): merge L/R saccades if onset difference <= this (binocular pair)\n",
    "        short_isi_ms (float):      highlight ISIs shorter than this\n",
    "        smooth_win (int):          moving-average window for speed (set 1/None to disable)\n",
    "        font_family (str):         UI font\n",
    "        height (int):              figure height in pixels\n",
    "        export_html (str|Path|None): if set, save a self-contained HTML\n",
    "\n",
    "    Returns:\n",
    "        fig: plotly.graph_objects.Figure\n",
    "        out: dict with dedup events, short mask, time and speed arrays\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def _find_series(df, candidates):\n",
    "        for c in candidates:\n",
    "            if c in df.columns:\n",
    "                s = pd.to_numeric(df[c], errors='coerce')\n",
    "                if s.notna().any():\n",
    "                    return s\n",
    "        return None\n",
    "\n",
    "    def _collect_events_from_block(_b):\n",
    "        rows = []\n",
    "        for eye, sdf in (('L', getattr(_b, 'l_saccade_df', None)),\n",
    "                         ('R', getattr(_b, 'r_saccade_df', None))):\n",
    "            if sdf is None or len(sdf) == 0:\n",
    "                continue\n",
    "            s_on = pd.to_numeric(sdf.get('saccade_on_ms', pd.Series([], dtype='float64')), errors='coerce')\n",
    "            s_off = pd.to_numeric(sdf.get('saccade_off_ms', pd.Series([], dtype='float64')), errors='coerce')\n",
    "            hm = sdf['head_movement'] if 'head_movement' in sdf.columns else pd.Series(False, index=sdf.index)\n",
    "            df = pd.DataFrame({'eye': eye, 'on_ms': s_on, 'off_ms': s_off, 'head_movement': hm.astype(bool)})\n",
    "            rows.append(df.dropna(subset=['on_ms']))\n",
    "        return (pd.concat(rows, ignore_index=True)\n",
    "                if rows else pd.DataFrame(columns=['eye','on_ms','off_ms','head_movement']))\n",
    "\n",
    "    def _greedy_dedup_lr(events_df, thr_ms=60.0):\n",
    "        if events_df.empty:\n",
    "            return events_df.assign(merged='none')\n",
    "        L = events_df[events_df.eye=='L'].sort_values('on_ms')\n",
    "        R = events_df[events_df.eye=='R'].sort_values('on_ms')\n",
    "        lt, rt = L['on_ms'].to_numpy(float), R['on_ms'].to_numpy(float)\n",
    "        lhm = L['head_movement'].to_numpy(bool) if len(L) else np.array([], bool)\n",
    "        rhm = R['head_movement'].to_numpy(bool) if len(R) else np.array([], bool)\n",
    "        i = j = 0; out = []\n",
    "        while i<len(lt) and j<len(rt):\n",
    "            dt = lt[i]-rt[j]\n",
    "            if abs(dt) <= thr_ms:\n",
    "                out.append({'eye':'LR','on_ms':min(lt[i],rt[j]), 'head_movement':bool(lhm[i] or rhm[j]), 'merged':'pair'})\n",
    "                i+=1; j+=1\n",
    "            elif lt[i] < rt[j] - thr_ms:\n",
    "                out.append({'eye':'L','on_ms':lt[i], 'head_movement':bool(lhm[i]), 'merged':'single'}); i+=1\n",
    "            else:\n",
    "                out.append({'eye':'R','on_ms':rt[j], 'head_movement':bool(rhm[j]), 'merged':'single'}); j+=1\n",
    "        while i<len(lt): out.append({'eye':'L','on_ms':lt[i], 'head_movement':bool(lhm[i]), 'merged':'single'}); i+=1\n",
    "        while j<len(rt): out.append({'eye':'R','on_ms':rt[j], 'head_movement':bool(rhm[j]), 'merged':'single'}); j+=1\n",
    "        return pd.DataFrame(out).sort_values('on_ms').reset_index(drop=True)\n",
    "\n",
    "    def _compute_speed_fallback(df):\n",
    "        t = pd.to_numeric(df.get('ms_axis', pd.Series([], dtype='float64')), errors='coerce').to_numpy()\n",
    "        if t.size < 3: return None, None\n",
    "        dt = np.diff(t); dt[~np.isfinite(dt) | (dt<=0)] = np.nan\n",
    "\n",
    "        # Try angular speed from phi\n",
    "        if 'phi' in df.columns:\n",
    "            phi = pd.to_numeric(df['phi'], errors='coerce').to_numpy()\n",
    "            # unwrap if reasonable:\n",
    "            try:\n",
    "                phi_unw = np.rad2deg(np.unwrap(np.deg2rad(phi)))\n",
    "                dphi = np.diff(phi_unw)\n",
    "            except Exception:\n",
    "                dphi = np.diff(phi)\n",
    "            ang_speed = np.abs(dphi) / dt   # deg/ms\n",
    "            return t[1:], ang_speed\n",
    "\n",
    "        # Pixel speed fallback\n",
    "        if {'center_x','center_y'}.issubset(df.columns):\n",
    "            xpx = pd.to_numeric(df['center_x'], errors='coerce').to_numpy()\n",
    "            ypx = pd.to_numeric(df['center_y'], errors='coerce').to_numpy()\n",
    "            dx, dy = np.diff(xpx), np.diff(ypx)\n",
    "            pix_speed = np.sqrt(dx*dx + dy*dy) / dt  # px/ms\n",
    "            return t[1:], pix_speed\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    # ---------- speed series ----------\n",
    "    re_df = getattr(block, 'right_eye_data', None)\n",
    "    if re_df is None or len(re_df) == 0:\n",
    "        raise ValueError(\"block.right_eye_data is missing or empty.\")\n",
    "\n",
    "    t_ms = pd.to_numeric(re_df.get('ms_axis', pd.Series([], dtype='float64')), errors='coerce')\n",
    "    speer = _find_series(re_df, ['speer_r','speed_r','vel_r','angular_speed','ang_speed','omega'])\n",
    "    if speer is not None and t_ms.notna().any():\n",
    "        x = t_ms.to_numpy()\n",
    "        y = pd.to_numeric(speer, errors='coerce').to_numpy()\n",
    "    else:\n",
    "        x, y = _compute_speed_fallback(re_df)\n",
    "        if x is None:\n",
    "            raise ValueError(\"Could not locate or compute a right-eye speed vector.\")\n",
    "\n",
    "    # Smooth (simple moving average) for readability\n",
    "    if smooth_win and smooth_win > 1:\n",
    "        from numpy.lib.stride_tricks import sliding_window_view\n",
    "        pad = (smooth_win//2, smooth_win-1 - smooth_win//2)\n",
    "        y_pad = np.pad(y, pad, mode='edge')\n",
    "        y = np.nanmean(sliding_window_view(y_pad, smooth_win), axis=-1)\n",
    "\n",
    "    y_min = np.nanpercentile(y[np.isfinite(y)], 1) if np.isfinite(y).any() else 0.0\n",
    "    y_max = np.nanpercentile(y[np.isfinite(y)], 99) if np.isfinite(y).any() else 1.0\n",
    "    y_rng = (y_max - y_min) or 1.0\n",
    "    vline_y0, vline_y1 = y_min - 0.05*y_rng, y_max + 0.05*y_rng\n",
    "\n",
    "    # ---------- events & short ISIs ----------\n",
    "    ev = _collect_events_from_block(block)\n",
    "    dedup = _greedy_dedup_lr(ev, thr_ms=pair_threshold_ms)\n",
    "    et = dedup['on_ms'].to_numpy(float)\n",
    "    short_mask = np.zeros_like(et, dtype=bool)\n",
    "    if et.size > 1:\n",
    "        isi = np.diff(et)\n",
    "        short_idx = np.where(isi < float(short_isi_ms))[0]\n",
    "        short_mask[short_idx] = True; short_mask[short_idx+1] = True\n",
    "\n",
    "    # ---------- build figure ----------\n",
    "    fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "    # speed trace\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x, y=y, mode='lines', name='Right-eye speed',\n",
    "        line=dict(width=1),\n",
    "        hovertemplate=\"t=%{x:.1f} ms<br>speed=%{y:.3g}<extra></extra>\"\n",
    "    ))\n",
    "\n",
    "    # saccade vlines per group\n",
    "    color_map = {'LR':'#000000', 'L':'#0072B2', 'R':'#D55E00'}\n",
    "    alpha_map = {'LR':0.6, 'L':0.35, 'R':0.35}\n",
    "    for eye in ['LR','L','R']:\n",
    "        sel = dedup['eye'] == eye\n",
    "        if sel.any():\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=dedup.loc[sel, 'on_ms'],\n",
    "                y=np.full(sel.sum(), vline_y1),\n",
    "                mode='markers',\n",
    "                name=f'{eye} saccade',\n",
    "                marker=dict(color=color_map[eye], size=6, symbol='line-ns', line=dict(width=2)),\n",
    "                hovertemplate=f\"{eye} saccade<br>t=%{{x:.1f}} ms<extra></extra>\",\n",
    "                opacity=alpha_map[eye]\n",
    "            ))\n",
    "\n",
    "    # short-ISI highlights (overplot)\n",
    "    if short_mask.any():\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=et[short_mask],\n",
    "            y=np.full(short_mask.sum(), vline_y1),\n",
    "            mode='markers',\n",
    "            name=f'ISI < {short_isi_ms:.0f} ms',\n",
    "            marker=dict(color='#CC79A7', size=8, symbol='line-ns', line=dict(width=3)),\n",
    "            hovertemplate=f\"suspect ISI<br>t=%{{x:.1f}} ms<extra></extra>\",\n",
    "            opacity=0.9\n",
    "        ))\n",
    "\n",
    "    # layout\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        height=height,\n",
    "        dragmode='zoom',\n",
    "        hovermode='x',\n",
    "        legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1.0),\n",
    "        font=dict(family=font_family, size=12),\n",
    "        margin=dict(l=60, r=20, t=20, b=60)\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Time [ms]\", showline=True, linewidth=1, mirror=False)\n",
    "    fig.update_yaxes(title_text=\"Speed (a.u.)\", showline=True, linewidth=1, range=[vline_y0, vline_y1])\n",
    "\n",
    "    if export_html is not None:\n",
    "        import plotly.io as pio\n",
    "        pio.write_html(fig, file=str(export_html), auto_open=False, include_plotlyjs='cdn')\n",
    "\n",
    "    out = {'dedup_events': dedup, 'short_mask': short_mask, 'speed_t': x, 'speed_y': y}\n",
    "    return fig, out\n"
   ],
   "id": "411dcd41682f54c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, out = plot_block_saccades_plotly(block_collection, block,\n",
    "                                      pair_threshold_ms=60.0,\n",
    "                                      short_isi_ms=60.0,\n",
    "                                      smooth_win=5,\n",
    "                                      export_html=\"saccades_vs_speed.html\")\n",
    "fig.show()"
   ],
   "id": "b9da6a3e500fb63c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def calculate_orientation_tuning(saccade_angles):\n",
    "    \"\"\"\n",
    "    Calculates the orientation tuning statistic for a collection of saccade angles.\n",
    "\n",
    "    Parameters:\n",
    "        saccade_angles (array-like): List or array of saccade angles in degrees.\n",
    "\n",
    "    Returns:\n",
    "        float: Orientation tuning statistic.\n",
    "    \"\"\"\n",
    "    # Convert angles to range [0, 360] if they aren't already\n",
    "    saccade_angles = np.array(saccade_angles) % 360\n",
    "\n",
    "    # Define horizontal saccade ranges\n",
    "    horizontal_ranges = [\n",
    "        (315, 360),  # 330° to 0°\n",
    "        (0, 45),     # 0° to 30°\n",
    "        (135, 225)   # 150° to 210°\n",
    "    ]\n",
    "\n",
    "    # Identify horizontal saccades\n",
    "    is_horizontal = np.logical_or.reduce([\n",
    "        (saccade_angles >= low) & (saccade_angles <= high)\n",
    "        if low < high else\n",
    "        (saccade_angles >= low) | (saccade_angles <= high)\n",
    "        for low, high in horizontal_ranges\n",
    "    ])\n",
    "\n",
    "    # Calculate probabilities\n",
    "    p_horizontal = np.sum(is_horizontal) / len(saccade_angles)\n",
    "    p_vertical = 1 - p_horizontal  # Vertical saccades are the complement\n",
    "\n",
    "    # Calculate orientation tuning\n",
    "    tuning_statistic = (p_horizontal - p_vertical) / (p_horizontal + p_vertical)\n",
    "\n",
    "    return tuning_statistic\n",
    "\n",
    "def analyze_orientation_tuning(synced_df, non_synced_df, export_path=None):\n",
    "    \"\"\"\n",
    "    Analyze orientation tuning for each unique animal in the given dataframes.\n",
    "\n",
    "    Parameters:\n",
    "        synced_df (pd.DataFrame): DataFrame containing 'animal' and 'angle' columns for synchronized saccades.\n",
    "        non_synced_df (pd.DataFrame): DataFrame containing 'animal' and 'angle' columns for non-synchronized saccades.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with animal IDs as keys and a tuple of (synced_tuning, non_synced_tuning) as values.\n",
    "    \"\"\"\n",
    "    # Initialize dictionary to store results\n",
    "    tuning_results = {}\n",
    "\n",
    "    # Get unique animals\n",
    "    unique_animals = synced_df['animal'].unique()\n",
    "\n",
    "    for animal in unique_animals:\n",
    "        # Filter data for the current animal\n",
    "        synced_angles = synced_df.query('animal == @animal')['overall_angle_deg'].values\n",
    "        non_synced_angles = non_synced_df.query('animal == @animal')['overall_angle_deg'].values\n",
    "\n",
    "        # Calculate orientation tuning\n",
    "        synced_tuning = calculate_orientation_tuning(synced_angles)\n",
    "        non_synced_tuning = calculate_orientation_tuning(non_synced_angles)\n",
    "\n",
    "        # Save to dictionary\n",
    "        tuning_results[animal] = (synced_tuning, non_synced_tuning)\n",
    "\n",
    "    # Plot results\n",
    "    fig, ax = plt.subplots(figsize=(5, 2), dpi=150)\n",
    "\n",
    "    synced_tuning_values = [v[0] for v in tuning_results.values()]\n",
    "    non_synced_tuning_values = [v[1] for v in tuning_results.values()]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_animals)))  # Generate unique colors\n",
    "\n",
    "    for i, animal in enumerate(unique_animals):\n",
    "        ax.scatter(\n",
    "            synced_tuning_values[i],\n",
    "            non_synced_tuning_values[i],\n",
    "            color=colors[i],\n",
    "            label=f\"{animal}\",\n",
    "            s=30  # Marker size\n",
    "        )\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel(\"Synced Tuning\", fontsize=10)\n",
    "    ax.set_ylabel(\"Non-Synced Tuning\", fontsize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "    ax.set_aspect('equal')  # Ensure equal scaling\n",
    "    ax.axhline(0, color='gray', linestyle='--', linewidth=0.7)\n",
    "    ax.axvline(0, color='gray', linestyle='--', linewidth=0.7)\n",
    "\n",
    "    # Adjust axes limits to make sure x and y scales are identical\n",
    "    all_values = synced_tuning_values + non_synced_tuning_values\n",
    "    axis_limit = max(abs(min(all_values)), abs(max(all_values))) * 1.1\n",
    "    ax.set_xlim(-axis_limit, axis_limit)\n",
    "    ax.set_ylim(-axis_limit, axis_limit)\n",
    "\n",
    "    # Move legend outside the plot\n",
    "    ax.legend(fontsize=8, loc='center left', bbox_to_anchor=(1.05, 0.5), borderaxespad=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if export_path:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "        export_dir = pathlib.Path(export_path) / f'{timestamp}_horizontal_tuning_per_animal'\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save PDF\n",
    "        export_file_pdf = export_dir / f'{timestamp}_horizontal_tuning_per_animal.pdf'\n",
    "        plt.savefig(export_file_pdf, format='pdf')\n",
    "        print(f\"Exported plot to {export_file_pdf}\")\n",
    "\n",
    "        # Save Pickle\n",
    "        export_file_pkl = export_dir / f'{timestamp}_saccade_angles_data.pkl'\n",
    "        with open(export_file_pkl, 'wb') as pkl_file:\n",
    "            pickle.dump(saccade_collection, pkl_file)\n",
    "        print(f\"Exported data to {export_file_pkl}\")\n",
    "    plt.show()\n",
    "\n",
    "    return tuning_results\n",
    "\n",
    "export_path = r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_3\\materials\\horizontal_tuning'\n",
    "analyze_orientation_tuning(synced_saccade_collection,non_synced_saccade_collection,export_path=export_path)"
   ],
   "id": "62b37e42f79848c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %% Sanity summaries for all_saccade_collection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "okabe_ito = [\"#E69F00\",\"#56B4E9\",\"#009E73\",\"#F0E442\",\"#0072B2\",\"#D55E00\",\"#CC79A7\",\"#999999\"]\n",
    "alpha = 0.85\n",
    "bar_width = 0.38\n",
    "figsize = (7.0, 3.8)\n",
    "dpi = 140\n",
    "ylim_buffer = 1.10  # pad ymax by 10%\n",
    "\n",
    "# ---------- PREP DATA ----------\n",
    "df = all_saccade_collection.copy()\n",
    "\n",
    "# Ensure required columns exist / compute fallbacks\n",
    "# duration in ms\n",
    "if not {'saccade_on_ms', 'saccade_off_ms'}.issubset(df.columns):\n",
    "    raise ValueError(\"Expected 'saccade_on_ms' and 'saccade_off_ms' in all_saccade_collection.\")\n",
    "df['duration_ms'] = pd.to_numeric(df['saccade_off_ms'], errors='coerce') - pd.to_numeric(df['saccade_on_ms'], errors='coerce')\n",
    "\n",
    "# net angular displacement in deg\n",
    "if 'net_angular_disp' not in df.columns:\n",
    "    # fallback: compute from delta_phi and delta_theta if available\n",
    "    if {'delta_phi','delta_theta'}.issubset(df.columns):\n",
    "        df['net_angular_disp'] = np.sqrt(pd.to_numeric(df['delta_phi'], errors='coerce')**2 +\n",
    "                                         pd.to_numeric(df['delta_theta'], errors='coerce')**2)\n",
    "    else:\n",
    "        raise ValueError(\"No 'net_angular_disp' or ('delta_phi','delta_theta') in dataframe.\")\n",
    "\n",
    "# Basic cleaning\n",
    "df['net_angular_disp'] = pd.to_numeric(df['net_angular_disp'], errors='coerce')\n",
    "df['duration_ms'] = pd.to_numeric(df['duration_ms'], errors='coerce')\n",
    "df['eye'] = df['eye'].astype(str).str.upper().map({'L':'L','R':'R'})  # normalize\n",
    "df['animal'] = df['animal'].astype(str)\n",
    "\n",
    "# keep valid rows only\n",
    "mask_valid = np.isfinite(df['net_angular_disp']) & np.isfinite(df['duration_ms']) & (df['duration_ms'] > 0)\n",
    "df = df.loc[mask_valid].copy()\n",
    "\n",
    "# compute per-saccade average speed (deg/ms) using net displacement / duration\n",
    "df['avg_speed_deg_per_ms'] = df['net_angular_disp'] / df['duration_ms']\n",
    "\n",
    "# Optional: clip impossible/huge outliers (comment out if you prefer raw)\n",
    "for col, q in [('net_angular_disp', 99.9), ('duration_ms', 99.9), ('avg_speed_deg_per_ms', 99.9)]:\n",
    "    hi = np.nanpercentile(df[col], q)\n",
    "    df[col] = np.clip(df[col], None, hi)\n",
    "\n",
    "# ---------- GROUP STATS ----------\n",
    "def mean_sem(x):\n",
    "    x = pd.to_numeric(x, errors='coerce')\n",
    "    x = x[np.isfinite(x)]\n",
    "    n = x.size\n",
    "    if n == 0:\n",
    "        return pd.Series({'mean': np.nan, 'sem': np.nan, 'n': 0})\n",
    "    return pd.Series({'mean': np.mean(x), 'sem': np.std(x, ddof=1)/np.sqrt(n), 'n': n})\n",
    "\n",
    "# we’ll show bars per animal, split by eye (L/R)\n",
    "group_cols = ['animal','eye']\n",
    "\n",
    "stats_mag  = df.groupby(group_cols)['net_angular_disp'].apply(mean_sem).unstack()\n",
    "stats_len  = df.groupby(group_cols)['duration_ms'].apply(mean_sem).unstack()\n",
    "stats_spd  = df.groupby(group_cols)['avg_speed_deg_per_ms'].apply(mean_sem).unstack()\n",
    "\n",
    "# order animals consistently\n",
    "animals = sorted(df['animal'].unique())\n",
    "eyes = ['L','R'] if set(df['eye'].unique()) == {'L','R'} else sorted(df['eye'].unique())\n",
    "\n",
    "# color mapping per animal\n",
    "palette = (okabe_ito * ((len(animals)//len(okabe_ito))+1))[:len(animals)]\n",
    "color_map = {a:c for a,c in zip(animals, palette)}\n",
    "\n",
    "def _barplot_with_sem(ax, stats_df, ylabel, title):\n",
    "    x = np.arange(len(animals))\n",
    "    total_width = bar_width * (1 if len(eyes)==1 else 2)\n",
    "    offset = (-bar_width/2 if 'L' in eyes and 'R' in eyes else 0.0)\n",
    "\n",
    "    ymax = 0.0\n",
    "    for j, eye in enumerate(eyes):\n",
    "        means = [stats_df.loc[(a, eye)]['mean'] if (a, eye) in stats_df.index else np.nan for a in animals]\n",
    "        sems  = [stats_df.loc[(a, eye)]['sem']  if (a, eye) in stats_df.index else np.nan for a in animals]\n",
    "        ns    = [stats_df.loc[(a, eye)]['n']    if (a, eye) in stats_df.index else 0 for a in animals]\n",
    "        # choose consistent animal colors; eye distinguishes bar offset and hatch\n",
    "        bars = ax.bar(x + offset + j*bar_width, means, yerr=sems, width=bar_width,\n",
    "                      color=[color_map[a] for a in animals], alpha=alpha, capsize=3,\n",
    "                      edgecolor='black', linewidth=0.6,\n",
    "                      label=f\"Eye {eye}\")\n",
    "        # annotate N on top\n",
    "        for rect, n in zip(bars, ns):\n",
    "            if np.isfinite(rect.get_height()):\n",
    "                ax.text(rect.get_x() + rect.get_width()/2, rect.get_height()*1.01,\n",
    "                        f\"n={n}\", ha='center', va='bottom', fontsize=7, rotation=0)\n",
    "        if np.nanmax(means) > ymax:\n",
    "            ymax = np.nanmax(means)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(animals, rotation=0)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.set_yscale('linear')\n",
    "    if np.isfinite(ymax):\n",
    "        ax.set_ylim(0, ymax * ylim_buffer)\n",
    "    if len(eyes) > 1:\n",
    "        ax.legend(frameon=False, fontsize=8, loc='upper right')\n",
    "\n",
    "# ---------- PLOTS ----------\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# 1) Average magnitude (deg)\n",
    "fig1, ax1 = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "_barplot_with_sem(ax1, stats_mag, ylabel=\"Magnitude (deg)\", title=\"Average saccade magnitude\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) Average length (ms)\n",
    "fig2, ax2 = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "_barplot_with_sem(ax2, stats_len, ylabel=\"Duration (ms)\", title=\"Average saccade length\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Average speed (deg/ms), computed as net displacement / duration\n",
    "fig3, ax3 = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "_barplot_with_sem(ax3, stats_spd, ylabel=\"Average speed (deg/ms)\", title=\"Average saccade speed\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional quick printouts for sanity:\n",
    "print(\"Summary (per animal/eye):\")\n",
    "print(\"\\nMagnitude (deg):\\n\", stats_mag.round(3))\n",
    "print(\"\\nDuration (ms):\\n\", stats_len.round(3))\n",
    "print(\"\\nAvg speed (deg/ms):\\n\", stats_spd.round(4))\n"
   ],
   "id": "5fe8d1c2afe7601c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import pathlib\n",
    "import tqdm\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd  # Assuming pandas is imported as well\n",
    "\n",
    "def plot_right_vs_left_eye_hist2d(all_saccade_collection, block_dict, export_path=None,\n",
    "                                  iqr_multiplier=1.5, bins=100, sync_diff_threshold=31, value_range=(0,2)):\n",
    "    \"\"\"\n",
    "    Plots a 2D probability histogram (normalized so that the sum of all bins is 1) where the x-axis\n",
    "    represents the right eye's max speed and the y-axis represents the left eye's max speed.\n",
    "    Uses a custom colormap based on 'turbo' (with only the 0 value set to white) and applies a robust\n",
    "    exclusion rule so that saccades that are within sync_diff_threshold of a previously processed saccade\n",
    "    (for the same animal and block) are excluded.\n",
    "\n",
    "    The axes are kept equal and the colorbar is saved to a separate PDF.\n",
    "\n",
    "    Parameters:\n",
    "        all_saccade_collection (pd.DataFrame): DataFrame containing detected saccades.\n",
    "        block_dict (dict): Dictionary mapping block keys to block objects with eye data.\n",
    "        export_path (str or pathlib.Path, optional): Path to save the plot and colorbar PDFs.\n",
    "        iqr_multiplier (float): Multiplier for the interquartile range (IQR) in outlier detection.\n",
    "        bins (int): Number of bins to use for both x and y directions in the histogram.\n",
    "        sync_diff_threshold (float): Time difference threshold to consider saccades synchronized\n",
    "                                       (in the same units as 'saccade_on_ms').\n",
    "    \"\"\"\n",
    "    right_eye_speeds = []\n",
    "    left_eye_speeds = []\n",
    "\n",
    "    # Dictionary to track processed saccades for each (animal, block).\n",
    "    # Keys are (animal, block) and values are lists of saccade timestamps already used.\n",
    "    processed_events = {}\n",
    "\n",
    "    # Loop through each saccade entry.\n",
    "    for _, row in tqdm.tqdm(all_saccade_collection.iterrows()):\n",
    "        key = (row['animal'], row['block'])\n",
    "        current_timestamp = row['saccade_on_ms']\n",
    "\n",
    "        # Check if a saccade for this (animal, block) has already been processed\n",
    "        # within sync_diff_threshold.\n",
    "        if key in processed_events:\n",
    "            if any(abs(current_timestamp - ts) < sync_diff_threshold for ts in processed_events[key]):\n",
    "                continue  # Skip as this saccade is considered synchronized with a previous one.\n",
    "            else:\n",
    "                processed_events[key].append(current_timestamp)\n",
    "        else:\n",
    "            processed_events[key] = [current_timestamp]\n",
    "\n",
    "        block_key = f\"{row['animal']}_block_{row['block']}\"\n",
    "        if block_key not in block_dict:\n",
    "            continue\n",
    "\n",
    "        block = block_dict[block_key]\n",
    "\n",
    "        # Get contralateral eye's data based on which eye was detected.\n",
    "        if row['eye'] == 'L':\n",
    "            contralateral_eye_speed_profile = block.right_eye_data.query(\n",
    "                'ms_axis >= @current_timestamp - 51 and ms_axis <= @current_timestamp + 51'\n",
    "            )['angular_speed_r']\n",
    "        elif row['eye'] == 'R':\n",
    "            contralateral_eye_speed_profile = block.left_eye_data.query(\n",
    "                'ms_axis >= @current_timestamp - 51 and ms_axis <= @current_timestamp + 51'\n",
    "            )['angular_speed_r']\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Compute the maximum speeds (scaling as in the original function)\n",
    "        max_detected = np.nanmax(row['speed_profile_angular'] / 17) if len(row['speed_profile_angular']) > 0 else np.nan\n",
    "        max_contralateral = (np.nanmax(contralateral_eye_speed_profile.values / 17)\n",
    "                             if contralateral_eye_speed_profile.notna().sum() > 0 else np.nan)\n",
    "\n",
    "        # Append valid pairs.\n",
    "        if not np.isnan(max_detected) and not np.isnan(max_contralateral):\n",
    "            if row['eye'] == 'L':\n",
    "                left_eye_speeds.append(max_detected)\n",
    "                right_eye_speeds.append(max_contralateral)\n",
    "            elif row['eye'] == 'R':\n",
    "                right_eye_speeds.append(max_detected)\n",
    "                left_eye_speeds.append(max_contralateral)\n",
    "\n",
    "    # Convert lists to NumPy arrays.\n",
    "    right_eye_speeds = np.array(right_eye_speeds)\n",
    "    left_eye_speeds = np.array(left_eye_speeds)\n",
    "    print(\"Number of data points:\", len(right_eye_speeds))\n",
    "\n",
    "    # Outlier removal using IQR on the combined speeds.\n",
    "    all_speeds = np.concatenate([right_eye_speeds, left_eye_speeds])\n",
    "    q1, q3 = np.percentile(all_speeds, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - iqr_multiplier * iqr\n",
    "    upper_bound = q3 + iqr_multiplier * iqr\n",
    "\n",
    "    valid_indices = ((right_eye_speeds >= lower_bound) & (right_eye_speeds <= upper_bound) &\n",
    "                     (left_eye_speeds >= lower_bound) & (left_eye_speeds <= upper_bound))\n",
    "    right_eye_speeds = right_eye_speeds[valid_indices]\n",
    "    left_eye_speeds = left_eye_speeds[valid_indices]\n",
    "\n",
    "    # Fixed axis limits.\n",
    "    min_value = value_range[0]\n",
    "    max_value = value_range[1]\n",
    "\n",
    "    # Create the custom turbo colormap: get a discrete version of turbo and replace the first color with white.\n",
    "    n_colors = 256\n",
    "    turbo = plt.get_cmap('turbo', n_colors)\n",
    "    turbo_colors = turbo(np.linspace(0, 1, n_colors))\n",
    "    turbo_colors[0] = np.array([1, 1, 1, 1])  # Set the lowest count (0 probability) to perfect white.\n",
    "    custom_turbo = mcolors.ListedColormap(turbo_colors)\n",
    "\n",
    "    # Prepare the figure for the histogram.\n",
    "    fig, ax = plt.subplots(figsize=(2, 2), dpi=150)\n",
    "    xbins = np.linspace(min_value, max_value, bins)\n",
    "    ybins = np.linspace(min_value, max_value, bins)\n",
    "\n",
    "    # Compute the 2D histogram manually.\n",
    "    counts, xedges, yedges = np.histogram2d(right_eye_speeds, left_eye_speeds, bins=[xbins, ybins])\n",
    "    normalized_counts = counts / counts.sum()  # Normalize so that the overall sum equals 1.\n",
    "\n",
    "    # Plot the normalized histogram using pcolormesh.\n",
    "    pc = ax.pcolormesh(xedges, yedges, normalized_counts.T, cmap=custom_turbo,\n",
    "                         vmin=0, vmax=normalized_counts.max())\n",
    "\n",
    "    # Ensure equal axes.\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.set_xlim(min_value, max_value)\n",
    "    ax.set_ylim(min_value, max_value)\n",
    "\n",
    "    # Add a diagonal reference line.\n",
    "    ax.plot([min_value, max_value], [min_value, max_value], linestyle='--', color='gray', linewidth=1)\n",
    "\n",
    "    # Set axis labels.\n",
    "    ax.set_xlabel(\"Right max V [deg/ms]\", fontsize=10)\n",
    "    ax.set_ylabel(\"Left max V [deg/ms]\", fontsize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the histogram plot if export_path is provided.\n",
    "    if export_path:\n",
    "        export_dir = pathlib.Path(export_path)\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "        hist_pdf_path = export_dir / \"right_vs_left_hist2d.pdf\"\n",
    "        fig.savefig(hist_pdf_path, format='pdf', bbox_inches='tight')\n",
    "        print(f\"2D histogram saved to {hist_pdf_path}\")\n",
    "\n",
    "    # Create a separate figure for the colorbar.\n",
    "    vmin_val = 0\n",
    "    vmax_val = normalized_counts.max()\n",
    "    sm = plt.cm.ScalarMappable(cmap=custom_turbo, norm=plt.Normalize(vmin=vmin_val, vmax=vmax_val))\n",
    "    sm.set_array([])\n",
    "    fig2 = plt.figure(figsize=(1, 3), dpi=150)\n",
    "    cbar = fig2.colorbar(sm, orientation='vertical')\n",
    "    cbar.set_label(\"Probability\", fontsize=8)\n",
    "    cbar.ax.tick_params(labelsize=8)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the colorbar to a separate PDF if export_path is provided.\n",
    "    if export_path:\n",
    "        cbar_pdf_path = export_dir / \"right_vs_left_colorbar.pdf\"\n",
    "        fig2.savefig(cbar_pdf_path, format='pdf', bbox_inches='tight')\n",
    "        print(f\"Colorbar saved to {cbar_pdf_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "t = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "export_path = rf'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_3\\materials\\show_monocular_saccades\\{t}'\n",
    "plot_right_vs_left_eye_hist2d(all_saccade_collection, block_dict, export_path=None, iqr_multiplier=200, bins=200,value_range=(0,2))\n"
   ],
   "id": "954a595fc6d6bcf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_log_mult17_bins(num_bins, low=17, high=20000):\n",
    "    # Define k domain: ISI = 17 * k\n",
    "    k_min = 1\n",
    "    k_max = high // low  # integer division\n",
    "    # We need num_bins+1 edges\n",
    "    k_edges = np.geomspace(k_min, k_max, num_bins+1)\n",
    "    # Round to the nearest integer and multiply by 17\n",
    "    bins = np.unique(np.round(k_edges).astype(int) * low)\n",
    "    return bins\n",
    "\n",
    "# Example usage:\n",
    "num_bins = 20\n",
    "bins = generate_log_mult17_bins(num_bins)\n",
    "print(\"Bin edges:\", bins)\n"
   ],
   "id": "5d0659b11ef847a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "# monocular saccade graph:\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "def process_profile(profile, expected_length):\n",
    "    \"\"\"\n",
    "    Adjusts a profile to the expected length by either padding or snipping.\n",
    "    \"\"\"\n",
    "    current_length = len(profile)\n",
    "\n",
    "    if current_length == expected_length:\n",
    "        return profile\n",
    "\n",
    "    # Snip or pad to match the expected length\n",
    "    if current_length > expected_length:\n",
    "        excess = (current_length - expected_length) // 2\n",
    "        return profile[excess:excess + expected_length]\n",
    "    else:\n",
    "        pad_before = (expected_length - current_length) // 2\n",
    "        pad_after = expected_length - current_length - pad_before\n",
    "        return np.pad(profile, (pad_before, pad_after), constant_values=np.nan)\n",
    "\n",
    "\n",
    "def average_saccade_profile(non_synced_saccade_collection, block_dict, window_ms=200, sampling_rate=60,\n",
    "                            export_path=None):\n",
    "    half_window = window_ms // 2\n",
    "    expected_length = int((window_ms / 1000) * sampling_rate)\n",
    "    time_axis = (np.arange(expected_length) * 1000 / sampling_rate) - half_window\n",
    "\n",
    "    saccade_profiles = []\n",
    "    opposite_eye_profiles = []\n",
    "\n",
    "    for _, row in non_synced_saccade_collection.iterrows():\n",
    "        block_num = row['block']\n",
    "        eye = row['eye']\n",
    "        start_ts = row['saccade_on_ms']\n",
    "        opposite_eye = 'L' if eye == 'R' else 'R'\n",
    "\n",
    "        block_key = f\"{row['animal']}_block_{block_num}\"\n",
    "        if block_key not in block_dict:\n",
    "            continue\n",
    "\n",
    "        block = block_dict[block_key]\n",
    "        current_eye_data = block.left_eye_data if eye == 'L' else block.right_eye_data\n",
    "        opposite_eye_data = block.left_eye_data if opposite_eye == 'L' else block.right_eye_data\n",
    "\n",
    "        saccade_profile = current_eye_data.query(\n",
    "            'ms_axis >= @start_ts - @half_window and ms_axis <= @start_ts + @half_window')['speed_r'].values\n",
    "        opposite_profile = opposite_eye_data.query(\n",
    "            'ms_axis >= @start_ts - @half_window and ms_axis <= @start_ts + @half_window')['speed_r'].values\n",
    "\n",
    "        if len(saccade_profile) == 0 or len(opposite_profile) == 0:\n",
    "            print(f\"Skipped saccade at {start_ts} due to missing data.\")\n",
    "            continue\n",
    "\n",
    "        # Ensure fixed length through padding or snipping\n",
    "        saccade_profiles.append(process_profile(saccade_profile, expected_length))\n",
    "        opposite_eye_profiles.append(process_profile(opposite_profile, expected_length))\n",
    "\n",
    "    if not saccade_profiles or not opposite_eye_profiles:\n",
    "        print(\"No valid saccade profiles found.\")\n",
    "        return None, None\n",
    "\n",
    "    avg_saccade = np.nanmean(np.vstack(saccade_profiles), axis=0)\n",
    "    avg_opposite_eye = np.nanmean(np.vstack(opposite_eye_profiles), axis=0)\n",
    "\n",
    "    # Plot the results\n",
    "    fig, ax = plt.subplots(figsize=(1.7, 1.2), dpi=100)\n",
    "    ax.grid(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.tick_params(axis='both', which='major', length=5, color='black')\n",
    "    ax.ticklabel_format(axis='y', style='plain')\n",
    "    ax.set_ylabel(\"Speed (pixels/ms)\", fontsize=10)\n",
    "    ax.tick_params(axis='both', labelsize=8)\n",
    "    ax.plot(time_axis, avg_saccade, label=\"Monocular Saccade\", color='g')\n",
    "    ax.plot(time_axis, avg_opposite_eye, label=\"Opposite Eye\", color='b')\n",
    "    ax.set_title(\"Average Saccade Profiles\")\n",
    "    ax.set_xlabel(\"Time (ms)\")\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(0.6, 0.8), fontsize=6)\n",
    "    plt.show()\n",
    "\n",
    "    # Save results\n",
    "    if export_path:\n",
    "        t = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "        export_dir = pathlib.Path(export_path) / f\"Saccade_Profile_{t}\"\n",
    "        os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "        pdf_path = export_dir / \"average_saccade_profiles.pdf\"\n",
    "        fig.savefig(pdf_path, format='pdf', bbox_inches='tight')\n",
    "\n",
    "        data_dict = {\n",
    "            'time_axis': time_axis,\n",
    "            'avg_saccade': avg_saccade,\n",
    "            'avg_opposite_eye': avg_opposite_eye,\n",
    "            'saccade_collection': non_synced_saccade_collection\n",
    "        }\n",
    "        pickle_path = export_dir / \"saccade_profiles.pkl\"\n",
    "        with open(pickle_path, \"wb\") as f:\n",
    "            pickle.dump(data_dict, f)\n",
    "        print(f\"Data saved to {export_dir}\")\n",
    "\n",
    "    return saccade_profiles, opposite_eye_profiles\n",
    "\n",
    "\n"
   ],
   "id": "16abecb6335b9366",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "no_head_mov_saccades = non_synced_saccade_collection.query('head_movement == False')\n",
    "export_path = pathlib.Path(\n",
    "    r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_3\\materials\\Monocular_saccade_averages_no_head_mov')\n",
    "saccade_profiles = average_saccade_profile(no_head_mov_saccades, block_dict, window_ms=1000, sampling_rate=60,\n",
    "                                           export_path=None)"
   ],
   "id": "4eb41f9cad66748",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "\n",
    "export_path = pathlib.Path(\n",
    "    r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_3\\materials\\Monocular_saccade_averages')\n",
    "saccade_profiles = average_saccade_profile(non_synced_saccade_collection, block_dict, window_ms=1000, sampling_rate=60,\n",
    "                                           export_path=export_path)\n"
   ],
   "id": "7b64e6af20cc6b63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "all_saccade_collection.columns",
   "id": "b561b7bc7d85ea44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "\n",
    "def plot_faster_vs_slower_final(all_saccade_collection, block_dict, export_path=None, iqr_multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Plots a scatter plot where the x-axis always represents the larger velocity (either detected or contralateral),\n",
    "    and the y-axis represents the smaller velocity.\n",
    "\n",
    "    Parameters:\n",
    "        all_saccade_collection (pd.DataFrame): DataFrame containing detected saccades.\n",
    "        block_dict (dict): Dictionary mapping block keys to block objects with eye data.\n",
    "        export_path (str or pathlib.Path, optional): Path to save the plot. Defaults to None.\n",
    "        iqr_multiplier (float): Multiplier for interquartile range (IQR) in outlier detection. Defaults to 1.5.\n",
    "    \"\"\"\n",
    "    faster_speeds = []\n",
    "    slower_speeds = []\n",
    "\n",
    "    for _, row in tqdm.tqdm(all_saccade_collection.iterrows()):\n",
    "        block_num = row['block']\n",
    "        start_ts = row['saccade_on_ms']\n",
    "        detected_speed_profile = row['speed_profile_angular']  # Velocity profile for the detected eye\n",
    "        block_key = f\"{row['animal']}_block_{block_num}\"\n",
    "\n",
    "        if block_key not in block_dict:\n",
    "            continue\n",
    "\n",
    "        block = block_dict[block_key]\n",
    "\n",
    "        # Get contralateral eye data\n",
    "        if row['eye'] == 'L':\n",
    "            contralateral_eye_speed_profile = block.right_eye_data.query(\n",
    "                'ms_axis >= @start_ts - 34 and ms_axis <= @start_ts + 34')['angular_speed_r']\n",
    "        elif row['eye'] == 'R':\n",
    "            contralateral_eye_speed_profile = block.left_eye_data.query(\n",
    "                'ms_axis >= @start_ts - 34 and ms_axis <= @start_ts + 34')['angular_speed_r']\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Compute the maximum speeds\n",
    "        max_detected = np.nanmax(detected_speed_profile / 17) if len(detected_speed_profile) > 0 else np.nan\n",
    "        max_contralateral = np.nanmax(contralateral_eye_speed_profile.values / 17) \\\n",
    "            if contralateral_eye_speed_profile.notna().sum() > 0 else np.nan\n",
    "\n",
    "        # Assign faster and slower speeds\n",
    "        if not np.isnan(max_detected) and not np.isnan(max_contralateral):\n",
    "            faster_speeds.append(max(max_detected, max_contralateral))\n",
    "            slower_speeds.append(min(max_detected, max_contralateral))\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    faster_speeds = np.array(faster_speeds) / 17\n",
    "    slower_speeds = np.array(slower_speeds) / 17\n",
    "    print(len(faster_speeds), len(slower_speeds))\n",
    "\n",
    "    # Outlier removal using the IQR method\n",
    "    all_speeds = np.concatenate([faster_speeds, slower_speeds])\n",
    "    q1, q3 = np.percentile(all_speeds, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - iqr_multiplier * iqr\n",
    "    upper_bound = q3 + iqr_multiplier * iqr\n",
    "\n",
    "    valid_indices = (faster_speeds >= lower_bound) & (faster_speeds <= upper_bound) & \\\n",
    "                    (slower_speeds >= lower_bound) & (slower_speeds <= upper_bound)\n",
    "\n",
    "    faster_speeds = faster_speeds[valid_indices]\n",
    "    slower_speeds = slower_speeds[valid_indices]\n",
    "\n",
    "    # Determine axis limits\n",
    "    min_value = min(np.min(faster_speeds), np.min(slower_speeds))\n",
    "    max_value = max(np.max(faster_speeds), np.max(slower_speeds))\n",
    "\n",
    "    # Create scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(2, 2), dpi=150)\n",
    "    ax.scatter(faster_speeds, slower_speeds, alpha=0.7, s=1, c='k', edgecolors='none')\n",
    "\n",
    "    # Plot diagonal reference line\n",
    "    ax.plot([min_value, max_value], [min_value, max_value], linestyle='--', color='gray', linewidth=1)\n",
    "\n",
    "    # Configure plot appearance\n",
    "    ax.set_title(\"Monocular detections\", fontsize=10)\n",
    "    ax.set_xlabel(\"Detected max speed (deg/ms)\", fontsize=10)\n",
    "    ax.set_ylabel(\"Contralateral max speed (deg/ms)\", fontsize=10)\n",
    "    ax.set_xlim(min_value, max_value)\n",
    "    ax.set_ylim(min_value, max_value)\n",
    "    ax.grid(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if export_path is provided\n",
    "    if export_path:\n",
    "        export_dir = pathlib.Path(export_path)\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "        pdf_path = export_dir / \"faster_vs_slower_scatter.pdf\"\n",
    "        fig.savefig(pdf_path, format='pdf', bbox_inches='tight')\n",
    "        print(f\"Scatter plot saved to {pdf_path}\")\n",
    "\n",
    "    plt.show()\n",
    "t = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "export_path = rf'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_3\\materials\\show_monocular_saccades\\{t}'\n",
    "# Example usage\n",
    "plot_faster_vs_slower_final(all_saccade_collection, block_dict, export_path=export_path, iqr_multiplier=10)\n"
   ],
   "id": "a0b9d069c15c89de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import tqdm  # Make sure tqdm is imported if you use it\n",
    "\n",
    "def plot_right_vs_left_eye(all_saccade_collection, block_dict, export_path=None, iqr_multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Plots a scatter plot where the x-axis represents the right eye's max speed and the y-axis represents the left eye's max speed.\n",
    "\n",
    "    Parameters:\n",
    "        all_saccade_collection (pd.DataFrame): DataFrame containing detected saccades.\n",
    "        block_dict (dict): Dictionary mapping block keys to block objects with eye data.\n",
    "        export_path (str or pathlib.Path, optional): Path to save the plot. Defaults to None.\n",
    "        iqr_multiplier (float): Multiplier for interquartile range (IQR) in outlier detection. Defaults to 1.5.\n",
    "    \"\"\"\n",
    "    right_eye_speeds = []\n",
    "    left_eye_speeds = []\n",
    "\n",
    "    for _, row in tqdm.tqdm(all_saccade_collection.iterrows()):\n",
    "        block_num = row['block']\n",
    "        start_ts = row['saccade_on_ms']\n",
    "        detected_speed_profile = row['speed_profile_angular']  # Velocity profile for the detected eye\n",
    "        block_key = f\"{row['animal']}_block_{block_num}\"\n",
    "\n",
    "        if block_key not in block_dict:\n",
    "            continue\n",
    "\n",
    "        block = block_dict[block_key]\n",
    "\n",
    "        # Get contralateral eye data\n",
    "        if row['eye'] == 'L':\n",
    "            # Detected eye is left; so get right eye data as contralateral.\n",
    "            contralateral_eye_speed_profile = block.right_eye_data.query(\n",
    "                'ms_axis >= @start_ts - 34 and ms_axis <= @start_ts + 34'\n",
    "            )['angular_speed_r']\n",
    "        elif row['eye'] == 'R':\n",
    "            # Detected eye is right; so get left eye data as contralateral.\n",
    "            contralateral_eye_speed_profile = block.left_eye_data.query(\n",
    "                'ms_axis >= @start_ts - 34 and ms_axis <= @start_ts + 34'\n",
    "            )['angular_speed_r']\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Compute the maximum speeds (dividing by 17 as in the original function)\n",
    "        max_detected = np.nanmax(detected_speed_profile / 17) if len(detected_speed_profile) > 0 else np.nan\n",
    "        max_contralateral = np.nanmax(contralateral_eye_speed_profile.values / 17) \\\n",
    "            if contralateral_eye_speed_profile.notna().sum() > 0 else np.nan\n",
    "\n",
    "        # Only add if both values are valid\n",
    "        if not np.isnan(max_detected) and not np.isnan(max_contralateral):\n",
    "            if row['eye'] == 'L':\n",
    "                # Detected is left eye, so:\n",
    "                left_eye_speeds.append(max_detected)\n",
    "                right_eye_speeds.append(max_contralateral)\n",
    "            elif row['eye'] == 'R':\n",
    "                # Detected is right eye, so:\n",
    "                right_eye_speeds.append(max_detected)\n",
    "                left_eye_speeds.append(max_contralateral)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    right_eye_speeds = np.array(right_eye_speeds)\n",
    "    left_eye_speeds = np.array(left_eye_speeds)\n",
    "    print(len(right_eye_speeds), len(left_eye_speeds))\n",
    "\n",
    "    # Outlier removal using the IQR method (based on combined speeds)\n",
    "    all_speeds = np.concatenate([right_eye_speeds, left_eye_speeds])\n",
    "    q1, q3 = np.percentile(all_speeds, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - iqr_multiplier * iqr\n",
    "    upper_bound = q3 + iqr_multiplier * iqr\n",
    "\n",
    "    valid_indices = (right_eye_speeds >= lower_bound) & (right_eye_speeds <= upper_bound) & \\\n",
    "                    (left_eye_speeds >= lower_bound) & (left_eye_speeds <= upper_bound)\n",
    "\n",
    "    right_eye_speeds = right_eye_speeds[valid_indices]\n",
    "    left_eye_speeds = left_eye_speeds[valid_indices]\n",
    "\n",
    "    # Determine axis limits\n",
    "    min_value = min(np.min(right_eye_speeds), np.min(left_eye_speeds))\n",
    "    max_value = max(np.max(right_eye_speeds), np.max(left_eye_speeds))\n",
    "\n",
    "    # Create scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(1.7, 1.7), dpi=150)\n",
    "    ax.scatter(right_eye_speeds, left_eye_speeds, alpha=0.7, s=1, c='k', edgecolors='none')\n",
    "\n",
    "    # Plot diagonal reference line for comparison\n",
    "    ax.plot([min_value, max_value], [min_value, max_value], linestyle='--', color='gray', linewidth=1)\n",
    "\n",
    "    # Configure plot appearance\n",
    "    #ax.set_title(\"Eye speed comparison\", fontsize=10)\n",
    "    ax.set_xlabel(\"Right max V [deg/ms]\", fontsize=10)\n",
    "    ax.set_ylabel(\"Left max V [deg/ms]\", fontsize=10)\n",
    "    ax.set_xlim(min_value, max_value)\n",
    "    ax.set_ylim(min_value, max_value)\n",
    "    ax.grid(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "    ax.set_xlim(0,3)\n",
    "    ax.set_ylim(0,3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if export_path is provided\n",
    "    if export_path:\n",
    "        export_dir = pathlib.Path(export_path)\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "        pdf_path = export_dir / \"right_vs_left_scatter.pdf\"\n",
    "        fig.savefig(pdf_path, format='pdf', bbox_inches='tight')\n",
    "        print(f\"Scatter plot saved to {pdf_path}\")\n",
    "        data_dict = {\n",
    "            'saccade_collection': all_saccade_collection\n",
    "        }\n",
    "        pickle_path = export_dir / \"saccade_collection.pkl\"\n",
    "\n",
    "        with open(pickle_path, \"wb\") as f:\n",
    "            pickle.dump(data_dict, f)\n",
    "        print(f\"Data saved to {export_dir}\")\n",
    "    plt.show()\n",
    "t = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "export_path = rf'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_3\\materials\\show_monocular_saccades\\{t}'\n",
    "\n",
    "plot_right_vs_left_eye(all_saccade_collection, block_dict, export_path=None, iqr_multiplier=200)\n"
   ],
   "id": "a0d7f4f54ee92590",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import pathlib\n",
    "import tqdm\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd  # Assuming pandas is imported as well\n",
    "\n",
    "def plot_right_vs_left_eye_hist2d(all_saccade_collection, block_dict, export_path=None,\n",
    "                                  iqr_multiplier=1.5, bins=100, sync_diff_threshold=31):\n",
    "    \"\"\"\n",
    "    Plots a 2D histogram where the x-axis represents the right eye's max speed and\n",
    "    the y-axis represents the left eye's max speed. Uses a custom colormap based on 'turbo'\n",
    "    (with only the 0 count set to white) and applies a robust exclusion rule so that saccades\n",
    "    that are within sync_diff_threshold of a previously processed saccade (for the same animal and block)\n",
    "    are excluded.\n",
    "\n",
    "    The axes are kept equal and the colorbar is saved to a separate PDF.\n",
    "\n",
    "    Parameters:\n",
    "        all_saccade_collection (pd.DataFrame): DataFrame containing detected saccades.\n",
    "        block_dict (dict): Dictionary mapping block keys to block objects with eye data.\n",
    "        export_path (str or pathlib.Path, optional): Path to save the plot and colorbar PDFs.\n",
    "        iqr_multiplier (float): Multiplier for the interquartile range (IQR) in outlier detection.\n",
    "        bins (int): Number of bins to use for both x and y directions in the histogram.\n",
    "        sync_diff_threshold (float): Time difference threshold to consider saccades synchronized (in the same units as 'saccade_on_ms').\n",
    "    \"\"\"\n",
    "    right_eye_speeds = []\n",
    "    left_eye_speeds = []\n",
    "\n",
    "    # Dictionary to track processed saccades for each (animal, block).\n",
    "    # Keys are (animal, block) and values are lists of saccade timestamps already used.\n",
    "    processed_events = {}\n",
    "\n",
    "    # Loop through each saccade entry.\n",
    "    for _, row in tqdm.tqdm(all_saccade_collection.iterrows()):\n",
    "        key = (row['animal'], row['block'])\n",
    "        current_timestamp = row['saccade_on_ms']\n",
    "\n",
    "        # If we have already processed a saccade for this (animal, block),\n",
    "        # check if the current saccade is within sync_diff_threshold of any previous one.\n",
    "        if key in processed_events:\n",
    "            if any(abs(current_timestamp - ts) < sync_diff_threshold for ts in processed_events[key]):\n",
    "                continue  # Skip this row as it is considered synchronized with a previous saccade.\n",
    "            else:\n",
    "                processed_events[key].append(current_timestamp)\n",
    "        else:\n",
    "            processed_events[key] = [current_timestamp]\n",
    "\n",
    "        block_key = f\"{row['animal']}_block_{row['block']}\"\n",
    "        if block_key not in block_dict:\n",
    "            continue\n",
    "\n",
    "        block = block_dict[block_key]\n",
    "\n",
    "        # Get contralateral eye's data based on which eye was detected.\n",
    "        if row['eye'] == 'L':\n",
    "            contralateral_eye_speed_profile = block.right_eye_data.query(\n",
    "                'ms_axis >= @current_timestamp - 51 and ms_axis <= @current_timestamp + 51'\n",
    "            )['angular_speed_r']\n",
    "        elif row['eye'] == 'R':\n",
    "            contralateral_eye_speed_profile = block.left_eye_data.query(\n",
    "                'ms_axis >= @current_timestamp - 51 and ms_axis <= @current_timestamp + 51'\n",
    "            )['angular_speed_r']\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Compute the maximum speeds (scaling as in the original function)\n",
    "        max_detected = np.nanmax(row['speed_profile_angular'] / 17) if len(row['speed_profile_angular']) > 0 else np.nan\n",
    "        max_contralateral = (np.nanmax(contralateral_eye_speed_profile.values / 17)\n",
    "                             if contralateral_eye_speed_profile.notna().sum() > 0 else np.nan)\n",
    "\n",
    "        # Append valid pairs\n",
    "        if not np.isnan(max_detected) and not np.isnan(max_contralateral):\n",
    "            if row['eye'] == 'L':\n",
    "                left_eye_speeds.append(max_detected)\n",
    "                right_eye_speeds.append(max_contralateral)\n",
    "            elif row['eye'] == 'R':\n",
    "                right_eye_speeds.append(max_detected)\n",
    "                left_eye_speeds.append(max_contralateral)\n",
    "\n",
    "    # Convert lists to NumPy arrays.\n",
    "    right_eye_speeds = np.array(right_eye_speeds)\n",
    "    left_eye_speeds = np.array(left_eye_speeds)\n",
    "    print(\"Number of data points:\", len(right_eye_speeds))\n",
    "\n",
    "    # Outlier removal using IQR on the combined speeds.\n",
    "    all_speeds = np.concatenate([right_eye_speeds, left_eye_speeds])\n",
    "    q1, q3 = np.percentile(all_speeds, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - iqr_multiplier * iqr\n",
    "    upper_bound = q3 + iqr_multiplier * iqr\n",
    "\n",
    "    valid_indices = ((right_eye_speeds >= lower_bound) & (right_eye_speeds <= upper_bound) &\n",
    "                     (left_eye_speeds >= lower_bound) & (left_eye_speeds <= upper_bound))\n",
    "    right_eye_speeds = right_eye_speeds[valid_indices]\n",
    "    left_eye_speeds = left_eye_speeds[valid_indices]\n",
    "\n",
    "    # Fixed axis limits.\n",
    "    min_value = 0\n",
    "    max_value = 3\n",
    "\n",
    "    # Create the custom turbo colormap: Get the discrete turbo colormap and replace the first color with white.\n",
    "    n_colors = 256\n",
    "    turbo = plt.get_cmap('turbo', n_colors)\n",
    "    turbo_colors = turbo(np.linspace(0, 1, n_colors))\n",
    "    turbo_colors[0] = np.array([1, 1, 1, 1])  # Set the lowest count to perfect white.\n",
    "    custom_turbo = mcolors.ListedColormap(turbo_colors)\n",
    "\n",
    "    # Prepare the figure for the histogram.\n",
    "    fig, ax = plt.subplots(figsize=(2, 2), dpi=300)\n",
    "    xbins = np.linspace(min_value, max_value, bins)\n",
    "    ybins = np.linspace(min_value, max_value, bins)\n",
    "\n",
    "    # Plot the 2D histogram.\n",
    "    hist_tuple = ax.hist2d(right_eye_speeds, left_eye_speeds, bins=[xbins, ybins], cmap=custom_turbo, vmin=0, density=True)\n",
    "    image = hist_tuple[3]  # QuadMesh object for the histogram.\n",
    "\n",
    "    # Ensure equal axes.\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.set_xlim(min_value, max_value)\n",
    "    ax.set_ylim(min_value, max_value)\n",
    "\n",
    "    # Add a diagonal reference line.\n",
    "    ax.plot([min_value, max_value], [min_value, max_value], linestyle='--', color='gray', linewidth=1)\n",
    "\n",
    "    # Set axis labels.\n",
    "    ax.set_xlabel(\"Right max V [deg/ms]\", fontsize=10)\n",
    "    ax.set_ylabel(\"Left max V [deg/ms]\", fontsize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the histogram plot if export_path is provided.\n",
    "    if export_path:\n",
    "        export_dir = pathlib.Path(export_path)\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "        hist_pdf_path = export_dir / \"right_vs_left_hist2d.pdf\"\n",
    "        fig.savefig(hist_pdf_path, format='pdf', bbox_inches='tight')\n",
    "        print(f\"2D histogram saved to {hist_pdf_path}\")\n",
    "\n",
    "    # Create a separate figure for the colorbar.\n",
    "    vmin = 0\n",
    "    vmax = image.get_clim()[1]\n",
    "    sm = plt.cm.ScalarMappable(cmap=custom_turbo, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "    sm.set_array([])\n",
    "    fig2 = plt.figure(figsize=(1, 3), dpi=150)\n",
    "    cbar = fig2.colorbar(sm, orientation='vertical')\n",
    "    cbar.set_label(\"Count\", fontsize=8)\n",
    "    cbar.ax.tick_params(labelsize=8)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the colorbar to a separate PDF if export_path is provided.\n",
    "    if export_path:\n",
    "        cbar_pdf_path = export_dir / \"right_vs_left_colorbar.pdf\"\n",
    "        fig2.savefig(cbar_pdf_path, format='pdf', bbox_inches='tight')\n",
    "        print(f\"Colorbar saved to {cbar_pdf_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Generate a timestamp for export path naming.\n",
    "t = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "export_path = rf'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_3\\materials\\show_monocular_saccades\\{t}'\n",
    "\n",
    "# Before calling, replace 'all_saccade_collection' and 'block_dict' with your actual data.\n",
    "plot_right_vs_left_eye_hist2d(all_saccade_collection, block_dict, export_path=export_path, iqr_multiplier=200, bins=250)\n"
   ],
   "id": "8e930231b4a710a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import pathlib\n",
    "import tqdm\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd  # Assuming pandas is imported as well\n",
    "\n",
    "def plot_right_vs_left_eye_hist2d(all_saccade_collection, block_dict, export_path=None,\n",
    "                                  iqr_multiplier=1.5, bins=100, sync_diff_threshold=31):\n",
    "    \"\"\"\n",
    "    Plots a 2D probability histogram (normalized so that the sum of all bins is 1) where the x-axis\n",
    "    represents the right eye's max speed and the y-axis represents the left eye's max speed.\n",
    "    Uses a custom colormap based on 'turbo' (with only the 0 value set to white) and applies a robust\n",
    "    exclusion rule so that saccades that are within sync_diff_threshold of a previously processed saccade\n",
    "    (for the same animal and block) are excluded.\n",
    "\n",
    "    The axes are kept equal and the colorbar is saved to a separate PDF.\n",
    "\n",
    "    Parameters:\n",
    "        all_saccade_collection (pd.DataFrame): DataFrame containing detected saccades.\n",
    "        block_dict (dict): Dictionary mapping block keys to block objects with eye data.\n",
    "        export_path (str or pathlib.Path, optional): Path to save the plot and colorbar PDFs.\n",
    "        iqr_multiplier (float): Multiplier for the interquartile range (IQR) in outlier detection.\n",
    "        bins (int): Number of bins to use for both x and y directions in the histogram.\n",
    "        sync_diff_threshold (float): Time difference threshold to consider saccades synchronized\n",
    "                                       (in the same units as 'saccade_on_ms').\n",
    "    \"\"\"\n",
    "    right_eye_speeds = []\n",
    "    left_eye_speeds = []\n",
    "\n",
    "    # Dictionary to track processed saccades for each (animal, block).\n",
    "    # Keys are (animal, block) and values are lists of saccade timestamps already used.\n",
    "    processed_events = {}\n",
    "\n",
    "    # Loop through each saccade entry.\n",
    "    for _, row in tqdm.tqdm(all_saccade_collection.iterrows()):\n",
    "        key = (row['animal'], row['block'])\n",
    "        current_timestamp = row['saccade_on_ms']\n",
    "\n",
    "        # Check if a saccade for this (animal, block) has already been processed\n",
    "        # within sync_diff_threshold.\n",
    "        if key in processed_events:\n",
    "            if any(abs(current_timestamp - ts) < sync_diff_threshold for ts in processed_events[key]):\n",
    "                continue  # Skip as this saccade is considered synchronized with a previous one.\n",
    "            else:\n",
    "                processed_events[key].append(current_timestamp)\n",
    "        else:\n",
    "            processed_events[key] = [current_timestamp]\n",
    "\n",
    "        block_key = f\"{row['animal']}_block_{row['block']}\"\n",
    "        if block_key not in block_dict:\n",
    "            continue\n",
    "\n",
    "        block = block_dict[block_key]\n",
    "\n",
    "        # Get contralateral eye's data based on which eye was detected.\n",
    "        if row['eye'] == 'L':\n",
    "            contralateral_eye_speed_profile = block.right_eye_data.query(\n",
    "                'ms_axis >= @current_timestamp - 51 and ms_axis <= @current_timestamp + 51'\n",
    "            )['angular_speed_r']\n",
    "        elif row['eye'] == 'R':\n",
    "            contralateral_eye_speed_profile = block.left_eye_data.query(\n",
    "                'ms_axis >= @current_timestamp - 51 and ms_axis <= @current_timestamp + 51'\n",
    "            )['angular_speed_r']\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Compute the maximum speeds (scaling as in the original function)\n",
    "        max_detected = np.nanmax(row['speed_profile_angular'] / 17) if len(row['speed_profile_angular']) > 0 else np.nan\n",
    "        max_contralateral = (np.nanmax(contralateral_eye_speed_profile.values / 17)\n",
    "                             if contralateral_eye_speed_profile.notna().sum() > 0 else np.nan)\n",
    "\n",
    "        # Append valid pairs.\n",
    "        if not np.isnan(max_detected) and not np.isnan(max_contralateral):\n",
    "            if row['eye'] == 'L':\n",
    "                left_eye_speeds.append(max_detected)\n",
    "                right_eye_speeds.append(max_contralateral)\n",
    "            elif row['eye'] == 'R':\n",
    "                right_eye_speeds.append(max_detected)\n",
    "                left_eye_speeds.append(max_contralateral)\n",
    "\n",
    "    # Convert lists to NumPy arrays.\n",
    "    right_eye_speeds = np.array(right_eye_speeds)\n",
    "    left_eye_speeds = np.array(left_eye_speeds)\n",
    "    print(\"Number of data points:\", len(right_eye_speeds))\n",
    "\n",
    "    # Outlier removal using IQR on the combined speeds.\n",
    "    all_speeds = np.concatenate([right_eye_speeds, left_eye_speeds])\n",
    "    q1, q3 = np.percentile(all_speeds, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - iqr_multiplier * iqr\n",
    "    upper_bound = q3 + iqr_multiplier * iqr\n",
    "\n",
    "    valid_indices = ((right_eye_speeds >= lower_bound) & (right_eye_speeds <= upper_bound) &\n",
    "                     (left_eye_speeds >= lower_bound) & (left_eye_speeds <= upper_bound))\n",
    "    right_eye_speeds = right_eye_speeds[valid_indices]\n",
    "    left_eye_speeds = left_eye_speeds[valid_indices]\n",
    "\n",
    "    # Fixed axis limits.\n",
    "    min_value = 0\n",
    "    max_value = 3\n",
    "\n",
    "    # Create the custom turbo colormap: get a discrete version of turbo and replace the first color with white.\n",
    "    n_colors = 256\n",
    "    turbo = plt.get_cmap('turbo', n_colors)\n",
    "    turbo_colors = turbo(np.linspace(0, 1, n_colors))\n",
    "    turbo_colors[0] = np.array([1, 1, 1, 1])  # Set the lowest count (0 probability) to perfect white.\n",
    "    custom_turbo = mcolors.ListedColormap(turbo_colors)\n",
    "\n",
    "    # Prepare the figure for the histogram.\n",
    "    fig, ax = plt.subplots(figsize=(2, 2), dpi=150)\n",
    "    xbins = np.linspace(min_value, max_value, bins)\n",
    "    ybins = np.linspace(min_value, max_value, bins)\n",
    "\n",
    "    # Compute the 2D histogram manually.\n",
    "    counts, xedges, yedges = np.histogram2d(right_eye_speeds, left_eye_speeds, bins=[xbins, ybins])\n",
    "    normalized_counts = counts / counts.sum()  # Normalize so that the overall sum equals 1.\n",
    "\n",
    "    # Plot the normalized histogram using pcolormesh.\n",
    "    pc = ax.pcolormesh(xedges, yedges, normalized_counts.T, cmap=custom_turbo,\n",
    "                         vmin=0, vmax=normalized_counts.max())\n",
    "\n",
    "    # Ensure equal axes.\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.set_xlim(min_value, max_value)\n",
    "    ax.set_ylim(min_value, max_value)\n",
    "\n",
    "    # Add a diagonal reference line.\n",
    "    ax.plot([min_value, max_value], [min_value, max_value], linestyle='--', color='gray', linewidth=1)\n",
    "\n",
    "    # Set axis labels.\n",
    "    ax.set_xlabel(\"Right max V [deg/ms]\", fontsize=10)\n",
    "    ax.set_ylabel(\"Left max V [deg/ms]\", fontsize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the histogram plot if export_path is provided.\n",
    "    if export_path:\n",
    "        export_dir = pathlib.Path(export_path)\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "        hist_pdf_path = export_dir / \"right_vs_left_hist2d.pdf\"\n",
    "        fig.savefig(hist_pdf_path, format='pdf', bbox_inches='tight')\n",
    "        print(f\"2D histogram saved to {hist_pdf_path}\")\n",
    "\n",
    "    # Create a separate figure for the colorbar.\n",
    "    vmin_val = 0\n",
    "    vmax_val = normalized_counts.max()\n",
    "    sm = plt.cm.ScalarMappable(cmap=custom_turbo, norm=plt.Normalize(vmin=vmin_val, vmax=vmax_val))\n",
    "    sm.set_array([])\n",
    "    fig2 = plt.figure(figsize=(1, 3), dpi=150)\n",
    "    cbar = fig2.colorbar(sm, orientation='vertical')\n",
    "    cbar.set_label(\"Probability\", fontsize=8)\n",
    "    cbar.ax.tick_params(labelsize=8)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the colorbar to a separate PDF if export_path is provided.\n",
    "    if export_path:\n",
    "        cbar_pdf_path = export_dir / \"right_vs_left_colorbar.pdf\"\n",
    "        fig2.savefig(cbar_pdf_path, format='pdf', bbox_inches='tight')\n",
    "        print(f\"Colorbar saved to {cbar_pdf_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "t = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "export_path = rf'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_3\\materials\\show_monocular_saccades\\{t}'\n",
    "plot_right_vs_left_eye_hist2d(all_saccade_collection, block_dict, export_path=export_path, iqr_multiplier=200, bins=250)\n"
   ],
   "id": "48ea1a6f859e8080",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "def plot_correlation_max_speed_vs_amplitude(\n",
    "    saccade_collection, sampling_rate=60, export_path=None, fig_size=(4,3)\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the correlation between the maximal angular speed and the overall amplitude of saccades.\n",
    "\n",
    "    For each saccade (row in the dataframe), the function computes the maximal value of the\n",
    "    'speed_profile_angular' array (converted from deg/frame to deg/ms using the provided sampling_rate)\n",
    "    and compares it to the 'magnitude_raw_angular' value.\n",
    "\n",
    "    A scatter plot is generated with a fitted regression line. The Pearson correlation coefficient\n",
    "    and its p-value are printed and also shown on the plot.\n",
    "\n",
    "    Parameters:\n",
    "      - saccade_collection: DataFrame containing at least the columns 'speed_profile_angular' (an array)\n",
    "                            and 'magnitude_raw_angular'.\n",
    "      - sampling_rate: Sampling rate in Hz (default 60 Hz) used to convert speeds from deg/frame to deg/ms.\n",
    "      - export_path: Optional directory path to export the plot and data.\n",
    "      - fig_size: Tuple specifying the size (width, height) of the figure.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import datetime\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.stats import pearsonr\n",
    "\n",
    "    # Compute frame duration (ms per frame)\n",
    "    frame_duration = 1000 / sampling_rate\n",
    "\n",
    "    max_speeds = []\n",
    "    amplitudes = []\n",
    "\n",
    "    # Iterate over rows to compute maximum speed and corresponding amplitude\n",
    "    for idx, row in saccade_collection.iterrows():\n",
    "        profile = row['speed_profile_angular']\n",
    "        amplitude = row['magnitude_raw_angular']\n",
    "        # Check for missing data or empty profiles\n",
    "        if profile is None or len(profile) == 0 or np.isnan(amplitude):\n",
    "            continue\n",
    "        # Convert profile to numpy array and convert to deg/ms\n",
    "        profile = np.array(profile)\n",
    "        # Compute maximum, ignoring any NaN values\n",
    "        max_speed = np.nanmax(profile / frame_duration)\n",
    "        max_speeds.append(max_speed)\n",
    "        amplitudes.append(amplitude)\n",
    "\n",
    "    max_speeds = np.array(max_speeds)\n",
    "    amplitudes = np.array(amplitudes)\n",
    "\n",
    "    if len(max_speeds) == 0:\n",
    "        print(\"No valid saccade entries to compute correlation.\")\n",
    "        return\n",
    "\n",
    "    # Compute Pearson correlation coefficient and p-value\n",
    "    corr_coef, p_value = pearsonr(max_speeds, amplitudes)\n",
    "\n",
    "    # Create scatter plot\n",
    "    fig, ax = plt.subplots(figsize=fig_size, dpi=300)\n",
    "    ax.scatter(max_speeds, amplitudes, s=10, color='blue', alpha=0.7)\n",
    "\n",
    "    # Compute and plot the regression line\n",
    "    slope, intercept = np.polyfit(max_speeds, amplitudes, 1)\n",
    "    x_vals = np.linspace(max_speeds.min(), max_speeds.max(), 100)\n",
    "    y_vals = slope * x_vals + intercept\n",
    "    ax.plot(x_vals, y_vals, color='red', lw=1)\n",
    "\n",
    "    # Label the plot\n",
    "    ax.set_xlabel('Max Angular Speed [deg/ms]', fontsize=10)\n",
    "    ax.set_ylabel('Overall Amplitude [deg]', fontsize=10)\n",
    "    ax.set_title('Correlation between Max Angular Speed and Overall Amplitude', fontsize=12)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "    # Display correlation stats on the plot\n",
    "    textstr = f'Pearson r = {corr_coef:.3f}\\nP-value = {p_value:.3e}'\n",
    "    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=8, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "\n",
    "    print(f\"Pearson correlation coefficient: {corr_coef:.3f}\")\n",
    "    print(f\"P-value: {p_value:.3e}\")\n",
    "\n",
    "    # Optionally export the plot and saccade data\n",
    "    if export_path is not None:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        full_export_path = os.path.join(export_path, f\"max_speed_vs_amplitude_{timestamp}\")\n",
    "        os.makedirs(full_export_path, exist_ok=True)\n",
    "        pdf_file = os.path.join(full_export_path, \"max_speed_vs_amplitude.pdf\")\n",
    "        fig.savefig(pdf_file, format='pdf', bbox_inches='tight', dpi=300)\n",
    "        data_file = os.path.join(full_export_path, \"saccade_data.pkl\")\n",
    "        with open(data_file, 'wb') as f:\n",
    "            pickle.dump(saccade_collection, f)\n",
    "        print(f\"Exported plot and data to: {full_export_path}\")\n",
    "\n",
    "    plt.show()\n",
    "export_path = r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_3\\materials\\correlation_plots'\n",
    "plot_correlation_max_speed_vs_amplitude(\n",
    "    saccade_collection, sampling_rate=60, export_path=None, fig_size=(4,3)\n",
    ")"
   ],
   "id": "1a480f0692fe3157",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "synced_saccade_collection.columns",
   "id": "7ba62f96306ed4a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def plot_max_speeds_scatter(all_saccade_collection, block_dict, export_path=None,iqr_multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Plots a scatter plot of maximum speeds of the left eye vs the right eye for all saccades.\n",
    "\n",
    "    Parameters:\n",
    "        all_saccade_collection (pd.DataFrame): DataFrame containing all saccade data.\n",
    "        block_dict (dict): Dictionary containing block objects with eye data.\n",
    "        export_path (str or pathlib.Path, optional): Path to save the plot. Defaults to None.\n",
    "    \"\"\"\n",
    "    left_eye_max_speeds = []\n",
    "    right_eye_max_speeds = []\n",
    "\n",
    "    for _, row in all_saccade_collection.iterrows():\n",
    "        block_num = row['block']\n",
    "        start_ts = row['saccade_on_ms']\n",
    "\n",
    "        block_key = f\"{row['animal']}_block_{block_num}\"\n",
    "        if block_key not in block_dict:\n",
    "            continue\n",
    "\n",
    "        block = block_dict[block_key]\n",
    "\n",
    "        # Get speed data for the left and right eyes\n",
    "        left_eye_data = block.left_eye_data.query(\n",
    "            'ms_axis >= @start_ts - 100 and ms_axis <= @start_ts + 100')['speed_r'].values / 17\n",
    "        right_eye_data = block.right_eye_data.query(\n",
    "            'ms_axis >= @start_ts - 100 and ms_axis <= @start_ts + 100')['speed_r'].values / 17\n",
    "\n",
    "        if len(left_eye_data) == 0 or len(right_eye_data) == 0:\n",
    "            continue\n",
    "        # Append maximum speeds only if valid data is available\n",
    "        if not np.all(np.isnan(left_eye_data)):\n",
    "            left_eye_max_speeds.append(np.nanmax(left_eye_data))\n",
    "        else:\n",
    "            left_eye_max_speeds.append(0)  # Or any other default value, like np.nan\n",
    "\n",
    "        if not np.all(np.isnan(right_eye_data)):\n",
    "            right_eye_max_speeds.append(np.nanmax(right_eye_data))\n",
    "        else:\n",
    "            right_eye_max_speeds.append(0)  # Or any other default value, like np.nan\n",
    "\n",
    "    # Convert to NumPy arrays for easier filtering\n",
    "    left_eye_max_speeds = np.array(left_eye_max_speeds)\n",
    "    right_eye_max_speeds = np.array(right_eye_max_speeds)\n",
    "\n",
    "    # Outlier removal using the IQR method\n",
    "    all_speeds = np.concatenate([left_eye_max_speeds, right_eye_max_speeds])\n",
    "    q1, q3 = np.percentile(all_speeds, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - iqr_multiplier * iqr\n",
    "    upper_bound = q3 + iqr_multiplier * iqr\n",
    "\n",
    "    # Filter points within bounds\n",
    "    valid_indices = (left_eye_max_speeds >= lower_bound) & (left_eye_max_speeds <= upper_bound) & \\\n",
    "                    (right_eye_max_speeds >= lower_bound) & (right_eye_max_speeds <= upper_bound)\n",
    "\n",
    "    left_eye_max_speeds = left_eye_max_speeds[valid_indices]\n",
    "    right_eye_max_speeds = right_eye_max_speeds[valid_indices]\n",
    "\n",
    "    # Create scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(4, 4), dpi=150)\n",
    "    ax.scatter(left_eye_max_speeds, right_eye_max_speeds, alpha=0.7, s=2, c='k', edgecolors='none')\n",
    "\n",
    "    # Plot diagonal reference line\n",
    "    max_val = max(max(left_eye_max_speeds, default=0), max(right_eye_max_speeds, default=0))\n",
    "    ax.plot([0, max_val], [0, max_val], linestyle='--', color='gray', linewidth=1)\n",
    "\n",
    "    # Configure plot appearance\n",
    "    ax.set_title(\"saccade max speed correlation\", fontsize=10)\n",
    "    ax.set_xlabel(\"Max Speed (Left Eye, pix/ms)\", fontsize=10)\n",
    "    ax.set_ylabel(\"Max Speed (Right Eye, pix/ms)\", fontsize=10)\n",
    "    ax.grid(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "    ax.set_xlim(0,3)\n",
    "    ax.set_ylim(0,3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if export_path is provided\n",
    "    if export_path:\n",
    "        export_dir = pathlib.Path(export_path)\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "        pdf_path = export_dir / \"max_speeds_scatter.pdf\"\n",
    "        fig.savefig(pdf_path, format='pdf', bbox_inches='tight')\n",
    "        print(f\"Scatter plot saved to {pdf_path}\")\n",
    "\n",
    "    plt.show()\n",
    "export_path = r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\temporary_drafts\\saccade_speed_correlation_all'\n",
    "plot_max_speeds_scatter(all_saccade_collection, block_dict, iqr_multiplier=4, export_path=None)"
   ],
   "id": "ad6b00e4a839ac54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def create_saccade_polar_histogram_multianimal(saccade_collection, figure_size=(8, 4), export_path=None):\n",
    "    # Extract unique animals\n",
    "    animals = saccade_collection['animal'].unique()\n",
    "    num_bins = 36\n",
    "\n",
    "    # Set up the polar plots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=figure_size, dpi=150, subplot_kw=dict(projection='polar'))\n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.5, len(animals)))  # Unique colors for each animal\n",
    "\n",
    "    # Iterate through each animal\n",
    "    for color, animal in zip(colors, animals):\n",
    "        # Filter saccades for the current animal\n",
    "        animal_saccades = saccade_collection.query('animal == @animal')\n",
    "\n",
    "        # Process right eye\n",
    "        saccade_angles_r = animal_saccades.query('eye == \"R\"')['overall_angle_deg']\n",
    "        hist_r, bin_edges_r = np.histogram(saccade_angles_r, bins=num_bins, range=(0, 360), density=True)\n",
    "        bin_centers_r = list((bin_edges_r[:-1] + bin_edges_r[1:]) / 2)\n",
    "        # Close the circular plot\n",
    "        bin_centers_r.append(bin_centers_r[0])\n",
    "        hist_r = np.append(hist_r, hist_r[0])\n",
    "        axs[0].plot(\n",
    "            np.deg2rad(bin_centers_r), hist_r,\n",
    "            label=f'{animal} saccades)',\n",
    "            color=color\n",
    "        )\n",
    "\n",
    "        # Process left eye\n",
    "        saccade_angles_l = animal_saccades.query('eye == \"L\"')['overall_angle_deg']\n",
    "        hist_l, bin_edges_l = np.histogram(saccade_angles_l, bins=num_bins, range=(0, 360), density=True)\n",
    "        bin_centers_l = list((bin_edges_l[:-1] + bin_edges_l[1:]) / 2)\n",
    "        # Close the circular plot\n",
    "        bin_centers_l.append(bin_centers_l[0])\n",
    "        hist_l = np.append(hist_l, hist_l[0])\n",
    "        axs[1].plot(\n",
    "            np.deg2rad(bin_centers_l), hist_l,\n",
    "            label=f'{animal}',\n",
    "            color=color\n",
    "        )\n",
    "\n",
    "    # Add shaded areas for probability = 0.1\n",
    "    for ax in axs:\n",
    "        theta = np.linspace(0, 2 * np.pi, 500)\n",
    "        ax.fill_between(theta, 0, 0.005, color='gray', alpha=0.2, zorder=0, label='0.005 Prob.')\n",
    "\n",
    "    # Format plots\n",
    "    for ax, title in zip(axs, ['Right Eye', 'Left Eye']):\n",
    "        if title == 'Left Eye':\n",
    "            ax.legend(loc='upper right', bbox_to_anchor=(1, 1), fontsize=6)\n",
    "        else:\n",
    "            ax.legend().remove()  # Remove legend for the right eye plot\n",
    "        ax.grid(False)\n",
    "        ax.set_yticks([])\n",
    "        # Set font size for polar tick markers\n",
    "        ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure if export_path is provided\n",
    "    if export_path:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "        export_dir = pathlib.Path(export_path) / f'{timestamp}_MultiAnimal_Saccade_Histogram'\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save PDF\n",
    "        export_file_pdf = export_dir / f'{timestamp}_saccade_angles_histogram.pdf'\n",
    "        plt.savefig(export_file_pdf, format='pdf')\n",
    "        print(f\"Exported histogram to {export_file_pdf}\")\n",
    "\n",
    "        # Save Pickle\n",
    "        export_file_pkl = export_dir / f'{timestamp}_saccade_angles_data.pkl'\n",
    "        with open(export_file_pkl, 'wb') as pkl_file:\n",
    "            pickle.dump(saccade_collection, pkl_file)\n",
    "        print(f\"Exported data to {export_file_pkl}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "b14ffb83da1ca324",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "export_path = pathlib.Path(r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_3\\materials\\synced_saccade_polar_multi_animal')\n",
    "create_saccade_polar_histogram_multianimal(non_synced_saccade_collection.query('length > 2 and magnitude_raw_angular > 4'), (3,2),export_path=None)"
   ],
   "id": "1f7a1ec15d3774a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "all_saccade_collection.head()",
   "id": "523b6fdf4eb92b08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "all_saccade_collection.iloc[0]",
   "id": "48181cc8abeba35e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "plt.hist(all_saccade_collection.overall_angle_deg)",
   "id": "9da6c111cc979063",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "# overall saccade magnitude histogram\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def plot_averaged_saccade_amplitude_distribution_angle(\n",
    "    synced_saccade_collection, non_synced_saccade_collection, figure_size=(2.5, 1.7), export_path=None,\n",
    "    bins=np.linspace(0,5,40)\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the average angular saccade amplitude likelihood histogram across animals,\n",
    "    showing only the outline (with SEM shading) of the distributions.\n",
    "\n",
    "    For each saccade event, the angular amplitude is computed as:\n",
    "\n",
    "        angular_amplitude [deg] = sqrt( (angle_dx)^2 + (angle_dy)^2 )\n",
    "\n",
    "    Parameters:\n",
    "    - synced_saccade_collection (pd.DataFrame): DataFrame containing synced saccade data with an 'animal' column,\n",
    "          and the angular delta columns 'angle_dx' and 'angle_dy'.\n",
    "    - non_synced_saccade_collection (pd.DataFrame): DataFrame containing non-synced saccade data with an 'animal' column,\n",
    "          and the angular delta columns.\n",
    "    - figure_size (tuple): Size of the figure for the plot.\n",
    "    - export_path (str or pathlib.Path, optional): Path to save the plot and data.\n",
    "    - bins (array-like): Bin edges for the histogram (default spans 0 to 5 deg).\n",
    "    \"\"\"\n",
    "    # Compute angular amplitude for each event (if not already computed)\n",
    "    # We'll add a new column \"angular_magnitude\" computed from angle_dx and angle_dy.\n",
    "    for df in [synced_saccade_collection, non_synced_saccade_collection]:\n",
    "        if 'magnitude_raw_angular' not in df.columns:\n",
    "            df['magnitude_raw_angular'] = np.abs(df['delta_phi']) + np.abs(df['delta_theta'])\n",
    "\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2  # For plotting the outline\n",
    "\n",
    "    # Group by animals\n",
    "    animals = synced_saccade_collection['animal'].unique()\n",
    "    num_animals = len(animals)\n",
    "\n",
    "    # Store normalized histograms for synced and non-synced data\n",
    "    synced_likelihoods = []\n",
    "    non_synced_likelihoods = []\n",
    "\n",
    "    for animal in animals:\n",
    "        # For each animal, filter the angular amplitudes\n",
    "        synced_data = synced_saccade_collection.query(\"animal == @animal\")['magnitude_raw_angular'].values\n",
    "        non_synced_data = non_synced_saccade_collection.query(\"animal == @animal\")['magnitude_raw_angular'].values\n",
    "\n",
    "        # Calculate histograms normalized to probabilities\n",
    "        synced_hist, _ = np.histogram(synced_data, bins=bins)\n",
    "        non_synced_hist, _ = np.histogram(non_synced_data, bins=bins)\n",
    "        if synced_hist.sum() > 0:\n",
    "            synced_hist = synced_hist / synced_hist.sum()\n",
    "        if non_synced_hist.sum() > 0:\n",
    "            non_synced_hist = non_synced_hist / non_synced_hist.sum()\n",
    "\n",
    "        synced_likelihoods.append(synced_hist)\n",
    "        non_synced_likelihoods.append(non_synced_hist)\n",
    "\n",
    "    # Convert to arrays for averaging\n",
    "    synced_likelihoods = np.array(synced_likelihoods)\n",
    "    non_synced_likelihoods = np.array(non_synced_likelihoods)\n",
    "    print(np.shape(synced_likelihoods))\n",
    "\n",
    "    # Calculate mean and SEM across animals\n",
    "    synced_mean = synced_likelihoods.mean(axis=0)\n",
    "    synced_sem = synced_likelihoods.std(axis=0) / np.sqrt(num_animals)\n",
    "\n",
    "    non_synced_mean = non_synced_likelihoods.mean(axis=0)\n",
    "    non_synced_sem = non_synced_likelihoods.std(axis=0) / np.sqrt(num_animals)\n",
    "\n",
    "    # Plot averaged histograms\n",
    "    fig, ax = plt.subplots(figsize=figure_size, dpi=300)\n",
    "\n",
    "    # Plot outline and SEM for synced (angle-based)\n",
    "    ax.plot(bin_centers, synced_mean, color='green', linestyle='-', linewidth=1.5, label='Synchronized')\n",
    "    ax.fill_between(bin_centers, synced_mean - synced_sem, synced_mean + synced_sem, color='green', alpha=0.3)\n",
    "\n",
    "    # Plot outline and SEM for non-synced (angle-based)\n",
    "    ax.plot(bin_centers, non_synced_mean, color='blue', linestyle='-', linewidth=1.5, label='Monocular')\n",
    "    ax.fill_between(bin_centers, non_synced_mean - non_synced_sem, non_synced_mean + non_synced_sem, color='blue', alpha=0.3)\n",
    "\n",
    "    # Customize the plot\n",
    "    ax.set_title('Average Angular Saccade Amplitude Distribution', fontsize=12)\n",
    "    ax.set_xlabel('Saccade Amplitude [deg]', fontsize=10)\n",
    "    ax.set_ylabel('Probability', fontsize=10)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "    ax.set_xlim(left=0)\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "    # Handle export\n",
    "    if export_path is not None:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        full_export_path = os.path.join(export_path, f\"saccade_amplitude_averaged_histogram_angle_{timestamp}\")\n",
    "        os.makedirs(full_export_path, exist_ok=True)\n",
    "        pdf_file = os.path.join(full_export_path, \"saccade_amplitude_averaged_histogram_angle.pdf\")\n",
    "        fig.savefig(pdf_file, format='pdf', bbox_inches='tight', dpi=300)\n",
    "        data_file = os.path.join(full_export_path, \"averaged_saccade_amplitude_angle_data.pkl\")\n",
    "        with open(data_file, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'synced_mean': synced_mean,\n",
    "                'synced_sem': synced_sem,\n",
    "                'non_synced_mean': non_synced_mean,\n",
    "                'non_synced_sem': non_synced_sem,\n",
    "                'bins': bins,\n",
    "            }, f)\n",
    "        print(f\"Exported plot and data to: {full_export_path}\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    print()\n",
    "\n",
    "export_path = r\"Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_3\\materials\\saccade_amp_hist_deg\"\n",
    "\n",
    "plot_averaged_saccade_amplitude_distribution_angle(\n",
    "    synced_saccade_collection, non_synced_saccade_collection, figure_size=(1.7, 1.2), export_path=None,\n",
    "    bins=np.linspace(0,50,40))"
   ],
   "id": "346978973790d0d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "synced_saccade_collection.columns",
   "id": "5fc65e3ede3791f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "def plot_peak_velocity_scatter(\n",
    "    saccade_collection, sampling_rate=60, example_animal=None,\n",
    "    export_path=None, fig_size=(3,2), set_ylim=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a scatter plot of peak angular velocities (converted from deg/frame to deg/ms)\n",
    "    for each animal in the saccade collection.\n",
    "\n",
    "    For each animal, it:\n",
    "      - Computes the peak velocity from each speed profile in 'speed_profile_angular'\n",
    "      - Converts the value from deg/frame to deg/ms using the provided sampling_rate\n",
    "      - Computes the mean peak velocity and its standard error (SEM)\n",
    "\n",
    "    The function then plots:\n",
    "      - All individual peak values as semi-transparent grey dots (with slight x-axis jitter)\n",
    "      - The mean peak velocity for each animal with vertical error bars (SEM)\n",
    "      - Optionally highlights a provided example animal in red.\n",
    "\n",
    "    Parameters:\n",
    "      - saccade_collection: DataFrame with at least the following columns:\n",
    "          'animal' and 'speed_profile_angular' (a list/array of angular speed values in deg/frame)\n",
    "      - sampling_rate: Sampling rate in Hz (default 60 Hz).\n",
    "      - example_animal: Optional string specifying an animal to highlight.\n",
    "      - export_path: Optional directory path to export the plot as a PDF.\n",
    "      - fig_size: Tuple defining the figure size.\n",
    "      - set_ylim: Optional y-axis limits.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import datetime, os\n",
    "\n",
    "    # Calculate frame duration in ms (used to convert deg/frame to deg/ms)\n",
    "    frame_duration = 1000 / sampling_rate\n",
    "\n",
    "    # Get sorted unique animal names\n",
    "    animals = sorted(saccade_collection['animal'].dropna().unique())\n",
    "\n",
    "    # Prepare dictionaries to store peak values, means and SEMs for each animal\n",
    "    all_peak_values = {}\n",
    "    peak_means = []\n",
    "    peak_stderr = []\n",
    "\n",
    "    # Loop over each animal\n",
    "    for animal in animals:\n",
    "        # Get the group's data\n",
    "        group = saccade_collection[saccade_collection['animal'] == animal]\n",
    "        peaks = []\n",
    "\n",
    "        # Iterate over each row (saccade) in the group\n",
    "        for _, row in group.iterrows():\n",
    "            profile = row['speed_profile_angular']\n",
    "            # Skip if the profile is missing or empty\n",
    "            if profile is None or (hasattr(profile, '__len__') and len(profile) == 0):\n",
    "                continue\n",
    "            # Ensure the profile is a NumPy array\n",
    "            profile = np.array(profile)\n",
    "            # Compute the peak velocity in deg/ms: max(angular speed) / (ms/frame)\n",
    "            peak = np.max(profile) / frame_duration\n",
    "            peaks.append(peak)\n",
    "\n",
    "        # Compute mean and standard error if any peaks were found\n",
    "        if len(peaks) > 0:\n",
    "            mean_peak = np.mean(peaks)\n",
    "            std_peak = np.std(peaks)\n",
    "            stderr_peak = std_peak / np.sqrt(len(peaks))\n",
    "        else:\n",
    "            mean_peak = np.nan\n",
    "            stderr_peak = np.nan\n",
    "\n",
    "        all_peak_values[animal] = peaks\n",
    "        peak_means.append(mean_peak)\n",
    "        peak_stderr.append(stderr_peak)\n",
    "\n",
    "    # Create the scatter plot\n",
    "    fig, ax = plt.subplots(figsize=fig_size, dpi=300)\n",
    "    x_positions = np.arange(len(animals))\n",
    "\n",
    "    # Plot individual peak velocity values with a slight horizontal jitter for clarity.\n",
    "    for i, animal in enumerate(animals):\n",
    "        points = all_peak_values[animal]\n",
    "        if len(points) > 0:\n",
    "            # Add small jitter to spread points out\n",
    "            jitter = np.random.normal(0, 0.04, size=len(points))\n",
    "            ax.scatter(np.full(len(points), x_positions[i]) + jitter, points,\n",
    "                       color='grey', alpha=0.6, s=10)\n",
    "\n",
    "    # Plot mean peak velocity with SEM error bars\n",
    "    ax.errorbar(x_positions, peak_means, yerr=peak_stderr, fmt='o',\n",
    "                color='black', capsize=3, markersize=5, label='Mean ± SEM')\n",
    "\n",
    "    # Optionally highlight the example animal in a different color\n",
    "    if example_animal is not None and example_animal in animals:\n",
    "        idx = animals.index(example_animal)\n",
    "        ax.errorbar(x_positions[idx], peak_means[idx], yerr=peak_stderr[idx],\n",
    "                    fmt='o', color='red', capsize=5, markersize=7, label=f'Example: {example_animal}')\n",
    "\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(animals, rotation=45, fontsize=8)\n",
    "    ax.set_xlabel('Animal', fontsize=8)\n",
    "    ax.set_ylabel('Peak Angular Velocity [deg/ms]', fontsize=8)\n",
    "    if set_ylim is not None:\n",
    "        ax.set_ylim(set_ylim)\n",
    "    ax.tick_params(axis='both', labelsize=8)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.legend(fontsize=6)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Export the plot if an export path is provided\n",
    "    if export_path is not None:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        full_export_path = os.path.join(export_path, f\"peak_velocity_scatter_{timestamp}\")\n",
    "        os.makedirs(full_export_path, exist_ok=True)\n",
    "        pdf_file = os.path.join(full_export_path, \"peak_velocity_scatter.pdf\")\n",
    "        fig.savefig(pdf_file, format='pdf', bbox_inches='tight', dpi=300)\n",
    "        print(f\"Exported plot to: {full_export_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_peak_velocity_scatter(all_saccade_collection,sampling_rate=60,example_animal=\"PV_126\",export_path=None,fig_size=(3,2))"
   ],
   "id": "e96affe8dae40ca5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "def plot_peak_velocity_vs_time(\n",
    "    saccade_collection, sampling_rate=60, floor=None, ceiling=None,\n",
    "    export_path=None, fig_size=(2.5,1.7), set_xlim=None, example_animal=None, set_ylim=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes and plots the peak angular velocity (in deg/ms) for each saccade,\n",
    "    grouped by time-to-peak (in ms) and by animal.\n",
    "\n",
    "    For each saccade:\n",
    "      - The peak velocity is computed as the maximum value in 'speed_profile_angular'\n",
    "        (which is in deg/frame) converted to deg/ms using the formula:\n",
    "            peak_velocity = max(speed_profile_angular) * (sampling_rate / 1000)\n",
    "\n",
    "    Then the data is grouped by 'time_to_peak_v' (already in ms) and 'animal', so that\n",
    "    each animal contributes one average peak velocity per time-to-peak value.\n",
    "\n",
    "    The function creates a scatter plot where:\n",
    "      - The x-axis is the time-to-peak (ms).\n",
    "      - Each unique time value has as many points as the number of animals with data for that time.\n",
    "      - A black error bar represents the mean ± SEM across animals for each time value.\n",
    "      - Optionally, one animal can be highlighted in red.\n",
    "\n",
    "    Parameters:\n",
    "      - saccade_collection: DataFrame that must include:\n",
    "            * 'time_to_peak_v': time-to-peak in ms.\n",
    "            * 'speed_profile_angular': a list/array of angular speeds (in deg/frame).\n",
    "            * 'animal': animal identifier.\n",
    "      - sampling_rate: Sampling rate in Hz (default 60). Used to convert deg/frame to deg/ms.\n",
    "      - floor: Minimum time-to-peak value (ms) to include.\n",
    "      - ceiling: Maximum time-to-peak value (ms) to include.\n",
    "      - export_path: Optional directory to export the plot as a PDF.\n",
    "      - fig_size: Figure size tuple.\n",
    "      - set_xlim: Optional x-axis limits.\n",
    "      - example_animal: Optional string specifying an animal to highlight.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import datetime, os\n",
    "\n",
    "    # Filter the dataframe for rows with valid time-to-peak and speed profile\n",
    "    df = saccade_collection.dropna(subset=['time_to_peak_v', 'speed_profile_angular']).copy()\n",
    "\n",
    "    # Apply floor and ceiling filters if provided (time-to-peak is in ms)\n",
    "    if floor is not None:\n",
    "        df = df[df['time_to_peak_v'] >= floor]\n",
    "    if ceiling is not None:\n",
    "        df = df[df['time_to_peak_v'] <= ceiling]\n",
    "\n",
    "    # Compute the peak velocity for each saccade.\n",
    "    # Convert from deg/frame to deg/ms by multiplying with (sampling_rate / 1000)\n",
    "    def compute_peak(row):\n",
    "        profile = np.array(row['speed_profile_angular'])\n",
    "        return np.max(profile) * (sampling_rate / 1000)\n",
    "\n",
    "    df['peak_velocity'] = df.apply(compute_peak, axis=1)\n",
    "\n",
    "    # Group by time-to-peak and animal, so that each animal contributes one (average) peak value per time value.\n",
    "    grouped = df.groupby(['time_to_peak_v', 'animal'])['peak_velocity'].mean().reset_index()\n",
    "\n",
    "    # For each unique time-to-peak, compute the overall mean and SEM across animals.\n",
    "    summary = grouped.groupby('time_to_peak_v').agg(\n",
    "        mean_peak=('peak_velocity', 'mean'),\n",
    "        sem_peak=('peak_velocity', lambda x: np.std(x, ddof=1)/np.sqrt(len(x)) if len(x) > 1 else 0)\n",
    "    ).reset_index()\n",
    "\n",
    "    # Create the scatter plot with x-axis in ms.\n",
    "    fig, ax = plt.subplots(figsize=fig_size, dpi=300)\n",
    "\n",
    "    # Plot individual animal points (with slight jitter along the x-axis for clarity).\n",
    "    for animal in grouped['animal'].unique():\n",
    "        animal_data = grouped[grouped['animal'] == animal]\n",
    "        jitter = np.random.normal(0, 0.5, size=len(animal_data))\n",
    "        ax.scatter(animal_data['time_to_peak_v'] + jitter, animal_data['peak_velocity'],\n",
    "                   s=5, alpha=0.7, label=animal)\n",
    "\n",
    "    # # Plot the overall mean ± SEM for each time-to-peak value.\n",
    "    # ax.errorbar(summary['time_to_peak_v'], summary['mean_peak'], yerr=summary['sem_peak'],\n",
    "    #             fmt='o', color='black', capsize=3, markersize=5, label='Mean ± SEM')\n",
    "    #\n",
    "    # Optionally, highlight a specific animal in red.\n",
    "    if example_animal is not None:\n",
    "        if example_animal in grouped['animal'].unique():\n",
    "            animal_data = grouped[grouped['animal'] == example_animal]\n",
    "            jitter = np.random.normal(0, 0.5, size=len(animal_data))\n",
    "            ax.scatter(animal_data['time_to_peak_v'] + jitter, animal_data['peak_velocity'],\n",
    "                       s=5, color='red', alpha=0.9, label=f'Example: {example_animal}')\n",
    "\n",
    "    ax.set_xlabel('TTP [ms]', fontsize=8)\n",
    "    ax.set_ylabel('Peak V [deg/ms]', fontsize=8)\n",
    "    if set_xlim is not None:\n",
    "        ax.set_xlim(set_xlim)\n",
    "    ax.tick_params(axis='both', labelsize=8)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    if set_ylim is not None:\n",
    "        ax.set_ylim(set_ylim)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    ax.legend(fontsize=6)\n",
    "\n",
    "    # Optionally export the plot as a PDF.\n",
    "    if export_path is not None:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        full_export_path = os.path.join(export_path, f\"peak_velocity_vs_time_{timestamp}\")\n",
    "        os.makedirs(full_export_path, exist_ok=True)\n",
    "        pdf_file = os.path.join(full_export_path, \"peak_velocity_vs_time.pdf\")\n",
    "        fig.savefig(pdf_file, format='pdf', bbox_inches='tight', dpi=300)\n",
    "        print(f\"Exported plot to: {full_export_path}\")\n",
    "\n",
    "    plt.show()\n",
    "export_path = r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_3\\materials\\ttp_profiles\\inset'\n",
    "plot_peak_velocity_vs_time(all_saccade_collection.query('time_to_peak_v > 0 and time_to_peak_v < 67'), sampling_rate=60, floor=1, ceiling=86,\n",
    "    export_path=export_path, fig_size=(1.2,1.5), set_xlim=None, example_animal=None)"
   ],
   "id": "cf42b06101bbd355",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "def plot_difference_saccade_amplitude_distribution_all_animals(\n",
    "    synced_saccade_collection, non_synced_saccade_collection, figure_size=(2, 1.5), export_path=None,\n",
    "    bins=np.linspace(0,5,40)\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the difference (synchronized minus monocular) of the normalized saccade amplitude histograms\n",
    "    for each animal on a single plot. Each animal's trace is given a unique color.\n",
    "\n",
    "    Parameters:\n",
    "    - synced_saccade_collection (pd.DataFrame): DataFrame containing synchronized saccade data with an 'animal' column,\n",
    "          and angular data (e.g. 'delta_phi' and 'delta_theta').\n",
    "    - non_synced_saccade_collection (pd.DataFrame): DataFrame containing non-synchronized saccade data with an 'animal' column.\n",
    "    - figure_size (tuple): Size of the figure.\n",
    "    - export_path (str or pathlib.Path, optional): Directory where the plot and data will be saved.\n",
    "    - bins (array-like): Bin edges for the histogram (default spans 0 to 5 deg).\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import datetime, os, pickle\n",
    "\n",
    "    # Make sure both DataFrames have the computed angular amplitude column.\n",
    "    for df in [synced_saccade_collection, non_synced_saccade_collection]:\n",
    "        if 'magnitude_raw_angular' not in df.columns:\n",
    "            df['magnitude_raw_angular'] = np.abs(df['delta_phi']) + np.abs(df['delta_theta'])\n",
    "\n",
    "    # Compute bin centers for plotting.\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    # Retrieve unique animals.\n",
    "    animals = synced_saccade_collection['animal'].unique()\n",
    "\n",
    "    # Set up a single figure and axes.\n",
    "    fig, ax = plt.subplots(figsize=figure_size, dpi=300)\n",
    "\n",
    "    # Get the default color cycle from matplotlib.\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "    # Loop over each animal and plot its difference trace.\n",
    "    for i, animal in enumerate(animals):\n",
    "        # Filter data for the current animal.\n",
    "        synced_data = synced_saccade_collection.query(\"animal == @animal\")['magnitude_raw_angular'].values\n",
    "        non_synced_data = non_synced_saccade_collection.query(\"animal == @animal\")['magnitude_raw_angular'].values\n",
    "\n",
    "        # Compute histograms.\n",
    "        synced_hist, _ = np.histogram(synced_data, bins=bins)\n",
    "        non_synced_hist, _ = np.histogram(non_synced_data, bins=bins)\n",
    "\n",
    "        # Normalize the histograms.\n",
    "        if synced_hist.sum() > 0:\n",
    "            synced_hist = synced_hist / synced_hist.sum()\n",
    "        if non_synced_hist.sum() > 0:\n",
    "            non_synced_hist = non_synced_hist / non_synced_hist.sum()\n",
    "\n",
    "        # Compute the difference: (synchronized minus monocular).\n",
    "        diff_trace = synced_hist - non_synced_hist\n",
    "\n",
    "        # Plot the trace using a color from the cycle.\n",
    "        ax.plot(bin_centers, diff_trace, color=colors[i % len(colors)], linestyle='-',\n",
    "                linewidth=1.5, label=f'{animal}')\n",
    "\n",
    "    # Add a horizontal reference line at zero.\n",
    "    ax.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Customize the plot.\n",
    "    #ax.set_title('Difference in Saccade Amplitude Distribution\\n(Synchronized - Monocular)', fontsize=12)\n",
    "    ax.set_xlabel('Amplitude [deg]', fontsize=10)\n",
    "    ax.set_ylabel('Diff [probability]', fontsize=10)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "    ax.set_xlim(left=0)\n",
    "\n",
    "    # Optionally export the plot and data.\n",
    "    if export_path is not None:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        full_export_path = os.path.join(export_path, f\"saccade_amplitude_difference_all_animals_{timestamp}\")\n",
    "        os.makedirs(full_export_path, exist_ok=True)\n",
    "        pdf_file = os.path.join(full_export_path, \"saccade_amplitude_difference_all_animals.pdf\")\n",
    "        fig.savefig(pdf_file, format='pdf', bbox_inches='tight', dpi=300)\n",
    "        data_file = os.path.join(full_export_path, \"saccade_amplitude_difference_all_animals_data.pkl\")\n",
    "        # Save each animal's difference trace keyed by animal name.\n",
    "        data_dict = {}\n",
    "        for animal in animals:\n",
    "            synced_data = synced_saccade_collection.query(\"animal == @animal\")['magnitude_raw_angular'].values\n",
    "            non_synced_data = non_synced_saccade_collection.query(\"animal == @animal\")['magnitude_raw_angular'].values\n",
    "            synced_hist, _ = np.histogram(synced_data, bins=bins)\n",
    "            non_synced_hist, _ = np.histogram(non_synced_data, bins=bins)\n",
    "            if synced_hist.sum() > 0:\n",
    "                synced_hist = synced_hist / synced_hist.sum()\n",
    "            if non_synced_hist.sum() > 0:\n",
    "                non_synced_hist = non_synced_hist / non_synced_hist.sum()\n",
    "            data_dict[animal] = synced_hist - non_synced_hist\n",
    "        with open(data_file, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'animal_diff_traces': data_dict,\n",
    "                'bins': bins,\n",
    "            }, f)\n",
    "        print(f\"Exported plot and data to: {full_export_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "export_path = r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_3\\materials\\saccade_amp_hist_deg\\diff_per_animal'\n",
    "plot_difference_saccade_amplitude_distribution_all_animals(synced_saccade_collection,\n",
    "                                                          non_synced_saccade_collection,\n",
    "                                                          figure_size=(2, 1.5),\n",
    "                                                          export_path=None,\n",
    "                                                          bins=np.linspace(0,50,35))\n"
   ],
   "id": "888e88ef83c0760f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "np.linspace(0,50,40)",
   "id": "b319846e78efeaf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "block = block_collection[15]\n",
   "id": "6022f40a4777f2d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "def pick_reference_opencv(self, eye):\n",
    "    \"\"\"\n",
    "    Display an image with candidate points overlaid as a continuous gradient\n",
    "    (using COLORMAP_TURBO) according to their ellipse ratio (major_ax/minor_ax)\n",
    "    from the rotated eye data. A background frame is shown (using a slider to\n",
    "    change the frame), candidate points are overlaid, and an extrapolated\n",
    "    reference is drawn. The user may click on the main image (excluding the\n",
    "    colorbar) to select a final reference point.\n",
    "\n",
    "    :param eye: 'left' or 'right'\n",
    "    :return: Tuple (ref_x, ref_y) representing the selected reference coordinates,\n",
    "             or None if canceled.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from scipy.optimize import minimize\n",
    "    from scipy.interpolate import Rbf\n",
    "\n",
    "    # --- 1. Select and clean the data ---\n",
    "    if eye == 'left':\n",
    "        df = self.left_eye_data.copy()\n",
    "    elif eye == 'right':\n",
    "        df = self.right_eye_data.copy()\n",
    "    else:\n",
    "        print(\"Eye not recognized. Choose 'left' or 'right'.\")\n",
    "        return None\n",
    "\n",
    "    # Drop rows with missing center coordinates.\n",
    "    df = df.dropna(subset=['center_x', 'center_y'])\n",
    "\n",
    "    # Ensure the 'ratio' column exists (ratio = major_ax / minor_ax).\n",
    "    if 'ratio' not in df.columns:\n",
    "        df['ratio'] = df['major_ax'] / df['minor_ax']\n",
    "\n",
    "    # Determine ratio range.\n",
    "    min_ratio = df['ratio'].min()\n",
    "    max_ratio = df['ratio'].max()\n",
    "\n",
    "    # --- 2. Extrapolate the ideal reference point via RBF regression on a stratified subset ---\n",
    "    x_data = df['center_x'].values\n",
    "    y_data = df['center_y'].values\n",
    "    ratio_data = df['ratio'].values\n",
    "\n",
    "    # Stratified sampling: split the ratio range into bins and sample up to n_per_bin points.\n",
    "    n_bins = 10\n",
    "    n_per_bin = 50\n",
    "    subset_indices = []\n",
    "    bin_edges = np.linspace(min_ratio, max_ratio, n_bins + 1)\n",
    "    for i in range(n_bins):\n",
    "        indices = np.where((ratio_data >= bin_edges[i]) & (ratio_data < bin_edges[i+1]))[0]\n",
    "        if len(indices) > 0:\n",
    "            n_select = min(n_per_bin, len(indices))\n",
    "            selected = np.random.choice(indices, n_select, replace=False)\n",
    "            subset_indices.extend(selected)\n",
    "    subset_indices = np.array(subset_indices)\n",
    "\n",
    "    if len(subset_indices) == 0:\n",
    "        print(\"No data available for regression.\")\n",
    "        return None\n",
    "\n",
    "    # Build the subset.\n",
    "    x_subset = x_data[subset_indices]\n",
    "    y_subset = y_data[subset_indices]\n",
    "    ratio_subset = ratio_data[subset_indices]\n",
    "\n",
    "    # Create the RBF interpolator.\n",
    "    rbf = Rbf(x_subset, y_subset, ratio_subset, function='multiquadric', smooth=1)\n",
    "\n",
    "    # Define an objective function: squared difference from 1.\n",
    "    def objective(p):\n",
    "        return (rbf(p[0], p[1]) - 1) ** 2\n",
    "\n",
    "    # Use the candidate with ratio closest to 1 as an initial guess.\n",
    "    idx_closest = np.argmin(np.abs(ratio_data - 1))\n",
    "    init_guess = np.array([x_data[idx_closest], y_data[idx_closest]])\n",
    "\n",
    "    res = minimize(objective, init_guess, method='Nelder-Mead')\n",
    "    extrapolated_ref = (int(round(res.x[0])), int(round(res.x[1])))\n",
    "\n",
    "    # --- 3. Determine frame range and initial frame ---\n",
    "    # We derive the frame range from the 'eye_frame' column.\n",
    "    min_frame = int(df['eye_frame'].min())\n",
    "    max_frame = int(df['eye_frame'].max())\n",
    "    best_frame_num = int(df.iloc[idx_closest]['eye_frame'])\n",
    "    current_frame_num = best_frame_num\n",
    "\n",
    "    # Create window and prepare a container for display images.\n",
    "    window_name = \"Select Reference\"\n",
    "    cv2.namedWindow(window_name)\n",
    "    display_images = {\"combined\": None, \"blended\": None}\n",
    "\n",
    "    # --- 4. Function to update the display given a frame number ---\n",
    "    def update_display(frame_num):\n",
    "        # Retrieve and process the new frame.\n",
    "        frame = self.get_rotated_frame(frame_num, eye)\n",
    "        if frame is None:\n",
    "            print(\"Error retrieving frame number {}\".format(frame_num))\n",
    "            return\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        background = cv2.cvtColor(frame_gray, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Overlay candidate points.\n",
    "        overlay = background.copy()\n",
    "        for _, row in df.iterrows():\n",
    "            x = int(round(row['center_x']))\n",
    "            y = int(round(row['center_y']))\n",
    "            ratio = row['ratio']\n",
    "            norm = (ratio - min_ratio) / (max_ratio - min_ratio) if max_ratio > min_ratio else 0.5\n",
    "            value = int(norm * 255)\n",
    "            dummy = np.uint8([[value]])\n",
    "            color = cv2.applyColorMap(dummy, cv2.COLORMAP_TURBO)[0, 0].tolist()\n",
    "            cv2.circle(overlay, (x, y), radius=4, color=color, thickness=-1)\n",
    "\n",
    "        alpha = 0.5  # transparency for candidate points\n",
    "        blended = cv2.addWeighted(overlay, alpha, background, 1 - alpha, 0)\n",
    "\n",
    "        # Mark the extrapolated best reference point.\n",
    "        cv2.drawMarker(blended, extrapolated_ref, color=(0, 0, 255),\n",
    "                       markerType=cv2.MARKER_TILTED_CROSS, markerSize=30, thickness=3)\n",
    "        cv2.putText(blended, \"Extrapolated Best Ref\", (extrapolated_ref[0] + 10, extrapolated_ref[1] + 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Create a vertical colorbar.\n",
    "        bar_width = 50\n",
    "        bar_height = blended.shape[0]\n",
    "        gradient = np.linspace(0, 255, bar_height, dtype=np.uint8).reshape(bar_height, 1)\n",
    "        gradient = np.repeat(gradient, bar_width, axis=1)\n",
    "        colorbar = cv2.applyColorMap(gradient, cv2.COLORMAP_TURBO)\n",
    "        cv2.putText(colorbar, f\"{min_ratio:.2f}\", (5, 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        cv2.putText(colorbar, f\"{max_ratio:.2f}\", (5, bar_height - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "        # Combine the blended image and the colorbar.\n",
    "        combined = np.hstack([blended, colorbar])\n",
    "        cv2.putText(combined, \"Click on main image to select ref (ESC to cancel)\",\n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        # Save the updated images in our container.\n",
    "        display_images[\"combined\"] = combined\n",
    "        display_images[\"blended\"] = blended\n",
    "        cv2.imshow(window_name, combined)\n",
    "\n",
    "    # Initial display update.\n",
    "    update_display(current_frame_num)\n",
    "\n",
    "    # --- 5. Create the slider (trackbar) ---\n",
    "    def on_trackbar(val):\n",
    "        # Convert trackbar value back to the actual frame number.\n",
    "        new_frame_num = val + min_frame\n",
    "        update_display(new_frame_num)\n",
    "\n",
    "    # The trackbar range is set from 0 to (max_frame - min_frame)\n",
    "    cv2.createTrackbar(\"Frame\", window_name, best_frame_num - min_frame, max_frame - min_frame, on_trackbar)\n",
    "\n",
    "    # --- 6. Interactive selection via mouse callback ---\n",
    "    ref_point = []\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        nonlocal ref_point\n",
    "        # Only register clicks in the main image area (exclude the colorbar).\n",
    "        if event == cv2.EVENT_LBUTTONDOWN and display_images[\"blended\"] is not None and x < display_images[\"blended\"].shape[1]:\n",
    "            ref_point = [x, y]\n",
    "            cv2.drawMarker(display_images[\"combined\"], (x, y), color=(0, 255, 255),\n",
    "                           markerType=cv2.MARKER_STAR, markerSize=30, thickness=3)\n",
    "            cv2.imshow(window_name, display_images[\"combined\"])\n",
    "\n",
    "    cv2.setMouseCallback(window_name, mouse_callback)\n",
    "\n",
    "    # --- 7. Wait for selection or cancel ---\n",
    "    while True:\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if ref_point:\n",
    "            break\n",
    "        if key == 27:  # ESC key to cancel.\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if ref_point:\n",
    "        print(\"Selected reference point: X = {}, Y = {}\".format(ref_point[0], ref_point[1]))\n",
    "        return ref_point[0], ref_point[1]\n",
    "    else:\n",
    "        print(\"No reference point selected.\")\n",
    "        return None\n",
    "\n",
    "pick_reference_opencv(block,'left')"
   ],
   "id": "b1a8a58d280d97c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "cv2.destroyAllWindows()",
   "id": "a9d41db0a6093ae9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "block.left_eye_data.head()",
   "id": "39eaf13989880c5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "def play_video_with_combined_eye_plot(block, eye, path_to_video=False, xflip=False,\n",
    "                                        transformation_matrix=None, phi_in_radians=False,\n",
    "                                        phi_range=(-60, 60), theta_range=(-60, 60),\n",
    "                                        nbins=100, colorscale='Turbo'):\n",
    "    \"\"\"\n",
    "    Plays a video and displays a combined window in which:\n",
    "      - The left half shows the current video frame.\n",
    "      - The right half shows a 3D plot (a curved sheet) representing the eye data, with\n",
    "        the current frame's gaze highlighted and angle values printed.\n",
    "\n",
    "    Parameters:\n",
    "      - block: An object containing video paths and eye data (dataframes with columns\n",
    "               'eye_frame', 'k_phi', and 'k_theta').\n",
    "      - eye: 'left' or 'right', to choose the correct video and data.\n",
    "      - path_to_video: Optional override for video file path.\n",
    "      - xflip: If True, flips the video frame along the x-axis.\n",
    "      - transformation_matrix: If provided, applies an affine transformation to each frame.\n",
    "      - phi_in_radians: If True, the k_phi/k_theta values are assumed to be in radians.\n",
    "                        Otherwise, they will be converted from degrees.\n",
    "      - phi_range, theta_range: Angular limits (in degrees) for constructing the 3D surface.\n",
    "      - nbins: Resolution for the grid defining the 3D surface.\n",
    "      - colorscale: Colorscale used for the 3D surface.\n",
    "    \"\"\"\n",
    "    # Select the appropriate video path and eye data.\n",
    "    if eye.lower() == 'left':\n",
    "        video_path = block.le_videos[0] if not path_to_video else path_to_video\n",
    "        eye_data = block.left_eye_data\n",
    "    elif eye.lower() == 'right':\n",
    "        video_path = block.re_videos[0] if not path_to_video else path_to_video\n",
    "        eye_data = block.right_eye_data\n",
    "    else:\n",
    "        raise ValueError(\"eye can only be 'left' or 'right'\")\n",
    "\n",
    "    # Open the video capture.\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file.\")\n",
    "        return\n",
    "\n",
    "    # ---- Build the static 3D surface background for the eye plot ----\n",
    "    # Create a grid over the specified angular ranges.\n",
    "    phi_lin = np.linspace(phi_range[0], phi_range[1], nbins)\n",
    "    theta_lin = np.linspace(theta_range[0], theta_range[1], nbins)\n",
    "    phi_grid, theta_grid = np.meshgrid(phi_lin, theta_lin)\n",
    "\n",
    "    # Convert the grid from degrees to radians.\n",
    "    phi_rad_grid = np.deg2rad(phi_grid)\n",
    "    theta_rad_grid = np.deg2rad(theta_grid)\n",
    "\n",
    "    # Map grid points to Cartesian coordinates on a unit sphere.\n",
    "    x_surface = np.cos(theta_rad_grid) * np.cos(phi_rad_grid)\n",
    "    y_surface = np.cos(theta_rad_grid) * np.sin(phi_rad_grid)\n",
    "    z_surface = np.sin(theta_rad_grid)\n",
    "\n",
    "    # Create the surface trace.\n",
    "    surface_trace = go.Surface(x=x_surface, y=y_surface, z=z_surface,\n",
    "                               colorscale=colorscale,\n",
    "                               opacity=0.7,\n",
    "                               showscale=False)\n",
    "\n",
    "    # Create an initial marker trace (to be updated with the current gaze).\n",
    "    marker_trace = go.Scatter3d(x=[0], y=[0], z=[0],\n",
    "                                mode='markers',\n",
    "                                marker=dict(size=6, color='red'))\n",
    "\n",
    "    # Build the Plotly figure.\n",
    "    fig = go.Figure(data=[surface_trace, marker_trace])\n",
    "    fig.update_layout(title=f\"{eye.capitalize()} Eye - Current Gaze Direction\",\n",
    "                      scene=dict(aspectmode='data'),\n",
    "                      margin=dict(l=0, r=0, t=30, b=0))\n",
    "\n",
    "    # Name for the combined window.\n",
    "    window_name = \"Video and 3D Eye Plot\"\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Process the video frame if needed.\n",
    "        if xflip:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "        if transformation_matrix is not None:\n",
    "            frame = cv2.warpAffine(frame, transformation_matrix, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "        # Get the current frame number (0-indexed).\n",
    "        current_frame_num = int(cap.get(cv2.CAP_PROP_POS_FRAMES)) - 1\n",
    "\n",
    "        # Try to extract the corresponding eye data for this frame.\n",
    "        try:\n",
    "            idx = eye_data.query('eye_frame == @current_frame_num').index[0]\n",
    "            current_frame_data = eye_data.iloc[idx]\n",
    "        except IndexError:\n",
    "            # If there's no data for this frame, show just the video frame.\n",
    "            combined = frame\n",
    "            cv2.imshow(window_name, combined)\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        # Extract k_phi and k_theta from the eye data.\n",
    "        if phi_in_radians:\n",
    "            k_phi = float(current_frame_data['k_phi'])\n",
    "            k_theta = float(current_frame_data['k_theta'])\n",
    "        else:\n",
    "            k_phi = np.deg2rad(float(current_frame_data['k_phi']))\n",
    "            k_theta = np.deg2rad(float(current_frame_data['k_theta']))\n",
    "\n",
    "        # Calculate Cartesian coordinates for the current gaze point.\n",
    "        current_x = np.cos(k_theta) * np.cos(k_phi)\n",
    "        current_y = np.cos(k_theta) * np.sin(k_phi)\n",
    "        current_z = np.sin(k_theta)\n",
    "\n",
    "        # Update the marker trace in the Plotly figure.\n",
    "        fig.data[1].x = [current_x]\n",
    "        fig.data[1].y = [current_y]\n",
    "        fig.data[1].z = [current_z]\n",
    "        fig.update_layout(title=(f\"{eye.capitalize()} Eye - Current Gaze: \"\n",
    "                                 f\"phi={np.rad2deg(k_phi):.2f}°, theta={np.rad2deg(k_theta):.2f}°\"))\n",
    "\n",
    "        # Render the updated Plotly figure to a PNG image.\n",
    "        try:\n",
    "            png_bytes = pio.to_image(fig, format='png')\n",
    "            png_array = np.frombuffer(png_bytes, np.uint8)\n",
    "            plot_img = cv2.imdecode(png_array, cv2.IMREAD_COLOR)\n",
    "        except Exception as e:\n",
    "            print(\"Error rendering Plotly figure:\", e)\n",
    "            plot_img = np.zeros((frame.shape[0], frame.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "        # Optionally overlay additional text on the plot image.\n",
    "        text = (f'Frame: {current_frame_num}\\n'\n",
    "                f'phi: {np.rad2deg(k_phi):.2f}°\\n'\n",
    "                f'theta: {np.rad2deg(k_theta):.2f}°')\n",
    "        for i, line in enumerate(text.split('\\n')):\n",
    "            cv2.putText(plot_img, line, (10, 30 + i*30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Resize the plot image so its height matches the video frame.\n",
    "        plot_img = cv2.resize(plot_img, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "        # Combine the video frame (left) and the plot image (right) side by side.\n",
    "        combined = np.hstack((frame, plot_img))\n",
    "\n",
    "        # Show the combined image.\n",
    "        cv2.imshow(window_name, combined)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Cleanup.\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "play_video_with_combined_eye_plot(block, 'left', path_to_video=False, xflip=False,\n",
    "                                        transformation_matrix=None, phi_in_radians=False,\n",
    "                                        phi_range=(-60, 60), theta_range=(-60, 60),\n",
    "                                        nbins=100, colorscale='Turbo')"
   ],
   "id": "82776dc37c598164",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "block.liz_mov_df",
   "id": "58b3aadfbd07c490",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "block = block_collection[0]\n",
   "id": "9144e34c36202bd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "def block_get_lizard_movement(self):\n",
    "    # collect accelerometer data\n",
    "    # path definition\n",
    "    p = self.oe_path / 'analysis'\n",
    "    analysis_list = os.listdir(p)\n",
    "    correct_analysis = [i for i in analysis_list if self.animal_call in i][0]\n",
    "    p = p / str(correct_analysis)\n",
    "    mat_path = p / 'lizMov.mat'\n",
    "    print(f'path to mat file is {mat_path}')\n",
    "\n",
    "    # read mat file\n",
    "    try:\n",
    "        mat_data = h5py.File(str(mat_path), 'r')\n",
    "        # Read the datasets as numpy arrays\n",
    "        mat_dict = {\n",
    "            't_mov_ms':   mat_data['t_mov_ms'][:],\n",
    "            'movAll':     mat_data['movAll'][:],\n",
    "            't_static_ms': mat_data['t_static_ms'][:],\n",
    "            'staticAll':  mat_data['staticAll'][:],\n",
    "            'angles':     mat_data['angles'][:]  # Now read the angles data as an array\n",
    "        }\n",
    "\n",
    "        # Assuming t_mov_ms and movAll are (N,1) and angles is (N,3),\n",
    "        # we can horizontally stack them. If needed, squeeze the (N,1) arrays.\n",
    "        t_mov = np.squeeze(mat_dict['t_mov_ms'])\n",
    "        movAll = np.squeeze(mat_dict['movAll'])\n",
    "        angles = mat_dict['angles']  # shape: (N,3)\n",
    "\n",
    "        # Combine the data into one array (each row corresponds to one observation)\n",
    "        combined_data = np.hstack((t_mov[:, np.newaxis],\n",
    "                                   movAll[:, np.newaxis],\n",
    "                                   angles))\n",
    "\n",
    "        # Create DataFrame with appropriate column names\n",
    "        acc_df = pd.DataFrame(combined_data,\n",
    "                              columns=['t_mov_ms', 'movAll', 'angle1', 'angle2', 'angle3'])\n",
    "\n",
    "        mat_data.close()\n",
    "        self.liz_mov_df = acc_df\n",
    "        print(f'liz_mov_df created for {self}')\n",
    "    except FileNotFoundError:\n",
    "        print('mat file does not exist - run the matlab getLizMovement function')\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "block_get_lizard_movement(block)"
   ],
   "id": "3b5daf4cc3a9eb78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "# collect accelerometer data\n",
    "# path definition\n",
    "p = block.oe_path / 'analysis'\n",
    "analysis_list = os.listdir(p)\n",
    "correct_analysis = [i for i in analysis_list if block.animal_call in i][0]\n",
    "p = p / str(correct_analysis)\n",
    "mat_path = p / 'lizMov.mat'\n",
    "print(f'path to mat file is {mat_path}')\n"
   ],
   "id": "4f521e1fe6e3e7f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "mat_data = h5py.File(str(mat_path), 'r')\n",
    "# Read the datasets as numpy arrays\n",
    "mat_dict = {\n",
    "    't_mov_ms':   mat_data['t_mov_ms'][:],\n",
    "    'movAll':     mat_data['movAll'][:],\n",
    "    't_static_ms': mat_data['t_static_ms'][:],\n",
    "    'staticAll':  mat_data['staticAll'][:],\n",
    "    'angles':     mat_data['angles'][:]  # Now read the angles data as an array\n",
    "}\n",
    "\n",
    "# Assuming t_mov_ms and movAll are (N,1) and angles is (N,3),\n",
    "# we can horizontally stack them. If needed, squeeze the (N,1) arrays.\n",
    "t_mov = np.squeeze(mat_dict['t_mov_ms'])\n",
    "t_static_ms = np.squeeze(mat_dict['t_static_ms'])\n",
    "movAll = np.squeeze(mat_dict['movAll'])\n",
    "angles = mat_dict['angles']  # shape: (N,3)\n",
    "\n",
    "# Combine the data into one array (each row corresponds to one observation)\n",
    "combined_data = np.hstack((t_mov[:, np.newaxis],\n",
    "                           movAll[:, np.newaxis],\n",
    "                           angles))\n",
    "\n",
    "\n",
    "# Create DataFrame with appropriate column names\n",
    "acc_df = pd.DataFrame(combined_data,\n",
    "                      columns=['t_static_ms', 'movAll', 'angle1', 'angle2', 'angle3'])"
   ],
   "id": "379f6499e27a5061",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def block_get_lizard_movement(self):\n",
    "    # Define the path to the analysis folder and the correct MAT file.\n",
    "    p = self.oe_path / 'analysis'\n",
    "    analysis_list = os.listdir(p)\n",
    "    correct_analysis = [i for i in analysis_list if self.animal_call in i][0]\n",
    "    p = p / str(correct_analysis)\n",
    "    mat_path = p / 'lizMov.mat'\n",
    "    print(f'Path to mat file is {mat_path}')\n",
    "\n",
    "    try:\n",
    "        with h5py.File(str(mat_path), 'r') as mat_data:\n",
    "            # Read all necessary datasets.\n",
    "            t_mov_ms    = np.squeeze(mat_data['t_mov_ms'][:])\n",
    "            movAll      = np.squeeze(mat_data['movAll'][:])\n",
    "            t_static_ms = np.squeeze(mat_data['t_static_ms'][:])\n",
    "            angles      = mat_data['angles'][:]  # angles is assumed to be (N, 3)\n",
    "\n",
    "        # Create movement dataframe (assuming t_mov_ms and movAll have the same length).\n",
    "        df_movement = pd.DataFrame({\n",
    "            't_mov_ms': t_mov_ms,\n",
    "            'movAll':   movAll\n",
    "        })\n",
    "\n",
    "        # Create static dataframe: prepend t_static_ms to the angles data.\n",
    "        # Make sure that t_static_ms has the same length as the first dimension of angles.\n",
    "        df_static = pd.DataFrame(angles, columns=['angle1', 'angle2', 'angle3'])\n",
    "        df_static.insert(0, 't_static_ms', t_static_ms)\n",
    "\n",
    "        # Optionally store the DataFrames as attributes of self.\n",
    "        self.liz_mov_df = df_movement\n",
    "        self.liz_static_df = df_static\n",
    "\n",
    "        print(f'liz_mov_df and liz_static_df created for {self}')\n",
    "        return df_movement, df_static\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print('MAT file does not exist - run the MATLAB getLizMovement function')\n",
    "        return None, None\n",
    "block_get_lizard_movement(block)"
   ],
   "id": "150df07c58c4f476",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_pitch_roll(df_static):\n",
    "    \"\"\"\n",
    "    Plot pitch and roll (in degrees) over time using the t_static_ms axis.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_static : pandas.DataFrame\n",
    "        DataFrame containing the static data with columns:\n",
    "        't_static_ms' - time in milliseconds,\n",
    "        'angle1', 'angle2', 'angle3' - where angle2 is pitch (radians)\n",
    "        and angle3 is roll (radians).\n",
    "    \"\"\"\n",
    "    # Conversion factor from radians to degrees\n",
    "    rad2deg = 180.0 / np.pi\n",
    "\n",
    "    # Convert pitch and roll to degrees\n",
    "    pitch_deg = df_static['angle2'] * rad2deg  # pitch is angle2\n",
    "    roll_deg  = df_static['angle3'] * rad2deg   # roll is angle3\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df_static['t_static_ms'], pitch_deg, label='Pitch (deg)', linewidth=1.5)\n",
    "    plt.plot(df_static['t_static_ms'], roll_deg, label='Roll (deg)', linewidth=1.5)\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Angle (degrees)')\n",
    "    plt.title('Pitch and Roll vs Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have created df_static from your MAT file\n",
    "\n",
    "plot_pitch_roll(block.liz_static_df)"
   ],
   "id": "778e2e55ff2bdb99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "block",
   "id": "18b70c91edc37bcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "block = block_collection[0]",
   "id": "2e94fb6434540007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": "block.left_eye_data.ms_axis.iloc[-1] / 1000 / 60",
   "id": "18816ce9bfab7840",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "def export_inter_saccade_intervals_density_average_with_shaded_sem(\n",
    "    all_saccade_collection, figure_size=(6, 4), export_path=None, plot_name='ISI_histogram'\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots an average density histogram of inter-saccade intervals with shaded SEM across animals.\n",
    "\n",
    "    Parameters:\n",
    "    - all_saccade_collection (pd.DataFrame): DataFrame with saccade data\n",
    "    - figure_size (tuple): Size of the plot\n",
    "    - export_path (str): Directory for export (optional)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    import pickle\n",
    "    import datetime\n",
    "    from scipy.stats import sem\n",
    "\n",
    "    # Sort and calculate inter-saccade intervals\n",
    "    all_saccade_collection_sorted = all_saccade_collection.sort_values(\n",
    "        by=[\"animal\", \"block\", \"saccade_on_ms\"]\n",
    "    )\n",
    "    grouped = all_saccade_collection_sorted.groupby(\"animal\")\n",
    "    bins = np.geomspace(17, 20000, 20)\n",
    "\n",
    "    animal_densities = []\n",
    "\n",
    "    # Compute density histograms for each animal\n",
    "    for animal, group in grouped:\n",
    "        inter_saccade_intervals = []\n",
    "        for block, block_group in group.groupby(\"block\"):\n",
    "            saccade_times = block_group[\"saccade_on_ms\"].values\n",
    "            if len(saccade_times) > 1:\n",
    "                inter_saccade_intervals.extend(np.diff(saccade_times))\n",
    "\n",
    "        # Compute density histogram\n",
    "        hist, _ = np.histogram(inter_saccade_intervals, bins=bins)\n",
    "        hist = hist / np.sum(hist)\n",
    "        animal_densities.append(hist)\n",
    "\n",
    "    # Convert to NumPy array for averaging\n",
    "    animal_densities = np.array(animal_densities)\n",
    "\n",
    "    # Compute mean and SEM\n",
    "    mean_density = animal_densities.mean(axis=0)\n",
    "    sem_density = sem(animal_densities, axis=0, nan_policy=\"omit\")\n",
    "\n",
    "    # Calculate the shaded limits\n",
    "    lower_bound = mean_density - sem_density\n",
    "    upper_bound = mean_density + sem_density\n",
    "\n",
    "    # Plot histogram with shaded SEM\n",
    "    bin_centers = np.sqrt(bins[:-1] * bins[1:])  # Geometric mean for log bins\n",
    "    fig, ax = plt.subplots(figsize=figure_size, dpi=300)\n",
    "    ax.bar(\n",
    "        bin_centers,\n",
    "        mean_density,\n",
    "        width=np.diff(bins),\n",
    "        align=\"center\",\n",
    "        color=\"gray\",\n",
    "        edgecolor=\"black\",\n",
    "        alpha=0.6,\n",
    "        label=\"Mean Density\",\n",
    "    )\n",
    "\n",
    "    # Add shaded SEM\n",
    "    ax.fill_between(\n",
    "        bin_centers,\n",
    "        lower_bound,\n",
    "        upper_bound,\n",
    "        color=\"lightblue\",\n",
    "        alpha=0.6,\n",
    "        label=\"SEM\",\n",
    "    )\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    #ax.set_title(\"Average Density Histogram of Inter-Saccade Intervals\", fontsize=12)\n",
    "    ax.set_xlabel(\"ISI [ms]\", fontsize=10)\n",
    "    ax.set_ylabel(\"Probability\", fontsize=10)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=8)\n",
    "    ax.legend(fontsize=8, loc=\"upper right\")\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Handle export\n",
    "    if export_path is not None:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        full_export_path = os.path.join(export_path, f\"inter_saccade_export_{timestamp}\")\n",
    "        os.makedirs(full_export_path, exist_ok=True)\n",
    "\n",
    "        # Save plot\n",
    "        pdf_file = os.path.join(full_export_path, plot_name)\n",
    "        fig.savefig(pdf_file, format=\"pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "\n",
    "        # Save data\n",
    "        data_file = os.path.join(full_export_path, \"all_saccade_collection.pkl\")\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(all_saccade_collection, f)\n",
    "\n",
    "        print(f\"Exported plot and data to: {full_export_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "export_path = r'Z:\\Nimrod\\Paper_Figures\\2024_Personalized_headstage_eye_tracking\\Figure_4\\material\\Inter_saccade_intervals_histogram'\n",
    "export_inter_saccade_intervals_density_average_with_shaded_sem(all_saccade_collection.query('animal==\"PV_62\"'), figure_size=(1.8, 1.2), export_path=None, plot_name='ISI_histogram_multi_animal.pdf')"
   ],
   "id": "3164a55d95bc9f86",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
